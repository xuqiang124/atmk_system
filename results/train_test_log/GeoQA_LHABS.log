/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2024-06-02 22:18:27,675 : INFO : Loading config...
2024-06-02 22:18:27,676 : INFO : {'cache_file_h5py': '../file_data/a7/math_data.h5', 'cache_file_pickle': '../file_data/a7/vocab_label.pkl', 'embeddings': '../file_data/a7/embeddings.pkl', 'maxlen': 120, 'emb_size': 300, 'epochs': 100, 'batch_size': 256, 'alpha': 4, 'hidden_size': 512, 'num_classes_list': [49], 'l_patience': 2, 'b_patience': 3}
2024-06-02 22:18:27,676 : INFO : Loading data...
2024-06-02 22:18:27,679 : INFO : Loading embeddings...
2024-06-02 22:18:27,682 : INFO : model name labs
TOTAL: 4950 TRAIN: [[3325 1645 3528 ...    0    0    0]
 [3325 1645 2529 ...    0    0    0]
 [1732 1399  586 ...    0    0    0]
 ...
 [ 607 2529  281 ...    0    0    0]
 [3325 1645  607 ...    0    0    0]
 [2899 1645  924 ...    0    0    0]] 3712 TEST: [[3325 1645 1109 ...    0    0    0]
 [2899 1645  607 ...    0    0    0]
 [3325 1645  554 ...    0    0    0]
 ...
 [1109 1645 3325 ...    0    0    0]
 [3325 1645 2529 ...    0    0    0]
 [3325 1645  607 ...    0    0    0]] 1238
2024-06-02 22:18:27,685 : INFO : =====Start final=====
2969
743
1238
2024-06-02 22:18:27.724883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:27.746994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:27.747096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:27.747333: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-02 22:18:27.749512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:27.749612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:27.749669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:28.090853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:28.090977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:28.091044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-02 22:18:28.091113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22102 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 49, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 49, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 49, 1024)     308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 dot (Dot)                      (None, 49)           0           ['label_lcm_emb[0][0]',          
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 49)           2450        ['dot[0][0]']                    
                                                                                                  
 concatenate (Concatenate)      (None, 98)           0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 5,092,791
Trainable params: 4,028,391
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
Epoch 1/100
2024-06-02 22:18:31.198629: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2024-06-02 22:18:31.232569: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8906
11/12 [==========================>...] - ETA: 0s - loss: 1.5677 - lcm_precision_1k: 0.1580 - lcm_precision_2k: 0.1397 - lcm_precision_3k: 0.1335 - lcm_precision_5k: 0.1158 - lcm_recall_1k: 0.0792 - lcm_recall_2k: 0.1363 - lcm_recall_3k: 0.1952 - lcm_recall_5k: 0.2774 - lcm_f1_1k: 0.1055 - lcm_f1_2k: 0.1379 - lcm_f1_3k: 0.1585 - lcm_f1_5k: 0.1633 - lcm_accuracy_1k: 0.1580 - lcm_accuracy_2k: 0.2603 - lcm_accuracy_3k: 0.3441 - lcm_accuracy_5k: 0.4609 - lcm_hamming_loss_k: 0.1706
Epoch 00001: val_loss improved from inf to 1.51459, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 4s 116ms/step - loss: 1.5641 - lcm_precision_1k: 0.1492 - lcm_precision_2k: 0.1333 - lcm_precision_3k: 0.1289 - lcm_precision_5k: 0.1143 - lcm_recall_1k: 0.0745 - lcm_recall_2k: 0.1297 - lcm_recall_3k: 0.1896 - lcm_recall_5k: 0.2759 - lcm_f1_1k: 0.0994 - lcm_f1_2k: 0.1314 - lcm_f1_3k: 0.1534 - lcm_f1_5k: 0.1615 - lcm_accuracy_1k: 0.1492 - lcm_accuracy_2k: 0.2490 - lcm_accuracy_3k: 0.3345 - lcm_accuracy_5k: 0.4606 - lcm_hamming_loss_k: 0.1740 - val_loss: 1.5146 - val_lcm_precision_1k: 0.0432 - val_lcm_precision_2k: 0.0579 - val_lcm_precision_3k: 0.0676 - val_lcm_precision_5k: 0.0915 - val_lcm_recall_1k: 0.0212 - val_lcm_recall_2k: 0.0550 - val_lcm_recall_3k: 0.1018 - val_lcm_recall_5k: 0.2318 - val_lcm_f1_1k: 0.0281 - val_lcm_f1_2k: 0.0559 - val_lcm_f1_3k: 0.0808 - val_lcm_f1_5k: 0.1305 - val_lcm_accuracy_1k: 0.0432 - val_lcm_accuracy_2k: 0.1158 - val_lcm_accuracy_3k: 0.1949 - val_lcm_accuracy_5k: 0.4353 - val_lcm_hamming_loss_k: 0.1888
Epoch 2/100
10/12 [========================>.....] - ETA: 0s - loss: 1.4962 - lcm_precision_1k: 0.0469 - lcm_precision_2k: 0.0455 - lcm_precision_3k: 0.0661 - lcm_precision_5k: 0.0959 - lcm_recall_1k: 0.0248 - lcm_recall_2k: 0.0461 - lcm_recall_3k: 0.1035 - lcm_recall_5k: 0.2441 - lcm_f1_1k: 0.0323 - lcm_f1_2k: 0.0457 - lcm_f1_3k: 0.0807 - lcm_f1_5k: 0.1377 - lcm_accuracy_1k: 0.0469 - lcm_accuracy_2k: 0.0906 - lcm_accuracy_3k: 0.1969 - lcm_accuracy_5k: 0.4664 - lcm_hamming_loss_k: 0.1076/home/dzq/k12/atmk_system-master/MathByte/models/evaluation_metrics.py:260: RuntimeWarning: invalid value encountered in true_divide
  return (2 * p_k * r_k) / (p_k + r_k)
11/12 [==========================>...] - ETA: 0s - loss: 1.4942 - lcm_precision_1k: 0.0426 - lcm_precision_2k: 0.0437 - lcm_precision_3k: 0.0659 - lcm_precision_5k: 0.0930 - lcm_recall_1k: 0.0225 - lcm_recall_2k: 0.0439 - lcm_recall_3k: 0.1030 - lcm_recall_5k: 0.2363 - lcm_f1_1k: nan - lcm_f1_2k: 0.0437 - lcm_f1_3k: 0.0803 - lcm_f1_5k: 0.1335 - lcm_accuracy_1k: 0.0426 - lcm_accuracy_2k: 0.0870 - lcm_accuracy_3k: 0.1964 - lcm_accuracy_5k: 0.4521 - lcm_hamming_loss_k: 0.1036   
Epoch 00002: val_loss improved from 1.51459 to 1.46692, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 1.4932 - lcm_precision_1k: 0.0423 - lcm_precision_2k: 0.0452 - lcm_precision_3k: 0.0686 - lcm_precision_5k: 0.0925 - lcm_recall_1k: 0.0217 - lcm_recall_2k: 0.0454 - lcm_recall_3k: 0.1066 - lcm_recall_5k: 0.2341 - lcm_f1_1k: nan - lcm_f1_2k: 0.0452 - lcm_f1_3k: 0.0834 - lcm_f1_5k: 0.1326 - lcm_accuracy_1k: 0.0423 - lcm_accuracy_2k: 0.0901 - lcm_accuracy_3k: 0.2040 - lcm_accuracy_5k: 0.4476 - lcm_hamming_loss_k: 0.1001 - val_loss: 1.4669 - val_lcm_precision_1k: 0.0182 - val_lcm_precision_2k: 0.0373 - val_lcm_precision_3k: 0.0758 - val_lcm_precision_5k: 0.0617 - val_lcm_recall_1k: 0.0092 - val_lcm_recall_2k: 0.0373 - val_lcm_recall_3k: 0.1146 - val_lcm_recall_5k: 0.1539 - val_lcm_f1_1k: nan - val_lcm_f1_2k: nan - val_lcm_f1_3k: 0.0905 - val_lcm_f1_5k: 0.0876 - val_lcm_accuracy_1k: 0.0182 - val_lcm_accuracy_2k: 0.0747 - val_lcm_accuracy_3k: 0.2218 - val_lcm_accuracy_5k: 0.2928 - val_lcm_hamming_loss_k: 0.0629
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4343 - lcm_precision_1k: 0.0113 - lcm_precision_2k: 0.0495 - lcm_precision_3k: 0.0697 - lcm_precision_5k: 0.0741 - lcm_recall_1k: 0.0043 - lcm_recall_2k: 0.0507 - lcm_recall_3k: 0.1080 - lcm_recall_5k: 0.1924 - lcm_f1_1k: nan - lcm_f1_2k: 0.0500 - lcm_f1_3k: 0.0847 - lcm_f1_5k: 0.1069 - lcm_accuracy_1k: 0.0113 - lcm_accuracy_2k: 0.0991 - lcm_accuracy_3k: 0.2067 - lcm_accuracy_5k: 0.3587 - lcm_hamming_loss_k: 0.0625      
Epoch 00003: val_loss improved from 1.46692 to 1.37244, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 1.4314 - lcm_precision_1k: 0.0131 - lcm_precision_2k: 0.0560 - lcm_precision_3k: 0.0748 - lcm_precision_5k: 0.0782 - lcm_recall_1k: 0.0049 - lcm_recall_2k: 0.0571 - lcm_recall_3k: 0.1155 - lcm_recall_5k: 0.2036 - lcm_f1_1k: nan - lcm_f1_2k: 0.0565 - lcm_f1_3k: 0.0908 - lcm_f1_5k: 0.1130 - lcm_accuracy_1k: 0.0131 - lcm_accuracy_2k: 0.1121 - lcm_accuracy_3k: 0.2221 - lcm_accuracy_5k: 0.3783 - lcm_hamming_loss_k: 0.0624 - val_loss: 1.3724 - val_lcm_precision_1k: 0.0943 - val_lcm_precision_2k: 0.1537 - val_lcm_precision_3k: 0.1515 - val_lcm_precision_5k: 0.1291 - val_lcm_recall_1k: 0.0503 - val_lcm_recall_2k: 0.1651 - val_lcm_recall_3k: 0.2379 - val_lcm_recall_5k: 0.3343 - val_lcm_f1_1k: 0.0651 - val_lcm_f1_2k: 0.1584 - val_lcm_f1_3k: 0.1845 - val_lcm_f1_5k: 0.1856 - val_lcm_accuracy_1k: 0.0943 - val_lcm_accuracy_2k: 0.2993 - val_lcm_accuracy_3k: 0.4342 - val_lcm_accuracy_5k: 0.5796 - val_lcm_hamming_loss_k: 0.0598
Epoch 4/100
12/12 [==============================] - ETA: 0s - loss: 1.2983 - lcm_precision_1k: 0.2334 - lcm_precision_2k: 0.2086 - lcm_precision_3k: 0.1911 - lcm_precision_5k: 0.1678 - lcm_recall_1k: 0.1227 - lcm_recall_2k: 0.2151 - lcm_recall_3k: 0.2963 - lcm_recall_5k: 0.4347 - lcm_f1_1k: 0.1606 - lcm_f1_2k: 0.2115 - lcm_f1_3k: 0.2321 - lcm_f1_5k: 0.2420 - lcm_accuracy_1k: 0.2334 - lcm_accuracy_2k: 0.4000 - lcm_accuracy_3k: 0.5144 - lcm_accuracy_5k: 0.6670 - lcm_hamming_loss_k: 0.0533
Epoch 00004: val_loss improved from 1.37244 to 1.20434, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 1.2983 - lcm_precision_1k: 0.2334 - lcm_precision_2k: 0.2086 - lcm_precision_3k: 0.1911 - lcm_precision_5k: 0.1678 - lcm_recall_1k: 0.1227 - lcm_recall_2k: 0.2151 - lcm_recall_3k: 0.2963 - lcm_recall_5k: 0.4347 - lcm_f1_1k: 0.1606 - lcm_f1_2k: 0.2115 - lcm_f1_3k: 0.2321 - lcm_f1_5k: 0.2420 - lcm_accuracy_1k: 0.2334 - lcm_accuracy_2k: 0.4000 - lcm_accuracy_3k: 0.5144 - lcm_accuracy_5k: 0.6670 - lcm_hamming_loss_k: 0.0533 - val_loss: 1.2043 - val_lcm_precision_1k: 0.4625 - val_lcm_precision_2k: 0.3328 - val_lcm_precision_3k: 0.2706 - val_lcm_precision_5k: 0.1963 - val_lcm_recall_1k: 0.2512 - val_lcm_recall_2k: 0.3514 - val_lcm_recall_3k: 0.4242 - val_lcm_recall_5k: 0.5030 - val_lcm_f1_1k: 0.3249 - val_lcm_f1_2k: 0.3414 - val_lcm_f1_3k: 0.3300 - val_lcm_f1_5k: 0.2821 - val_lcm_accuracy_1k: 0.4625 - val_lcm_accuracy_2k: 0.6258 - val_lcm_accuracy_3k: 0.6917 - val_lcm_accuracy_5k: 0.7606 - val_lcm_hamming_loss_k: 0.0447
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 1.1262 - lcm_precision_1k: 0.4514 - lcm_precision_2k: 0.3551 - lcm_precision_3k: 0.3017 - lcm_precision_5k: 0.2268 - lcm_recall_1k: 0.2379 - lcm_recall_2k: 0.3756 - lcm_recall_3k: 0.4730 - lcm_recall_5k: 0.5827 - lcm_f1_1k: 0.3114 - lcm_f1_2k: 0.3649 - lcm_f1_3k: 0.3683 - lcm_f1_5k: 0.3265 - lcm_accuracy_1k: 0.4514 - lcm_accuracy_2k: 0.6296 - lcm_accuracy_3k: 0.7127 - lcm_accuracy_5k: 0.7926 - lcm_hamming_loss_k: 0.0445
Epoch 00005: val_loss improved from 1.20434 to 1.07829, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 1.1247 - lcm_precision_1k: 0.4524 - lcm_precision_2k: 0.3593 - lcm_precision_3k: 0.3060 - lcm_precision_5k: 0.2298 - lcm_recall_1k: 0.2376 - lcm_recall_2k: 0.3774 - lcm_recall_3k: 0.4779 - lcm_recall_5k: 0.5872 - lcm_f1_1k: 0.3113 - lcm_f1_2k: 0.3680 - lcm_f1_3k: 0.3730 - lcm_f1_5k: 0.3303 - lcm_accuracy_1k: 0.4524 - lcm_accuracy_2k: 0.6283 - lcm_accuracy_3k: 0.7165 - lcm_accuracy_5k: 0.7963 - lcm_hamming_loss_k: 0.0445 - val_loss: 1.0783 - val_lcm_precision_1k: 0.5352 - val_lcm_precision_2k: 0.4324 - val_lcm_precision_3k: 0.3599 - val_lcm_precision_5k: 0.2640 - val_lcm_recall_1k: 0.2779 - val_lcm_recall_2k: 0.4426 - val_lcm_recall_3k: 0.5526 - val_lcm_recall_5k: 0.6696 - val_lcm_f1_1k: 0.3647 - val_lcm_f1_2k: 0.4369 - val_lcm_f1_3k: 0.4355 - val_lcm_f1_5k: 0.3783 - val_lcm_accuracy_1k: 0.5352 - val_lcm_accuracy_2k: 0.6780 - val_lcm_accuracy_3k: 0.7760 - val_lcm_accuracy_5k: 0.8669 - val_lcm_hamming_loss_k: 0.0418
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 1.0514 - lcm_precision_1k: 0.5195 - lcm_precision_2k: 0.4227 - lcm_precision_3k: 0.3418 - lcm_precision_5k: 0.2599 - lcm_recall_1k: 0.2760 - lcm_recall_2k: 0.4462 - lcm_recall_3k: 0.5353 - lcm_recall_5k: 0.6611 - lcm_f1_1k: 0.3603 - lcm_f1_2k: 0.4339 - lcm_f1_3k: 0.4171 - lcm_f1_5k: 0.3730 - lcm_accuracy_1k: 0.5195 - lcm_accuracy_2k: 0.6822 - lcm_accuracy_3k: 0.7646 - lcm_accuracy_5k: 0.8487 - lcm_hamming_loss_k: 0.0418
Epoch 00006: val_loss improved from 1.07829 to 0.92827, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 1.0461 - lcm_precision_1k: 0.5182 - lcm_precision_2k: 0.4207 - lcm_precision_3k: 0.3402 - lcm_precision_5k: 0.2603 - lcm_recall_1k: 0.2775 - lcm_recall_2k: 0.4471 - lcm_recall_3k: 0.5351 - lcm_recall_5k: 0.6631 - lcm_f1_1k: 0.3612 - lcm_f1_2k: 0.4332 - lcm_f1_3k: 0.4158 - lcm_f1_5k: 0.3738 - lcm_accuracy_1k: 0.5182 - lcm_accuracy_2k: 0.6836 - lcm_accuracy_3k: 0.7635 - lcm_accuracy_5k: 0.8510 - lcm_hamming_loss_k: 0.0418 - val_loss: 0.9283 - val_lcm_precision_1k: 0.5389 - val_lcm_precision_2k: 0.4418 - val_lcm_precision_3k: 0.3693 - val_lcm_precision_5k: 0.2760 - val_lcm_recall_1k: 0.2792 - val_lcm_recall_2k: 0.4686 - val_lcm_recall_3k: 0.5803 - val_lcm_recall_5k: 0.6933 - val_lcm_f1_1k: 0.3664 - val_lcm_f1_2k: 0.4544 - val_lcm_f1_3k: 0.4510 - val_lcm_f1_5k: 0.3945 - val_lcm_accuracy_1k: 0.5389 - val_lcm_accuracy_2k: 0.6978 - val_lcm_accuracy_3k: 0.8003 - val_lcm_accuracy_5k: 0.8724 - val_lcm_hamming_loss_k: 0.0416
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.8511 - lcm_precision_1k: 0.5597 - lcm_precision_2k: 0.4743 - lcm_precision_3k: 0.3978 - lcm_precision_5k: 0.2980 - lcm_recall_1k: 0.3089 - lcm_recall_2k: 0.5077 - lcm_recall_3k: 0.6261 - lcm_recall_5k: 0.7540 - lcm_f1_1k: 0.3978 - lcm_f1_2k: 0.4903 - lcm_f1_3k: 0.4865 - lcm_f1_5k: 0.4271 - lcm_accuracy_1k: 0.5597 - lcm_accuracy_2k: 0.7585 - lcm_accuracy_3k: 0.8356 - lcm_accuracy_5k: 0.9038 - lcm_hamming_loss_k: 0.0400 ETA: 0s - loss: 0.9131 - lcm_precision_1k: 0.5078 - lcm_precision_2k: 0.4445 - lcm_precision_3k: 0.3781 - lcm_precision_5k: 0.2845 - lcm_recall_1k: 0.2629 - lcm_recall_2k: 0.4614 - lcm_recall_3k: 0.5854 - lcm_recall_5k: 0.7099 - lcm_f1_1k: 0.3464 - lcm_f1_2k: 0.4527 - lcm_f1_3k: 0.4594 - lcm_f1_5k: 0.4062 - lcm_accuracy_1k: 0.5078 - lcm_accuracy_2k: 0.7086 - lcm_accuracy_3k: 0.7992 - lcm_accuracy_5k: 0.8781 - lcm_hamming_loss
Epoch 00007: val_loss improved from 0.92827 to 0.79067, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.8473 - lcm_precision_1k: 0.5626 - lcm_precision_2k: 0.4794 - lcm_precision_3k: 0.4032 - lcm_precision_5k: 0.3021 - lcm_recall_1k: 0.3083 - lcm_recall_2k: 0.5104 - lcm_recall_3k: 0.6296 - lcm_recall_5k: 0.7589 - lcm_f1_1k: 0.3980 - lcm_f1_2k: 0.4943 - lcm_f1_3k: 0.4914 - lcm_f1_5k: 0.4321 - lcm_accuracy_1k: 0.5626 - lcm_accuracy_2k: 0.7623 - lcm_accuracy_3k: 0.8389 - lcm_accuracy_5k: 0.9074 - lcm_hamming_loss_k: 0.0401 - val_loss: 0.7907 - val_lcm_precision_1k: 0.6621 - val_lcm_precision_2k: 0.5226 - val_lcm_precision_3k: 0.4286 - val_lcm_precision_5k: 0.3113 - val_lcm_recall_1k: 0.3664 - val_lcm_recall_2k: 0.5378 - val_lcm_recall_3k: 0.6507 - val_lcm_recall_5k: 0.7652 - val_lcm_f1_1k: 0.4713 - val_lcm_f1_2k: 0.5295 - val_lcm_f1_3k: 0.5162 - val_lcm_f1_5k: 0.4421 - val_lcm_accuracy_1k: 0.6621 - val_lcm_accuracy_2k: 0.7822 - val_lcm_accuracy_3k: 0.8526 - val_lcm_accuracy_5k: 0.9063 - val_lcm_hamming_loss_k: 0.0366
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.7603 - lcm_precision_1k: 0.6577 - lcm_precision_2k: 0.5272 - lcm_precision_3k: 0.4392 - lcm_precision_5k: 0.3202 - lcm_recall_1k: 0.3702 - lcm_recall_2k: 0.5658 - lcm_recall_3k: 0.6850 - lcm_recall_5k: 0.8067 - lcm_f1_1k: 0.4736 - lcm_f1_2k: 0.5457 - lcm_f1_3k: 0.5351 - lcm_f1_5k: 0.4584 - lcm_accuracy_1k: 0.6577 - lcm_accuracy_2k: 0.8118 - lcm_accuracy_3k: 0.8771 - lcm_accuracy_5k: 0.9382 - lcm_hamming_loss_k: 0.0363
Epoch 00008: val_loss improved from 0.79067 to 0.73498, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.7584 - lcm_precision_1k: 0.6579 - lcm_precision_2k: 0.5271 - lcm_precision_3k: 0.4369 - lcm_precision_5k: 0.3173 - lcm_recall_1k: 0.3737 - lcm_recall_2k: 0.5694 - lcm_recall_3k: 0.6850 - lcm_recall_5k: 0.8046 - lcm_f1_1k: 0.4764 - lcm_f1_2k: 0.5472 - lcm_f1_3k: 0.5334 - lcm_f1_5k: 0.4550 - lcm_accuracy_1k: 0.6579 - lcm_accuracy_2k: 0.8100 - lcm_accuracy_3k: 0.8748 - lcm_accuracy_5k: 0.9357 - lcm_hamming_loss_k: 0.0360 - val_loss: 0.7350 - val_lcm_precision_1k: 0.6812 - val_lcm_precision_2k: 0.5523 - val_lcm_precision_3k: 0.4552 - val_lcm_precision_5k: 0.3196 - val_lcm_recall_1k: 0.3888 - val_lcm_recall_2k: 0.5843 - val_lcm_recall_3k: 0.6970 - val_lcm_recall_5k: 0.7931 - val_lcm_f1_1k: 0.4948 - val_lcm_f1_2k: 0.5673 - val_lcm_f1_3k: 0.5501 - val_lcm_f1_5k: 0.4551 - val_lcm_accuracy_1k: 0.6812 - val_lcm_accuracy_2k: 0.8316 - val_lcm_accuracy_3k: 0.8873 - val_lcm_accuracy_5k: 0.9348 - val_lcm_hamming_loss_k: 0.0358
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6735 - lcm_precision_1k: 0.7216 - lcm_precision_2k: 0.5785 - lcm_precision_3k: 0.4645 - lcm_precision_5k: 0.3306 - lcm_recall_1k: 0.4107 - lcm_recall_2k: 0.6224 - lcm_recall_3k: 0.7280 - lcm_recall_5k: 0.8326 - lcm_f1_1k: 0.5234 - lcm_f1_2k: 0.5995 - lcm_f1_3k: 0.5670 - lcm_f1_5k: 0.4732 - lcm_accuracy_1k: 0.7216 - lcm_accuracy_2k: 0.8551 - lcm_accuracy_3k: 0.9080 - lcm_accuracy_5k: 0.9510 - lcm_hamming_loss_k: 0.0334
Epoch 00009: val_loss improved from 0.73498 to 0.63978, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.6685 - lcm_precision_1k: 0.7306 - lcm_precision_2k: 0.5850 - lcm_precision_3k: 0.4701 - lcm_precision_5k: 0.3336 - lcm_recall_1k: 0.4165 - lcm_recall_2k: 0.6273 - lcm_recall_3k: 0.7333 - lcm_recall_5k: 0.8362 - lcm_f1_1k: 0.5304 - lcm_f1_2k: 0.6053 - lcm_f1_3k: 0.5728 - lcm_f1_5k: 0.4768 - lcm_accuracy_1k: 0.7306 - lcm_accuracy_2k: 0.8612 - lcm_accuracy_3k: 0.9124 - lcm_accuracy_5k: 0.9540 - lcm_hamming_loss_k: 0.0332 - val_loss: 0.6398 - val_lcm_precision_1k: 0.7526 - val_lcm_precision_2k: 0.6004 - val_lcm_precision_3k: 0.4819 - val_lcm_precision_5k: 0.3389 - val_lcm_recall_1k: 0.4240 - val_lcm_recall_2k: 0.6279 - val_lcm_recall_3k: 0.7378 - val_lcm_recall_5k: 0.8343 - val_lcm_f1_1k: 0.5418 - val_lcm_f1_2k: 0.6132 - val_lcm_f1_3k: 0.5823 - val_lcm_f1_5k: 0.4814 - val_lcm_accuracy_1k: 0.7526 - val_lcm_accuracy_2k: 0.8699 - val_lcm_accuracy_3k: 0.9203 - val_lcm_accuracy_5k: 0.9569 - val_lcm_hamming_loss_k: 0.0329
Epoch 10/100
12/12 [==============================] - ETA: 0s - loss: 0.5881 - lcm_precision_1k: 0.7843 - lcm_precision_2k: 0.6235 - lcm_precision_3k: 0.4994 - lcm_precision_5k: 0.3482 - lcm_recall_1k: 0.4507 - lcm_recall_2k: 0.6640 - lcm_recall_3k: 0.7711 - lcm_recall_5k: 0.8681 - lcm_f1_1k: 0.5722 - lcm_f1_2k: 0.6429 - lcm_f1_3k: 0.6060 - lcm_f1_5k: 0.4969 - lcm_accuracy_1k: 0.7843 - lcm_accuracy_2k: 0.8908 - lcm_accuracy_3k: 0.9337 - lcm_accuracy_5k: 0.9700 - lcm_hamming_loss_k: 0.0310
Epoch 00010: val_loss improved from 0.63978 to 0.62688, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 108ms/step - loss: 0.5881 - lcm_precision_1k: 0.7843 - lcm_precision_2k: 0.6235 - lcm_precision_3k: 0.4994 - lcm_precision_5k: 0.3482 - lcm_recall_1k: 0.4507 - lcm_recall_2k: 0.6640 - lcm_recall_3k: 0.7711 - lcm_recall_5k: 0.8681 - lcm_f1_1k: 0.5722 - lcm_f1_2k: 0.6429 - lcm_f1_3k: 0.6060 - lcm_f1_5k: 0.4969 - lcm_accuracy_1k: 0.7843 - lcm_accuracy_2k: 0.8908 - lcm_accuracy_3k: 0.9337 - lcm_accuracy_5k: 0.9700 - lcm_hamming_loss_k: 0.0310 - val_loss: 0.6269 - val_lcm_precision_1k: 0.7442 - val_lcm_precision_2k: 0.6077 - val_lcm_precision_3k: 0.4942 - val_lcm_precision_5k: 0.3507 - val_lcm_recall_1k: 0.4279 - val_lcm_recall_2k: 0.6422 - val_lcm_recall_3k: 0.7515 - val_lcm_recall_5k: 0.8591 - val_lcm_f1_1k: 0.5433 - val_lcm_f1_2k: 0.6239 - val_lcm_f1_3k: 0.5956 - val_lcm_f1_5k: 0.4975 - val_lcm_accuracy_1k: 0.7442 - val_lcm_accuracy_2k: 0.8890 - val_lcm_accuracy_3k: 0.9298 - val_lcm_accuracy_5k: 0.9634 - val_lcm_hamming_loss_k: 0.0333
Epoch 11/100
12/12 [==============================] - ETA: 0s - loss: 0.5500 - lcm_precision_1k: 0.8095 - lcm_precision_2k: 0.6448 - lcm_precision_3k: 0.5176 - lcm_precision_5k: 0.3557 - lcm_recall_1k: 0.4670 - lcm_recall_2k: 0.6879 - lcm_recall_3k: 0.7986 - lcm_recall_5k: 0.8889 - lcm_f1_1k: 0.5923 - lcm_f1_2k: 0.6656 - lcm_f1_3k: 0.6280 - lcm_f1_5k: 0.5080 - lcm_accuracy_1k: 0.8095 - lcm_accuracy_2k: 0.9072 - lcm_accuracy_3k: 0.9505 - lcm_accuracy_5k: 0.9757 - lcm_hamming_loss_k: 0.0299
Epoch 00011: val_loss improved from 0.62688 to 0.59053, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 103ms/step - loss: 0.5500 - lcm_precision_1k: 0.8095 - lcm_precision_2k: 0.6448 - lcm_precision_3k: 0.5176 - lcm_precision_5k: 0.3557 - lcm_recall_1k: 0.4670 - lcm_recall_2k: 0.6879 - lcm_recall_3k: 0.7986 - lcm_recall_5k: 0.8889 - lcm_f1_1k: 0.5923 - lcm_f1_2k: 0.6656 - lcm_f1_3k: 0.6280 - lcm_f1_5k: 0.5080 - lcm_accuracy_1k: 0.8095 - lcm_accuracy_2k: 0.9072 - lcm_accuracy_3k: 0.9505 - lcm_accuracy_5k: 0.9757 - lcm_hamming_loss_k: 0.0299 - val_loss: 0.5905 - val_lcm_precision_1k: 0.7874 - val_lcm_precision_2k: 0.6165 - val_lcm_precision_3k: 0.5050 - val_lcm_precision_5k: 0.3478 - val_lcm_recall_1k: 0.4504 - val_lcm_recall_2k: 0.6470 - val_lcm_recall_3k: 0.7655 - val_lcm_recall_5k: 0.8543 - val_lcm_f1_1k: 0.5726 - val_lcm_f1_2k: 0.6308 - val_lcm_f1_3k: 0.6080 - val_lcm_f1_5k: 0.4939 - val_lcm_accuracy_1k: 0.7874 - val_lcm_accuracy_2k: 0.8741 - val_lcm_accuracy_3k: 0.9355 - val_lcm_accuracy_5k: 0.9627 - val_lcm_hamming_loss_k: 0.0315
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5090 - lcm_precision_1k: 0.8424 - lcm_precision_2k: 0.6678 - lcm_precision_3k: 0.5290 - lcm_precision_5k: 0.3651 - lcm_recall_1k: 0.4904 - lcm_recall_2k: 0.7123 - lcm_recall_3k: 0.8161 - lcm_recall_5k: 0.9086 - lcm_f1_1k: 0.6198 - lcm_f1_2k: 0.6892 - lcm_f1_3k: 0.6418 - lcm_f1_5k: 0.5208 - lcm_accuracy_1k: 0.8424 - lcm_accuracy_2k: 0.9279 - lcm_accuracy_3k: 0.9592 - lcm_accuracy_5k: 0.9826 - lcm_hamming_loss_k: 0.0285
Epoch 00012: val_loss improved from 0.59053 to 0.55725, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.5112 - lcm_precision_1k: 0.8402 - lcm_precision_2k: 0.6669 - lcm_precision_3k: 0.5279 - lcm_precision_5k: 0.3642 - lcm_recall_1k: 0.4898 - lcm_recall_2k: 0.7115 - lcm_recall_3k: 0.8139 - lcm_recall_5k: 0.9047 - lcm_f1_1k: 0.6188 - lcm_f1_2k: 0.6884 - lcm_f1_3k: 0.6404 - lcm_f1_5k: 0.5192 - lcm_accuracy_1k: 0.8402 - lcm_accuracy_2k: 0.9296 - lcm_accuracy_3k: 0.9598 - lcm_accuracy_5k: 0.9814 - lcm_hamming_loss_k: 0.0287 - val_loss: 0.5573 - val_lcm_precision_1k: 0.8246 - val_lcm_precision_2k: 0.6517 - val_lcm_precision_3k: 0.5234 - val_lcm_precision_5k: 0.3609 - val_lcm_recall_1k: 0.4618 - val_lcm_recall_2k: 0.6729 - val_lcm_recall_3k: 0.7898 - val_lcm_recall_5k: 0.8826 - val_lcm_f1_1k: 0.5913 - val_lcm_f1_2k: 0.6610 - val_lcm_f1_3k: 0.6288 - val_lcm_f1_5k: 0.5117 - val_lcm_accuracy_1k: 0.8246 - val_lcm_accuracy_2k: 0.9001 - val_lcm_accuracy_3k: 0.9444 - val_lcm_accuracy_5k: 0.9679 - val_lcm_hamming_loss_k: 0.0300
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.4775 - lcm_precision_1k: 0.8630 - lcm_precision_2k: 0.6801 - lcm_precision_3k: 0.5394 - lcm_precision_5k: 0.3686 - lcm_recall_1k: 0.5040 - lcm_recall_2k: 0.7254 - lcm_recall_3k: 0.8315 - lcm_recall_5k: 0.9152 - lcm_f1_1k: 0.6362 - lcm_f1_2k: 0.7019 - lcm_f1_3k: 0.6542 - lcm_f1_5k: 0.5254 - lcm_accuracy_1k: 0.8630 - lcm_accuracy_2k: 0.9396 - lcm_accuracy_3k: 0.9695 - lcm_accuracy_5k: 0.9866 - lcm_hamming_loss_k: 0.0277
Epoch 00013: val_loss improved from 0.55725 to 0.53870, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.4775 - lcm_precision_1k: 0.8630 - lcm_precision_2k: 0.6801 - lcm_precision_3k: 0.5394 - lcm_precision_5k: 0.3686 - lcm_recall_1k: 0.5040 - lcm_recall_2k: 0.7254 - lcm_recall_3k: 0.8315 - lcm_recall_5k: 0.9152 - lcm_f1_1k: 0.6362 - lcm_f1_2k: 0.7019 - lcm_f1_3k: 0.6542 - lcm_f1_5k: 0.5254 - lcm_accuracy_1k: 0.8630 - lcm_accuracy_2k: 0.9396 - lcm_accuracy_3k: 0.9695 - lcm_accuracy_5k: 0.9866 - lcm_hamming_loss_k: 0.0277 - val_loss: 0.5387 - val_lcm_precision_1k: 0.8481 - val_lcm_precision_2k: 0.6614 - val_lcm_precision_3k: 0.5307 - val_lcm_precision_5k: 0.3626 - val_lcm_recall_1k: 0.4779 - val_lcm_recall_2k: 0.6873 - val_lcm_recall_3k: 0.7997 - val_lcm_recall_5k: 0.8853 - val_lcm_f1_1k: 0.6105 - val_lcm_f1_2k: 0.6731 - val_lcm_f1_3k: 0.6372 - val_lcm_f1_5k: 0.5139 - val_lcm_accuracy_1k: 0.8481 - val_lcm_accuracy_2k: 0.9117 - val_lcm_accuracy_3k: 0.9488 - val_lcm_accuracy_5k: 0.9691 - val_lcm_hamming_loss_k: 0.0290
Epoch 14/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4505 - lcm_precision_1k: 0.8793 - lcm_precision_2k: 0.6934 - lcm_precision_3k: 0.5501 - lcm_precision_5k: 0.3749 - lcm_recall_1k: 0.5140 - lcm_recall_2k: 0.7378 - lcm_recall_3k: 0.8451 - lcm_recall_5k: 0.9288 - lcm_f1_1k: 0.6487 - lcm_f1_2k: 0.7148 - lcm_f1_3k: 0.6663 - lcm_f1_5k: 0.5342 - lcm_accuracy_1k: 0.8793 - lcm_accuracy_2k: 0.9492 - lcm_accuracy_3k: 0.9744 - lcm_accuracy_5k: 0.9890 - lcm_hamming_loss_k: 0.0271
Epoch 00014: val_loss improved from 0.53870 to 0.52008, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.4510 - lcm_precision_1k: 0.8784 - lcm_precision_2k: 0.6939 - lcm_precision_3k: 0.5493 - lcm_precision_5k: 0.3740 - lcm_recall_1k: 0.5142 - lcm_recall_2k: 0.7397 - lcm_recall_3k: 0.8448 - lcm_recall_5k: 0.9272 - lcm_f1_1k: 0.6486 - lcm_f1_2k: 0.7160 - lcm_f1_3k: 0.6657 - lcm_f1_5k: 0.5329 - lcm_accuracy_1k: 0.8784 - lcm_accuracy_2k: 0.9507 - lcm_accuracy_3k: 0.9749 - lcm_accuracy_5k: 0.9894 - lcm_hamming_loss_k: 0.0271 - val_loss: 0.5201 - val_lcm_precision_1k: 0.8409 - val_lcm_precision_2k: 0.6714 - val_lcm_precision_3k: 0.5424 - val_lcm_precision_5k: 0.3670 - val_lcm_recall_1k: 0.4730 - val_lcm_recall_2k: 0.6981 - val_lcm_recall_3k: 0.8198 - val_lcm_recall_5k: 0.8936 - val_lcm_f1_1k: 0.6049 - val_lcm_f1_2k: 0.6836 - val_lcm_f1_3k: 0.6520 - val_lcm_f1_5k: 0.5197 - val_lcm_accuracy_1k: 0.8409 - val_lcm_accuracy_2k: 0.9244 - val_lcm_accuracy_3k: 0.9610 - val_lcm_accuracy_5k: 0.9721 - val_lcm_hamming_loss_k: 0.0293
Epoch 15/100
12/12 [==============================] - ETA: 0s - loss: 0.4200 - lcm_precision_1k: 0.8910 - lcm_precision_2k: 0.7105 - lcm_precision_3k: 0.5627 - lcm_precision_5k: 0.3791 - lcm_recall_1k: 0.5232 - lcm_recall_2k: 0.7597 - lcm_recall_3k: 0.8645 - lcm_recall_5k: 0.9380 - lcm_f1_1k: 0.6591 - lcm_f1_2k: 0.7342 - lcm_f1_3k: 0.6816 - lcm_f1_5k: 0.5399 - lcm_accuracy_1k: 0.8910 - lcm_accuracy_2k: 0.9636 - lcm_accuracy_3k: 0.9824 - lcm_accuracy_5k: 0.9909 - lcm_hamming_loss_k: 0.0266
Epoch 00015: val_loss did not improve from 0.52008
12/12 [==============================] - 1s 72ms/step - loss: 0.4200 - lcm_precision_1k: 0.8910 - lcm_precision_2k: 0.7105 - lcm_precision_3k: 0.5627 - lcm_precision_5k: 0.3791 - lcm_recall_1k: 0.5232 - lcm_recall_2k: 0.7597 - lcm_recall_3k: 0.8645 - lcm_recall_5k: 0.9380 - lcm_f1_1k: 0.6591 - lcm_f1_2k: 0.7342 - lcm_f1_3k: 0.6816 - lcm_f1_5k: 0.5399 - lcm_accuracy_1k: 0.8910 - lcm_accuracy_2k: 0.9636 - lcm_accuracy_3k: 0.9824 - lcm_accuracy_5k: 0.9909 - lcm_hamming_loss_k: 0.0266 - val_loss: 0.5582 - val_lcm_precision_1k: 0.8272 - val_lcm_precision_2k: 0.6637 - val_lcm_precision_3k: 0.5240 - val_lcm_precision_5k: 0.3548 - val_lcm_recall_1k: 0.4783 - val_lcm_recall_2k: 0.6996 - val_lcm_recall_3k: 0.7940 - val_lcm_recall_5k: 0.8680 - val_lcm_f1_1k: 0.6056 - val_lcm_f1_2k: 0.6805 - val_lcm_f1_3k: 0.6306 - val_lcm_f1_5k: 0.5032 - val_lcm_accuracy_1k: 0.8272 - val_lcm_accuracy_2k: 0.9144 - val_lcm_accuracy_3k: 0.9406 - val_lcm_accuracy_5k: 0.9620 - val_lcm_hamming_loss_k: 0.0299
Epoch 16/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4095 - lcm_precision_1k: 0.9027 - lcm_precision_2k: 0.7157 - lcm_precision_3k: 0.5651 - lcm_precision_5k: 0.3795 - lcm_recall_1k: 0.5322 - lcm_recall_2k: 0.7669 - lcm_recall_3k: 0.8708 - lcm_recall_5k: 0.9414 - lcm_f1_1k: 0.6695 - lcm_f1_2k: 0.7404 - lcm_f1_3k: 0.6853 - lcm_f1_5k: 0.5409 - lcm_accuracy_1k: 0.9027 - lcm_accuracy_2k: 0.9652 - lcm_accuracy_3k: 0.9851 - lcm_accuracy_5k: 0.9943 - lcm_hamming_loss_k: 0.0260
Epoch 00016: val_loss improved from 0.52008 to 0.51768, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.4087 - lcm_precision_1k: 0.9043 - lcm_precision_2k: 0.7206 - lcm_precision_3k: 0.5674 - lcm_precision_5k: 0.3821 - lcm_recall_1k: 0.5307 - lcm_recall_2k: 0.7682 - lcm_recall_3k: 0.8703 - lcm_recall_5k: 0.9429 - lcm_f1_1k: 0.6688 - lcm_f1_2k: 0.7435 - lcm_f1_3k: 0.6868 - lcm_f1_5k: 0.5438 - lcm_accuracy_1k: 0.9043 - lcm_accuracy_2k: 0.9654 - lcm_accuracy_3k: 0.9853 - lcm_accuracy_5k: 0.9943 - lcm_hamming_loss_k: 0.0261 - val_loss: 0.5177 - val_lcm_precision_1k: 0.8491 - val_lcm_precision_2k: 0.6732 - val_lcm_precision_3k: 0.5380 - val_lcm_precision_5k: 0.3647 - val_lcm_recall_1k: 0.4816 - val_lcm_recall_2k: 0.7007 - val_lcm_recall_3k: 0.8146 - val_lcm_recall_5k: 0.8907 - val_lcm_f1_1k: 0.6140 - val_lcm_f1_2k: 0.6858 - val_lcm_f1_3k: 0.6472 - val_lcm_f1_5k: 0.5169 - val_lcm_accuracy_1k: 0.8491 - val_lcm_accuracy_2k: 0.9216 - val_lcm_accuracy_3k: 0.9584 - val_lcm_accuracy_5k: 0.9747 - val_lcm_hamming_loss_k: 0.0290
Epoch 17/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3899 - lcm_precision_1k: 0.9183 - lcm_precision_2k: 0.7207 - lcm_precision_3k: 0.5724 - lcm_precision_5k: 0.3841 - lcm_recall_1k: 0.5423 - lcm_recall_2k: 0.7693 - lcm_recall_3k: 0.8766 - lcm_recall_5k: 0.9472 - lcm_f1_1k: 0.6819 - lcm_f1_2k: 0.7442 - lcm_f1_3k: 0.6926 - lcm_f1_5k: 0.5465 - lcm_accuracy_1k: 0.9183 - lcm_accuracy_2k: 0.9680 - lcm_accuracy_3k: 0.9872 - lcm_accuracy_5k: 0.9954 - lcm_hamming_loss_k: 0.0256
Epoch 00017: val_loss did not improve from 0.51768
12/12 [==============================] - 1s 69ms/step - loss: 0.3896 - lcm_precision_1k: 0.9170 - lcm_precision_2k: 0.7200 - lcm_precision_3k: 0.5716 - lcm_precision_5k: 0.3838 - lcm_recall_1k: 0.5413 - lcm_recall_2k: 0.7693 - lcm_recall_3k: 0.8768 - lcm_recall_5k: 0.9480 - lcm_f1_1k: 0.6807 - lcm_f1_2k: 0.7438 - lcm_f1_3k: 0.6920 - lcm_f1_5k: 0.5464 - lcm_accuracy_1k: 0.9170 - lcm_accuracy_2k: 0.9663 - lcm_accuracy_3k: 0.9867 - lcm_accuracy_5k: 0.9952 - lcm_hamming_loss_k: 0.0255 - val_loss: 0.5185 - val_lcm_precision_1k: 0.8347 - val_lcm_precision_2k: 0.6668 - val_lcm_precision_3k: 0.5366 - val_lcm_precision_5k: 0.3629 - val_lcm_recall_1k: 0.4784 - val_lcm_recall_2k: 0.7014 - val_lcm_recall_3k: 0.8103 - val_lcm_recall_5k: 0.8856 - val_lcm_f1_1k: 0.6078 - val_lcm_f1_2k: 0.6830 - val_lcm_f1_3k: 0.6449 - val_lcm_f1_5k: 0.5142 - val_lcm_accuracy_1k: 0.8347 - val_lcm_accuracy_2k: 0.9280 - val_lcm_accuracy_3k: 0.9462 - val_lcm_accuracy_5k: 0.9691 - val_lcm_hamming_loss_k: 0.0296
Epoch 18/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3713 - lcm_precision_1k: 0.9251 - lcm_precision_2k: 0.7324 - lcm_precision_3k: 0.5788 - lcm_precision_5k: 0.3864 - lcm_recall_1k: 0.5488 - lcm_recall_2k: 0.7821 - lcm_recall_3k: 0.8859 - lcm_recall_5k: 0.9525 - lcm_f1_1k: 0.6888 - lcm_f1_2k: 0.7564 - lcm_f1_3k: 0.7001 - lcm_f1_5k: 0.5497 - lcm_accuracy_1k: 0.9251 - lcm_accuracy_2k: 0.9741 - lcm_accuracy_3k: 0.9897 - lcm_accuracy_5k: 0.9961 - lcm_hamming_loss_k: 0.0252
Epoch 00018: val_loss improved from 0.51768 to 0.49324, saving model to logs/ufzmvj-labs-0602-221827/model/checkpoint_labs.h5
12/12 [==============================] - 1s 105ms/step - loss: 0.3717 - lcm_precision_1k: 0.9226 - lcm_precision_2k: 0.7337 - lcm_precision_3k: 0.5789 - lcm_precision_5k: 0.3870 - lcm_recall_1k: 0.5453 - lcm_recall_2k: 0.7813 - lcm_recall_3k: 0.8844 - lcm_recall_5k: 0.9526 - lcm_f1_1k: 0.6853 - lcm_f1_2k: 0.7567 - lcm_f1_3k: 0.6997 - lcm_f1_5k: 0.5503 - lcm_accuracy_1k: 0.9226 - lcm_accuracy_2k: 0.9741 - lcm_accuracy_3k: 0.9900 - lcm_accuracy_5k: 0.9964 - lcm_hamming_loss_k: 0.0253 - val_loss: 0.4932 - val_lcm_precision_1k: 0.8724 - val_lcm_precision_2k: 0.6929 - val_lcm_precision_3k: 0.5508 - val_lcm_precision_5k: 0.3702 - val_lcm_recall_1k: 0.4948 - val_lcm_recall_2k: 0.7246 - val_lcm_recall_3k: 0.8351 - val_lcm_recall_5k: 0.9022 - val_lcm_f1_1k: 0.6309 - val_lcm_f1_2k: 0.7076 - val_lcm_f1_3k: 0.6629 - val_lcm_f1_5k: 0.5243 - val_lcm_accuracy_1k: 0.8724 - val_lcm_accuracy_2k: 0.9361 - val_lcm_accuracy_3k: 0.9649 - val_lcm_accuracy_5k: 0.9730 - val_lcm_hamming_loss_k: 0.0280
Epoch 19/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3475 - lcm_precision_1k: 0.9318 - lcm_precision_2k: 0.7472 - lcm_precision_3k: 0.5873 - lcm_precision_5k: 0.3920 - lcm_recall_1k: 0.5489 - lcm_recall_2k: 0.7938 - lcm_recall_3k: 0.8942 - lcm_recall_5k: 0.9601 - lcm_f1_1k: 0.6907 - lcm_f1_2k: 0.7697 - lcm_f1_3k: 0.7089 - lcm_f1_5k: 0.5567 - lcm_accuracy_1k: 0.9318 - lcm_accuracy_2k: 0.9769 - lcm_accuracy_3k: 0.9911 - lcm_accuracy_5k: 0.9954 - lcm_hamming_loss_k: 0.0250
Epoch 00019: val_loss did not improve from 0.49324
12/12 [==============================] - 1s 70ms/step - loss: 0.3497 - lcm_precision_1k: 0.9266 - lcm_precision_2k: 0.7432 - lcm_precision_3k: 0.5841 - lcm_precision_5k: 0.3900 - lcm_recall_1k: 0.5477 - lcm_recall_2k: 0.7928 - lcm_recall_3k: 0.8933 - lcm_recall_5k: 0.9593 - lcm_f1_1k: 0.6883 - lcm_f1_2k: 0.7671 - lcm_f1_3k: 0.7063 - lcm_f1_5k: 0.5544 - lcm_accuracy_1k: 0.9266 - lcm_accuracy_2k: 0.9761 - lcm_accuracy_3k: 0.9908 - lcm_accuracy_5k: 0.9952 - lcm_hamming_loss_k: 0.0251 - val_loss: 0.5106 - val_lcm_precision_1k: 0.8523 - val_lcm_precision_2k: 0.6763 - val_lcm_precision_3k: 0.5345 - val_lcm_precision_5k: 0.3638 - val_lcm_recall_1k: 0.4898 - val_lcm_recall_2k: 0.7078 - val_lcm_recall_3k: 0.8066 - val_lcm_recall_5k: 0.8870 - val_lcm_f1_1k: 0.6216 - val_lcm_f1_2k: 0.6909 - val_lcm_f1_3k: 0.6421 - val_lcm_f1_5k: 0.5154 - val_lcm_accuracy_1k: 0.8523 - val_lcm_accuracy_2k: 0.9308 - val_lcm_accuracy_3k: 0.9513 - val_lcm_accuracy_5k: 0.9703 - val_lcm_hamming_loss_k: 0.0288
Epoch 20/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3538 - lcm_precision_1k: 0.9283 - lcm_precision_2k: 0.7408 - lcm_precision_3k: 0.5857 - lcm_precision_5k: 0.3905 - lcm_recall_1k: 0.5482 - lcm_recall_2k: 0.7900 - lcm_recall_3k: 0.8940 - lcm_recall_5k: 0.9580 - lcm_f1_1k: 0.6892 - lcm_f1_2k: 0.7645 - lcm_f1_3k: 0.7076 - lcm_f1_5k: 0.5547 - lcm_accuracy_1k: 0.9283 - lcm_accuracy_2k: 0.9812 - lcm_accuracy_3k: 0.9936 - lcm_accuracy_5k: 0.9961 - lcm_hamming_loss_k: 0.0252
Epoch 00020: val_loss did not improve from 0.49324
12/12 [==============================] - 1s 69ms/step - loss: 0.3531 - lcm_precision_1k: 0.9299 - lcm_precision_2k: 0.7362 - lcm_precision_3k: 0.5844 - lcm_precision_5k: 0.3892 - lcm_recall_1k: 0.5522 - lcm_recall_2k: 0.7887 - lcm_recall_3k: 0.8954 - lcm_recall_5k: 0.9582 - lcm_f1_1k: 0.6928 - lcm_f1_2k: 0.7614 - lcm_f1_3k: 0.7071 - lcm_f1_5k: 0.5535 - lcm_accuracy_1k: 0.9299 - lcm_accuracy_2k: 0.9811 - lcm_accuracy_3k: 0.9941 - lcm_accuracy_5k: 0.9964 - lcm_hamming_loss_k: 0.0249 - val_loss: 0.5218 - val_lcm_precision_1k: 0.8516 - val_lcm_precision_2k: 0.6796 - val_lcm_precision_3k: 0.5342 - val_lcm_precision_5k: 0.3596 - val_lcm_recall_1k: 0.4878 - val_lcm_recall_2k: 0.7122 - val_lcm_recall_3k: 0.8078 - val_lcm_recall_5k: 0.8806 - val_lcm_f1_1k: 0.6198 - val_lcm_f1_2k: 0.6947 - val_lcm_f1_3k: 0.6424 - val_lcm_f1_5k: 0.5101 - val_lcm_accuracy_1k: 0.8516 - val_lcm_accuracy_2k: 0.9322 - val_lcm_accuracy_3k: 0.9488 - val_lcm_accuracy_5k: 0.9650 - val_lcm_hamming_loss_k: 0.0289
Epoch 00020: early stopping
39/39 [==============================] - 1s 21ms/step - loss: 0.4136 - lcm_precision_1k: 0.9236 - lcm_precision_2k: 0.7292 - lcm_precision_3k: 0.5730 - lcm_precision_5k: 0.3843 - lcm_recall_1k: 0.5265 - lcm_recall_2k: 0.7604 - lcm_recall_3k: 0.8633 - lcm_recall_5k: 0.9325 - lcm_f1_1k: 0.6697 - lcm_f1_2k: 0.7434 - lcm_f1_3k: 0.6878 - lcm_f1_5k: 0.5436 - lcm_accuracy_1k: 0.9236 - lcm_accuracy_2k: 0.9676 - lcm_accuracy_3k: 0.9880 - lcm_accuracy_5k: 0.9952 - lcm_hamming_loss_k: 0.0262
Best model result:  [0.4135601818561554, 0.9235796928405762, 0.7292384505271912, 0.5730154514312744, 0.3843333125114441, 0.5264948010444641, 0.7603538632392883, 0.8633204698562622, 0.9324718117713928, 0.6697353720664978, 0.7433788776397705, 0.6878405809402466, 0.5436285734176636, 0.9235796928405762, 0.9675999283790588, 0.9879896640777588, 0.9951999187469482, 0.02617768757045269]
2969
743
1238
Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_2[0][0
                                                                 ]']                              
                                                                                                  
 permute_1 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_1[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_2[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_3 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_3[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Model: "model_3"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_2[0][0
                                                                 ]']                              
                                                                                                  
 permute_1 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_1[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_2[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_3 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_3[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 49, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 dot_1 (Dot)                    (None, 49)           0           ['label_lcm_emb[0][0]',          
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 49)           2450        ['dot_1[0][0]']                  
                                                                                                  
 concatenate_1 (Concatenate)    (None, 98)           0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 5,092,791
Trainable params: 4,028,391
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 1.5804 - lcm_precision_1k: 0.0860 - lcm_precision_2k: 0.1293 - lcm_precision_3k: 0.1162 - lcm_precision_5k: 0.1094 - lcm_recall_1k: 0.0457 - lcm_recall_2k: 0.1357 - lcm_recall_3k: 0.1785 - lcm_recall_5k: 0.2778 - lcm_f1_1k: 0.0594 - lcm_f1_2k: 0.1322 - lcm_f1_3k: 0.1407 - lcm_f1_5k: 0.1570 - lcm_accuracy_1k: 0.0860 - lcm_accuracy_2k: 0.2578 - lcm_accuracy_3k: 0.3317 - lcm_accuracy_5k: 0.4741 - lcm_hamming_loss_k: 0.1198 ETA: 0s - loss: 1.6272 - lcm_precision_1k: 0.1111 - lcm_precision_2k: 0.1387 - lcm_precision_3k: 0.1246 - lcm_precision_5k: 0.1144 - lcm_recall_1k: 0.0616 - lcm_recall_2k: 0.1493 - lcm_recall_3k: 0.1943 - lcm_recall_5k: 0.2876 - lcm_f1_1k: 0.0790 - lcm_f1_2k: 0.1436 - lcm_f1_3k: 0.1518 - lcm_f1_5k: 0.1636 - lcm_accuracy_1k: 0.1111 - lcm_accuracy_2k: 0.2762 - lcm_accuracy_3k: 0.3499 - lcm_accuracy_5k: 0.4827 - lcm_hamming_loss_k: 
Epoch 00001: val_loss improved from inf to 1.51221, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 3s 115ms/step - loss: 1.5760 - lcm_precision_1k: 0.0826 - lcm_precision_2k: 0.1218 - lcm_precision_3k: 0.1100 - lcm_precision_5k: 0.1080 - lcm_recall_1k: 0.0442 - lcm_recall_2k: 0.1277 - lcm_recall_3k: 0.1700 - lcm_recall_5k: 0.2755 - lcm_f1_1k: 0.0573 - lcm_f1_2k: 0.1245 - lcm_f1_3k: 0.1334 - lcm_f1_5k: 0.1550 - lcm_accuracy_1k: 0.0826 - lcm_accuracy_2k: 0.2423 - lcm_accuracy_3k: 0.3128 - lcm_accuracy_5k: 0.4705 - lcm_hamming_loss_k: 0.1213 - val_loss: 1.5122 - val_lcm_precision_1k: 0.0381 - val_lcm_precision_2k: 0.0415 - val_lcm_precision_3k: 0.0466 - val_lcm_precision_5k: 0.0985 - val_lcm_recall_1k: 0.0190 - val_lcm_recall_2k: 0.0414 - val_lcm_recall_3k: 0.0796 - val_lcm_recall_5k: 0.2568 - val_lcm_f1_1k: nan - val_lcm_f1_2k: 0.0413 - val_lcm_f1_3k: 0.0588 - val_lcm_f1_5k: 0.1424 - val_lcm_accuracy_1k: 0.0381 - val_lcm_accuracy_2k: 0.0802 - val_lcm_accuracy_3k: 0.1312 - val_lcm_accuracy_5k: 0.4712 - val_lcm_hamming_loss_k: 0.1343
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4845 - lcm_precision_1k: 0.0540 - lcm_precision_2k: 0.1555 - lcm_precision_3k: 0.1278 - lcm_precision_5k: 0.1041 - lcm_recall_1k: 0.0300 - lcm_recall_2k: 0.1690 - lcm_recall_3k: 0.2084 - lcm_recall_5k: 0.2836 - lcm_f1_1k: 0.0384 - lcm_f1_2k: 0.1617 - lcm_f1_3k: 0.1583 - lcm_f1_5k: 0.1522 - lcm_accuracy_1k: 0.0540 - lcm_accuracy_2k: 0.3100 - lcm_accuracy_3k: 0.3718 - lcm_accuracy_5k: 0.4911 - lcm_hamming_loss_k: 0.0921
Epoch 00002: val_loss improved from 1.51221 to 1.46093, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 105ms/step - loss: 1.4829 - lcm_precision_1k: 0.0565 - lcm_precision_2k: 0.1578 - lcm_precision_3k: 0.1286 - lcm_precision_5k: 0.1040 - lcm_recall_1k: 0.0325 - lcm_recall_2k: 0.1731 - lcm_recall_3k: 0.2116 - lcm_recall_5k: 0.2859 - lcm_f1_1k: 0.0411 - lcm_f1_2k: 0.1648 - lcm_f1_3k: 0.1598 - lcm_f1_5k: 0.1524 - lcm_accuracy_1k: 0.0565 - lcm_accuracy_2k: 0.3142 - lcm_accuracy_3k: 0.3746 - lcm_accuracy_5k: 0.4916 - lcm_hamming_loss_k: 0.0893 - val_loss: 1.4609 - val_lcm_precision_1k: 0.0773 - val_lcm_precision_2k: 0.1819 - val_lcm_precision_3k: 0.1475 - val_lcm_precision_5k: 0.1060 - val_lcm_recall_1k: 0.0501 - val_lcm_recall_2k: 0.2035 - val_lcm_recall_3k: 0.2405 - val_lcm_recall_5k: 0.2802 - val_lcm_f1_1k: 0.0602 - val_lcm_f1_2k: 0.1921 - val_lcm_f1_3k: 0.1829 - val_lcm_f1_5k: 0.1538 - val_lcm_accuracy_1k: 0.0773 - val_lcm_accuracy_2k: 0.3624 - val_lcm_accuracy_3k: 0.4354 - val_lcm_accuracy_5k: 0.5082 - val_lcm_hamming_loss_k: 0.0601
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4166 - lcm_precision_1k: 0.0778 - lcm_precision_2k: 0.1616 - lcm_precision_3k: 0.1368 - lcm_precision_5k: 0.1105 - lcm_recall_1k: 0.0496 - lcm_recall_2k: 0.1828 - lcm_recall_3k: 0.2326 - lcm_recall_5k: 0.3026 - lcm_f1_1k: 0.0604 - lcm_f1_2k: 0.1714 - lcm_f1_3k: 0.1722 - lcm_f1_5k: 0.1619 - lcm_accuracy_1k: 0.0778 - lcm_accuracy_2k: 0.3203 - lcm_accuracy_3k: 0.3988 - lcm_accuracy_5k: 0.5071 - lcm_hamming_loss_k: 0.0598
Epoch 00003: val_loss improved from 1.46093 to 1.36502, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 1.4141 - lcm_precision_1k: 0.0784 - lcm_precision_2k: 0.1645 - lcm_precision_3k: 0.1419 - lcm_precision_5k: 0.1157 - lcm_recall_1k: 0.0490 - lcm_recall_2k: 0.1846 - lcm_recall_3k: 0.2378 - lcm_recall_5k: 0.3137 - lcm_f1_1k: 0.0601 - lcm_f1_2k: 0.1738 - lcm_f1_3k: 0.1776 - lcm_f1_5k: 0.1690 - lcm_accuracy_1k: 0.0784 - lcm_accuracy_2k: 0.3252 - lcm_accuracy_3k: 0.4091 - lcm_accuracy_5k: 0.5182 - lcm_hamming_loss_k: 0.0599 - val_loss: 1.3650 - val_lcm_precision_1k: 0.0734 - val_lcm_precision_2k: 0.1677 - val_lcm_precision_3k: 0.1858 - val_lcm_precision_5k: 0.1562 - val_lcm_recall_1k: 0.0469 - val_lcm_recall_2k: 0.1897 - val_lcm_recall_3k: 0.3056 - val_lcm_recall_5k: 0.4128 - val_lcm_f1_1k: 0.0566 - val_lcm_f1_2k: 0.1778 - val_lcm_f1_3k: 0.2310 - val_lcm_f1_5k: 0.2265 - val_lcm_accuracy_1k: 0.0734 - val_lcm_accuracy_2k: 0.3274 - val_lcm_accuracy_3k: 0.5111 - val_lcm_accuracy_5k: 0.6135 - val_lcm_hamming_loss_k: 0.0603
Epoch 4/100
12/12 [==============================] - ETA: 0s - loss: 1.2948 - lcm_precision_1k: 0.2308 - lcm_precision_2k: 0.2145 - lcm_precision_3k: 0.2029 - lcm_precision_5k: 0.1730 - lcm_recall_1k: 0.1320 - lcm_recall_2k: 0.2368 - lcm_recall_3k: 0.3284 - lcm_recall_5k: 0.4533 - lcm_f1_1k: 0.1678 - lcm_f1_2k: 0.2249 - lcm_f1_3k: 0.2507 - lcm_f1_5k: 0.2504 - lcm_accuracy_1k: 0.2308 - lcm_accuracy_2k: 0.4113 - lcm_accuracy_3k: 0.5405 - lcm_accuracy_5k: 0.6514 - lcm_hamming_loss_k: 0.0537
Epoch 00004: val_loss improved from 1.36502 to 1.22318, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 1.2948 - lcm_precision_1k: 0.2308 - lcm_precision_2k: 0.2145 - lcm_precision_3k: 0.2029 - lcm_precision_5k: 0.1730 - lcm_recall_1k: 0.1320 - lcm_recall_2k: 0.2368 - lcm_recall_3k: 0.3284 - lcm_recall_5k: 0.4533 - lcm_f1_1k: 0.1678 - lcm_f1_2k: 0.2249 - lcm_f1_3k: 0.2507 - lcm_f1_5k: 0.2504 - lcm_accuracy_1k: 0.2308 - lcm_accuracy_2k: 0.4113 - lcm_accuracy_3k: 0.5405 - lcm_accuracy_5k: 0.6514 - lcm_hamming_loss_k: 0.0537 - val_loss: 1.2232 - val_lcm_precision_1k: 0.3771 - val_lcm_precision_2k: 0.2543 - val_lcm_precision_3k: 0.1935 - val_lcm_precision_5k: 0.1781 - val_lcm_recall_1k: 0.1875 - val_lcm_recall_2k: 0.2647 - val_lcm_recall_3k: 0.3068 - val_lcm_recall_5k: 0.4452 - val_lcm_f1_1k: 0.2502 - val_lcm_f1_2k: 0.2588 - val_lcm_f1_3k: 0.2368 - val_lcm_f1_5k: 0.2541 - val_lcm_accuracy_1k: 0.3771 - val_lcm_accuracy_2k: 0.4990 - val_lcm_accuracy_3k: 0.5486 - val_lcm_accuracy_5k: 0.6931 - val_lcm_hamming_loss_k: 0.0479
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 1.1397 - lcm_precision_1k: 0.4062 - lcm_precision_2k: 0.3079 - lcm_precision_3k: 0.2538 - lcm_precision_5k: 0.2189 - lcm_recall_1k: 0.2234 - lcm_recall_2k: 0.3331 - lcm_recall_3k: 0.4039 - lcm_recall_5k: 0.5582 - lcm_f1_1k: 0.2879 - lcm_f1_2k: 0.3198 - lcm_f1_3k: 0.3117 - lcm_f1_5k: 0.3144 - lcm_accuracy_1k: 0.4062 - lcm_accuracy_2k: 0.5774 - lcm_accuracy_3k: 0.6602 - lcm_accuracy_5k: 0.7752 - lcm_hamming_loss_k: 0.0465
Epoch 00005: val_loss improved from 1.22318 to 1.07251, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 1.1337 - lcm_precision_1k: 0.4089 - lcm_precision_2k: 0.3106 - lcm_precision_3k: 0.2573 - lcm_precision_5k: 0.2231 - lcm_recall_1k: 0.2241 - lcm_recall_2k: 0.3327 - lcm_recall_3k: 0.4049 - lcm_recall_5k: 0.5664 - lcm_f1_1k: 0.2892 - lcm_f1_2k: 0.3210 - lcm_f1_3k: 0.3145 - lcm_f1_5k: 0.3200 - lcm_accuracy_1k: 0.4089 - lcm_accuracy_2k: 0.5761 - lcm_accuracy_3k: 0.6596 - lcm_accuracy_5k: 0.7809 - lcm_hamming_loss_k: 0.0463 - val_loss: 1.0725 - val_lcm_precision_1k: 0.4335 - val_lcm_precision_2k: 0.3516 - val_lcm_precision_3k: 0.3120 - val_lcm_precision_5k: 0.2556 - val_lcm_recall_1k: 0.2213 - val_lcm_recall_2k: 0.3486 - val_lcm_recall_3k: 0.4615 - val_lcm_recall_5k: 0.6214 - val_lcm_f1_1k: 0.2930 - val_lcm_f1_2k: 0.3499 - val_lcm_f1_3k: 0.3722 - val_lcm_f1_5k: 0.3622 - val_lcm_accuracy_1k: 0.4335 - val_lcm_accuracy_2k: 0.5435 - val_lcm_accuracy_3k: 0.6499 - val_lcm_accuracy_5k: 0.7877 - val_lcm_hamming_loss_k: 0.0456
Epoch 6/100
12/12 [==============================] - ETA: 0s - loss: 0.9707 - lcm_precision_1k: 0.5142 - lcm_precision_2k: 0.4018 - lcm_precision_3k: 0.3432 - lcm_precision_5k: 0.2714 - lcm_recall_1k: 0.2851 - lcm_recall_2k: 0.4259 - lcm_recall_3k: 0.5338 - lcm_recall_5k: 0.6813 - lcm_f1_1k: 0.3666 - lcm_f1_2k: 0.4134 - lcm_f1_3k: 0.4178 - lcm_f1_5k: 0.3881 - lcm_accuracy_1k: 0.5142 - lcm_accuracy_2k: 0.6719 - lcm_accuracy_3k: 0.7717 - lcm_accuracy_5k: 0.8590 - lcm_hamming_loss_k: 0.0422
Epoch 00006: val_loss improved from 1.07251 to 0.87807, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.9707 - lcm_precision_1k: 0.5142 - lcm_precision_2k: 0.4018 - lcm_precision_3k: 0.3432 - lcm_precision_5k: 0.2714 - lcm_recall_1k: 0.2851 - lcm_recall_2k: 0.4259 - lcm_recall_3k: 0.5338 - lcm_recall_5k: 0.6813 - lcm_f1_1k: 0.3666 - lcm_f1_2k: 0.4134 - lcm_f1_3k: 0.4178 - lcm_f1_5k: 0.3881 - lcm_accuracy_1k: 0.5142 - lcm_accuracy_2k: 0.6719 - lcm_accuracy_3k: 0.7717 - lcm_accuracy_5k: 0.8590 - lcm_hamming_loss_k: 0.0422 - val_loss: 0.8781 - val_lcm_precision_1k: 0.6055 - val_lcm_precision_2k: 0.4797 - val_lcm_precision_3k: 0.3970 - val_lcm_precision_5k: 0.2966 - val_lcm_recall_1k: 0.3375 - val_lcm_recall_2k: 0.5014 - val_lcm_recall_3k: 0.6063 - val_lcm_recall_5k: 0.7389 - val_lcm_f1_1k: 0.4333 - val_lcm_f1_2k: 0.4900 - val_lcm_f1_3k: 0.4795 - val_lcm_f1_5k: 0.4230 - val_lcm_accuracy_1k: 0.6055 - val_lcm_accuracy_2k: 0.7733 - val_lcm_accuracy_3k: 0.8287 - val_lcm_accuracy_5k: 0.9129 - val_lcm_hamming_loss_k: 0.0386
Epoch 7/100
12/12 [==============================] - ETA: 0s - loss: 0.7963 - lcm_precision_1k: 0.6406 - lcm_precision_2k: 0.5130 - lcm_precision_3k: 0.4197 - lcm_precision_5k: 0.3089 - lcm_recall_1k: 0.3603 - lcm_recall_2k: 0.5496 - lcm_recall_3k: 0.6580 - lcm_recall_5k: 0.7784 - lcm_f1_1k: 0.4610 - lcm_f1_2k: 0.5306 - lcm_f1_3k: 0.5124 - lcm_f1_5k: 0.4422 - lcm_accuracy_1k: 0.6406 - lcm_accuracy_2k: 0.7942 - lcm_accuracy_3k: 0.8628 - lcm_accuracy_5k: 0.9177 - lcm_hamming_loss_k: 0.0369
Epoch 00007: val_loss improved from 0.87807 to 0.74651, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.7963 - lcm_precision_1k: 0.6406 - lcm_precision_2k: 0.5130 - lcm_precision_3k: 0.4197 - lcm_precision_5k: 0.3089 - lcm_recall_1k: 0.3603 - lcm_recall_2k: 0.5496 - lcm_recall_3k: 0.6580 - lcm_recall_5k: 0.7784 - lcm_f1_1k: 0.4610 - lcm_f1_2k: 0.5306 - lcm_f1_3k: 0.5124 - lcm_f1_5k: 0.4422 - lcm_accuracy_1k: 0.6406 - lcm_accuracy_2k: 0.7942 - lcm_accuracy_3k: 0.8628 - lcm_accuracy_5k: 0.9177 - lcm_hamming_loss_k: 0.0369 - val_loss: 0.7465 - val_lcm_precision_1k: 0.6841 - val_lcm_precision_2k: 0.5732 - val_lcm_precision_3k: 0.4542 - val_lcm_precision_5k: 0.3232 - val_lcm_recall_1k: 0.3703 - val_lcm_recall_2k: 0.5932 - val_lcm_recall_3k: 0.6908 - val_lcm_recall_5k: 0.8004 - val_lcm_f1_1k: 0.4801 - val_lcm_f1_2k: 0.5826 - val_lcm_f1_3k: 0.5478 - val_lcm_f1_5k: 0.4602 - val_lcm_accuracy_1k: 0.6841 - val_lcm_accuracy_2k: 0.8213 - val_lcm_accuracy_3k: 0.8723 - val_lcm_accuracy_5k: 0.9282 - val_lcm_hamming_loss_k: 0.0354
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6960 - lcm_precision_1k: 0.7230 - lcm_precision_2k: 0.5728 - lcm_precision_3k: 0.4581 - lcm_precision_5k: 0.3273 - lcm_recall_1k: 0.4144 - lcm_recall_2k: 0.6154 - lcm_recall_3k: 0.7169 - lcm_recall_5k: 0.8232 - lcm_f1_1k: 0.5267 - lcm_f1_2k: 0.5932 - lcm_f1_3k: 0.5589 - lcm_f1_5k: 0.4683 - lcm_accuracy_1k: 0.7230 - lcm_accuracy_2k: 0.8526 - lcm_accuracy_3k: 0.9041 - lcm_accuracy_5k: 0.9464 - lcm_hamming_loss_k: 0.0336
Epoch 00008: val_loss improved from 0.74651 to 0.67334, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.6939 - lcm_precision_1k: 0.7281 - lcm_precision_2k: 0.5752 - lcm_precision_3k: 0.4593 - lcm_precision_5k: 0.3272 - lcm_recall_1k: 0.4183 - lcm_recall_2k: 0.6184 - lcm_recall_3k: 0.7192 - lcm_recall_5k: 0.8235 - lcm_f1_1k: 0.5312 - lcm_f1_2k: 0.5959 - lcm_f1_3k: 0.5605 - lcm_f1_5k: 0.4682 - lcm_accuracy_1k: 0.7281 - lcm_accuracy_2k: 0.8540 - lcm_accuracy_3k: 0.9056 - lcm_accuracy_5k: 0.9459 - lcm_hamming_loss_k: 0.0333 - val_loss: 0.6733 - val_lcm_precision_1k: 0.7592 - val_lcm_precision_2k: 0.6012 - val_lcm_precision_3k: 0.4771 - val_lcm_precision_5k: 0.3365 - val_lcm_recall_1k: 0.4185 - val_lcm_recall_2k: 0.6243 - val_lcm_recall_3k: 0.7239 - val_lcm_recall_5k: 0.8229 - val_lcm_f1_1k: 0.5390 - val_lcm_f1_2k: 0.6118 - val_lcm_f1_3k: 0.5747 - val_lcm_f1_5k: 0.4774 - val_lcm_accuracy_1k: 0.7592 - val_lcm_accuracy_2k: 0.8516 - val_lcm_accuracy_3k: 0.9011 - val_lcm_accuracy_5k: 0.9409 - val_lcm_hamming_loss_k: 0.0323
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6214 - lcm_precision_1k: 0.7720 - lcm_precision_2k: 0.6074 - lcm_precision_3k: 0.4838 - lcm_precision_5k: 0.3425 - lcm_recall_1k: 0.4464 - lcm_recall_2k: 0.6517 - lcm_recall_3k: 0.7546 - lcm_recall_5k: 0.8584 - lcm_f1_1k: 0.5655 - lcm_f1_2k: 0.6287 - lcm_f1_3k: 0.5895 - lcm_f1_5k: 0.4896 - lcm_accuracy_1k: 0.7720 - lcm_accuracy_2k: 0.8842 - lcm_accuracy_3k: 0.9304 - lcm_accuracy_5k: 0.9663 - lcm_hamming_loss_k: 0.0315
Epoch 00009: val_loss improved from 0.67334 to 0.64185, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.6198 - lcm_precision_1k: 0.7730 - lcm_precision_2k: 0.6088 - lcm_precision_3k: 0.4841 - lcm_precision_5k: 0.3425 - lcm_recall_1k: 0.4479 - lcm_recall_2k: 0.6535 - lcm_recall_3k: 0.7550 - lcm_recall_5k: 0.8586 - lcm_f1_1k: 0.5670 - lcm_f1_2k: 0.6303 - lcm_f1_3k: 0.5899 - lcm_f1_5k: 0.4896 - lcm_accuracy_1k: 0.7730 - lcm_accuracy_2k: 0.8846 - lcm_accuracy_3k: 0.9302 - lcm_accuracy_5k: 0.9664 - lcm_hamming_loss_k: 0.0315 - val_loss: 0.6418 - val_lcm_precision_1k: 0.7679 - val_lcm_precision_2k: 0.6231 - val_lcm_precision_3k: 0.4858 - val_lcm_precision_5k: 0.3391 - val_lcm_recall_1k: 0.4265 - val_lcm_recall_2k: 0.6436 - val_lcm_recall_3k: 0.7345 - val_lcm_recall_5k: 0.8304 - val_lcm_f1_1k: 0.5477 - val_lcm_f1_2k: 0.6325 - val_lcm_f1_3k: 0.5844 - val_lcm_f1_5k: 0.4813 - val_lcm_accuracy_1k: 0.7679 - val_lcm_accuracy_2k: 0.8792 - val_lcm_accuracy_3k: 0.9147 - val_lcm_accuracy_5k: 0.9462 - val_lcm_hamming_loss_k: 0.0319
Epoch 10/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5707 - lcm_precision_1k: 0.8018 - lcm_precision_2k: 0.6316 - lcm_precision_3k: 0.5091 - lcm_precision_5k: 0.3533 - lcm_recall_1k: 0.4647 - lcm_recall_2k: 0.6751 - lcm_recall_3k: 0.7873 - lcm_recall_5k: 0.8818 - lcm_f1_1k: 0.5883 - lcm_f1_2k: 0.6525 - lcm_f1_3k: 0.6183 - lcm_f1_5k: 0.5044 - lcm_accuracy_1k: 0.8018 - lcm_accuracy_2k: 0.9052 - lcm_accuracy_3k: 0.9464 - lcm_accuracy_5k: 0.9773 - lcm_hamming_loss_k: 0.0305
Epoch 00010: val_loss improved from 0.64185 to 0.63641, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.5696 - lcm_precision_1k: 0.8015 - lcm_precision_2k: 0.6307 - lcm_precision_3k: 0.5079 - lcm_precision_5k: 0.3518 - lcm_recall_1k: 0.4664 - lcm_recall_2k: 0.6774 - lcm_recall_3k: 0.7886 - lcm_recall_5k: 0.8817 - lcm_f1_1k: 0.5896 - lcm_f1_2k: 0.6531 - lcm_f1_3k: 0.6177 - lcm_f1_5k: 0.5029 - lcm_accuracy_1k: 0.8015 - lcm_accuracy_2k: 0.9038 - lcm_accuracy_3k: 0.9459 - lcm_accuracy_5k: 0.9754 - lcm_hamming_loss_k: 0.0302 - val_loss: 0.6364 - val_lcm_precision_1k: 0.7737 - val_lcm_precision_2k: 0.6137 - val_lcm_precision_3k: 0.5004 - val_lcm_precision_5k: 0.3434 - val_lcm_recall_1k: 0.4311 - val_lcm_recall_2k: 0.6357 - val_lcm_recall_3k: 0.7559 - val_lcm_recall_5k: 0.8419 - val_lcm_f1_1k: 0.5532 - val_lcm_f1_2k: 0.6240 - val_lcm_f1_3k: 0.6017 - val_lcm_f1_5k: 0.4876 - val_lcm_accuracy_1k: 0.7737 - val_lcm_accuracy_2k: 0.8682 - val_lcm_accuracy_3k: 0.9227 - val_lcm_accuracy_5k: 0.9514 - val_lcm_hamming_loss_k: 0.0317
Epoch 11/100
12/12 [==============================] - ETA: 0s - loss: 0.5291 - lcm_precision_1k: 0.8221 - lcm_precision_2k: 0.6540 - lcm_precision_3k: 0.5207 - lcm_precision_5k: 0.3609 - lcm_recall_1k: 0.4771 - lcm_recall_2k: 0.7016 - lcm_recall_3k: 0.8070 - lcm_recall_5k: 0.8999 - lcm_f1_1k: 0.6037 - lcm_f1_2k: 0.6768 - lcm_f1_3k: 0.6328 - lcm_f1_5k: 0.5151 - lcm_accuracy_1k: 0.8221 - lcm_accuracy_2k: 0.9223 - lcm_accuracy_3k: 0.9575 - lcm_accuracy_5k: 0.9800 - lcm_hamming_loss_k: 0.0294
Epoch 00011: val_loss improved from 0.63641 to 0.59353, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.5291 - lcm_precision_1k: 0.8221 - lcm_precision_2k: 0.6540 - lcm_precision_3k: 0.5207 - lcm_precision_5k: 0.3609 - lcm_recall_1k: 0.4771 - lcm_recall_2k: 0.7016 - lcm_recall_3k: 0.8070 - lcm_recall_5k: 0.8999 - lcm_f1_1k: 0.6037 - lcm_f1_2k: 0.6768 - lcm_f1_3k: 0.6328 - lcm_f1_5k: 0.5151 - lcm_accuracy_1k: 0.8221 - lcm_accuracy_2k: 0.9223 - lcm_accuracy_3k: 0.9575 - lcm_accuracy_5k: 0.9800 - lcm_hamming_loss_k: 0.0294 - val_loss: 0.5935 - val_lcm_precision_1k: 0.7906 - val_lcm_precision_2k: 0.6344 - val_lcm_precision_3k: 0.5032 - val_lcm_precision_5k: 0.3528 - val_lcm_recall_1k: 0.4352 - val_lcm_recall_2k: 0.6569 - val_lcm_recall_3k: 0.7602 - val_lcm_recall_5k: 0.8635 - val_lcm_f1_1k: 0.5605 - val_lcm_f1_2k: 0.6447 - val_lcm_f1_3k: 0.6050 - val_lcm_f1_5k: 0.5007 - val_lcm_accuracy_1k: 0.7906 - val_lcm_accuracy_2k: 0.8854 - val_lcm_accuracy_3k: 0.9227 - val_lcm_accuracy_5k: 0.9580 - val_lcm_hamming_loss_k: 0.0310
Epoch 12/100
12/12 [==============================] - ETA: 0s - loss: 0.5007 - lcm_precision_1k: 0.8455 - lcm_precision_2k: 0.6660 - lcm_precision_3k: 0.5334 - lcm_precision_5k: 0.3667 - lcm_recall_1k: 0.4935 - lcm_recall_2k: 0.7142 - lcm_recall_3k: 0.8242 - lcm_recall_5k: 0.9124 - lcm_f1_1k: 0.6230 - lcm_f1_2k: 0.6891 - lcm_f1_3k: 0.6475 - lcm_f1_5k: 0.5230 - lcm_accuracy_1k: 0.8455 - lcm_accuracy_2k: 0.9358 - lcm_accuracy_3k: 0.9685 - lcm_accuracy_5k: 0.9862 - lcm_hamming_loss_k: 0.0286
Epoch 00012: val_loss improved from 0.59353 to 0.58779, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.5007 - lcm_precision_1k: 0.8455 - lcm_precision_2k: 0.6660 - lcm_precision_3k: 0.5334 - lcm_precision_5k: 0.3667 - lcm_recall_1k: 0.4935 - lcm_recall_2k: 0.7142 - lcm_recall_3k: 0.8242 - lcm_recall_5k: 0.9124 - lcm_f1_1k: 0.6230 - lcm_f1_2k: 0.6891 - lcm_f1_3k: 0.6475 - lcm_f1_5k: 0.5230 - lcm_accuracy_1k: 0.8455 - lcm_accuracy_2k: 0.9358 - lcm_accuracy_3k: 0.9685 - lcm_accuracy_5k: 0.9862 - lcm_hamming_loss_k: 0.0286 - val_loss: 0.5878 - val_lcm_precision_1k: 0.8127 - val_lcm_precision_2k: 0.6571 - val_lcm_precision_3k: 0.5216 - val_lcm_precision_5k: 0.3584 - val_lcm_recall_1k: 0.4533 - val_lcm_recall_2k: 0.6802 - val_lcm_recall_3k: 0.7871 - val_lcm_recall_5k: 0.8775 - val_lcm_f1_1k: 0.5813 - val_lcm_f1_2k: 0.6678 - val_lcm_f1_3k: 0.6268 - val_lcm_f1_5k: 0.5086 - val_lcm_accuracy_1k: 0.8127 - val_lcm_accuracy_2k: 0.9093 - val_lcm_accuracy_3k: 0.9461 - val_lcm_accuracy_5k: 0.9692 - val_lcm_hamming_loss_k: 0.0301
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.4752 - lcm_precision_1k: 0.8725 - lcm_precision_2k: 0.6841 - lcm_precision_3k: 0.5488 - lcm_precision_5k: 0.3732 - lcm_recall_1k: 0.5124 - lcm_recall_2k: 0.7315 - lcm_recall_3k: 0.8441 - lcm_recall_5k: 0.9241 - lcm_f1_1k: 0.6454 - lcm_f1_2k: 0.7068 - lcm_f1_3k: 0.6650 - lcm_f1_5k: 0.5315 - lcm_accuracy_1k: 0.8725 - lcm_accuracy_2k: 0.9469 - lcm_accuracy_3k: 0.9749 - lcm_accuracy_5k: 0.9905 - lcm_hamming_loss_k: 0.0275
Epoch 00013: val_loss improved from 0.58779 to 0.58480, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.4752 - lcm_precision_1k: 0.8725 - lcm_precision_2k: 0.6841 - lcm_precision_3k: 0.5488 - lcm_precision_5k: 0.3732 - lcm_recall_1k: 0.5124 - lcm_recall_2k: 0.7315 - lcm_recall_3k: 0.8441 - lcm_recall_5k: 0.9241 - lcm_f1_1k: 0.6454 - lcm_f1_2k: 0.7068 - lcm_f1_3k: 0.6650 - lcm_f1_5k: 0.5315 - lcm_accuracy_1k: 0.8725 - lcm_accuracy_2k: 0.9469 - lcm_accuracy_3k: 0.9749 - lcm_accuracy_5k: 0.9905 - lcm_hamming_loss_k: 0.0275 - val_loss: 0.5848 - val_lcm_precision_1k: 0.7800 - val_lcm_precision_2k: 0.6360 - val_lcm_precision_3k: 0.5162 - val_lcm_precision_5k: 0.3529 - val_lcm_recall_1k: 0.4377 - val_lcm_recall_2k: 0.6608 - val_lcm_recall_3k: 0.7805 - val_lcm_recall_5k: 0.8713 - val_lcm_f1_1k: 0.5603 - val_lcm_f1_2k: 0.6476 - val_lcm_f1_3k: 0.6208 - val_lcm_f1_5k: 0.5019 - val_lcm_accuracy_1k: 0.7800 - val_lcm_accuracy_2k: 0.8886 - val_lcm_accuracy_3k: 0.9365 - val_lcm_accuracy_5k: 0.9660 - val_lcm_hamming_loss_k: 0.0314
Epoch 14/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4600 - lcm_precision_1k: 0.8739 - lcm_precision_2k: 0.6921 - lcm_precision_3k: 0.5554 - lcm_precision_5k: 0.3756 - lcm_recall_1k: 0.5119 - lcm_recall_2k: 0.7413 - lcm_recall_3k: 0.8566 - lcm_recall_5k: 0.9312 - lcm_f1_1k: 0.6454 - lcm_f1_2k: 0.7157 - lcm_f1_3k: 0.6738 - lcm_f1_5k: 0.5353 - lcm_accuracy_1k: 0.8739 - lcm_accuracy_2k: 0.9542 - lcm_accuracy_3k: 0.9801 - lcm_accuracy_5k: 0.9929 - lcm_hamming_loss_k: 0.0274
Epoch 00014: val_loss improved from 0.58480 to 0.55098, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.4586 - lcm_precision_1k: 0.8730 - lcm_precision_2k: 0.6922 - lcm_precision_3k: 0.5560 - lcm_precision_5k: 0.3760 - lcm_recall_1k: 0.5105 - lcm_recall_2k: 0.7398 - lcm_recall_3k: 0.8576 - lcm_recall_5k: 0.9319 - lcm_f1_1k: 0.6441 - lcm_f1_2k: 0.7150 - lcm_f1_3k: 0.6745 - lcm_f1_5k: 0.5358 - lcm_accuracy_1k: 0.8730 - lcm_accuracy_2k: 0.9520 - lcm_accuracy_3k: 0.9813 - lcm_accuracy_5k: 0.9935 - lcm_hamming_loss_k: 0.0274 - val_loss: 0.5510 - val_lcm_precision_1k: 0.8305 - val_lcm_precision_2k: 0.6660 - val_lcm_precision_3k: 0.5278 - val_lcm_precision_5k: 0.3571 - val_lcm_recall_1k: 0.4625 - val_lcm_recall_2k: 0.6883 - val_lcm_recall_3k: 0.7958 - val_lcm_recall_5k: 0.8782 - val_lcm_f1_1k: 0.5933 - val_lcm_f1_2k: 0.6762 - val_lcm_f1_3k: 0.6340 - val_lcm_f1_5k: 0.5074 - val_lcm_accuracy_1k: 0.8305 - val_lcm_accuracy_2k: 0.9214 - val_lcm_accuracy_3k: 0.9488 - val_lcm_accuracy_5k: 0.9718 - val_lcm_hamming_loss_k: 0.0294
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4200 - lcm_precision_1k: 0.9005 - lcm_precision_2k: 0.7141 - lcm_precision_3k: 0.5651 - lcm_precision_5k: 0.3816 - lcm_recall_1k: 0.5302 - lcm_recall_2k: 0.7631 - lcm_recall_3k: 0.8676 - lcm_recall_5k: 0.9409 - lcm_f1_1k: 0.6674 - lcm_f1_2k: 0.7377 - lcm_f1_3k: 0.6843 - lcm_f1_5k: 0.5429 - lcm_accuracy_1k: 0.9005 - lcm_accuracy_2k: 0.9680 - lcm_accuracy_3k: 0.9883 - lcm_accuracy_5k: 0.9943 - lcm_hamming_loss_k: 0.0263
Epoch 00015: val_loss improved from 0.55098 to 0.54439, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.4207 - lcm_precision_1k: 0.9007 - lcm_precision_2k: 0.7110 - lcm_precision_3k: 0.5628 - lcm_precision_5k: 0.3802 - lcm_recall_1k: 0.5320 - lcm_recall_2k: 0.7622 - lcm_recall_3k: 0.8664 - lcm_recall_5k: 0.9397 - lcm_f1_1k: 0.6688 - lcm_f1_2k: 0.7356 - lcm_f1_3k: 0.6823 - lcm_f1_5k: 0.5413 - lcm_accuracy_1k: 0.9007 - lcm_accuracy_2k: 0.9680 - lcm_accuracy_3k: 0.9876 - lcm_accuracy_5k: 0.9948 - lcm_hamming_loss_k: 0.0263 - val_loss: 0.5444 - val_lcm_precision_1k: 0.8440 - val_lcm_precision_2k: 0.6700 - val_lcm_precision_3k: 0.5256 - val_lcm_precision_5k: 0.3564 - val_lcm_recall_1k: 0.4728 - val_lcm_recall_2k: 0.6981 - val_lcm_recall_3k: 0.7987 - val_lcm_recall_5k: 0.8807 - val_lcm_f1_1k: 0.6054 - val_lcm_f1_2k: 0.6831 - val_lcm_f1_3k: 0.6334 - val_lcm_f1_5k: 0.5071 - val_lcm_accuracy_1k: 0.8440 - val_lcm_accuracy_2k: 0.9286 - val_lcm_accuracy_3k: 0.9531 - val_lcm_accuracy_5k: 0.9746 - val_lcm_hamming_loss_k: 0.0288
Epoch 16/100
12/12 [==============================] - ETA: 0s - loss: 0.4009 - lcm_precision_1k: 0.9050 - lcm_precision_2k: 0.7209 - lcm_precision_3k: 0.5710 - lcm_precision_5k: 0.3860 - lcm_recall_1k: 0.5348 - lcm_recall_2k: 0.7709 - lcm_recall_3k: 0.8751 - lcm_recall_5k: 0.9503 - lcm_f1_1k: 0.6721 - lcm_f1_2k: 0.7449 - lcm_f1_3k: 0.6910 - lcm_f1_5k: 0.5489 - lcm_accuracy_1k: 0.9050 - lcm_accuracy_2k: 0.9716 - lcm_accuracy_3k: 0.9871 - lcm_accuracy_5k: 0.9964 - lcm_hamming_loss_k: 0.0262
Epoch 00016: val_loss improved from 0.54439 to 0.53568, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.4009 - lcm_precision_1k: 0.9050 - lcm_precision_2k: 0.7209 - lcm_precision_3k: 0.5710 - lcm_precision_5k: 0.3860 - lcm_recall_1k: 0.5348 - lcm_recall_2k: 0.7709 - lcm_recall_3k: 0.8751 - lcm_recall_5k: 0.9503 - lcm_f1_1k: 0.6721 - lcm_f1_2k: 0.7449 - lcm_f1_3k: 0.6910 - lcm_f1_5k: 0.5489 - lcm_accuracy_1k: 0.9050 - lcm_accuracy_2k: 0.9716 - lcm_accuracy_3k: 0.9871 - lcm_accuracy_5k: 0.9964 - lcm_hamming_loss_k: 0.0262 - val_loss: 0.5357 - val_lcm_precision_1k: 0.8487 - val_lcm_precision_2k: 0.6744 - val_lcm_precision_3k: 0.5309 - val_lcm_precision_5k: 0.3593 - val_lcm_recall_1k: 0.4753 - val_lcm_recall_2k: 0.6981 - val_lcm_recall_3k: 0.7997 - val_lcm_recall_5k: 0.8813 - val_lcm_f1_1k: 0.6087 - val_lcm_f1_2k: 0.6852 - val_lcm_f1_3k: 0.6375 - val_lcm_f1_5k: 0.5101 - val_lcm_accuracy_1k: 0.8487 - val_lcm_accuracy_2k: 0.9215 - val_lcm_accuracy_3k: 0.9458 - val_lcm_accuracy_5k: 0.9688 - val_lcm_hamming_loss_k: 0.0286
Epoch 17/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3792 - lcm_precision_1k: 0.9194 - lcm_precision_2k: 0.7278 - lcm_precision_3k: 0.5772 - lcm_precision_5k: 0.3881 - lcm_recall_1k: 0.5443 - lcm_recall_2k: 0.7792 - lcm_recall_3k: 0.8844 - lcm_recall_5k: 0.9552 - lcm_f1_1k: 0.6837 - lcm_f1_2k: 0.7525 - lcm_f1_3k: 0.6984 - lcm_f1_5k: 0.5519 - lcm_accuracy_1k: 0.9194 - lcm_accuracy_2k: 0.9784 - lcm_accuracy_3k: 0.9904 - lcm_accuracy_5k: 0.9975 - lcm_hamming_loss_k: 0.0255
Epoch 00017: val_loss did not improve from 0.53568
12/12 [==============================] - 1s 62ms/step - loss: 0.3793 - lcm_precision_1k: 0.9185 - lcm_precision_2k: 0.7276 - lcm_precision_3k: 0.5763 - lcm_precision_5k: 0.3877 - lcm_recall_1k: 0.5455 - lcm_recall_2k: 0.7804 - lcm_recall_3k: 0.8848 - lcm_recall_5k: 0.9549 - lcm_f1_1k: 0.6843 - lcm_f1_2k: 0.7530 - lcm_f1_3k: 0.6979 - lcm_f1_5k: 0.5514 - lcm_accuracy_1k: 0.9185 - lcm_accuracy_2k: 0.9791 - lcm_accuracy_3k: 0.9912 - lcm_accuracy_5k: 0.9977 - lcm_hamming_loss_k: 0.0255 - val_loss: 0.5649 - val_lcm_precision_1k: 0.8279 - val_lcm_precision_2k: 0.6689 - val_lcm_precision_3k: 0.5313 - val_lcm_precision_5k: 0.3612 - val_lcm_recall_1k: 0.4635 - val_lcm_recall_2k: 0.6918 - val_lcm_recall_3k: 0.7999 - val_lcm_recall_5k: 0.8860 - val_lcm_f1_1k: 0.5937 - val_lcm_f1_2k: 0.6793 - val_lcm_f1_3k: 0.6379 - val_lcm_f1_5k: 0.5128 - val_lcm_accuracy_1k: 0.8279 - val_lcm_accuracy_2k: 0.9146 - val_lcm_accuracy_3k: 0.9500 - val_lcm_accuracy_5k: 0.9743 - val_lcm_hamming_loss_k: 0.0295
Epoch 18/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3723 - lcm_precision_1k: 0.9254 - lcm_precision_2k: 0.7376 - lcm_precision_3k: 0.5843 - lcm_precision_5k: 0.3915 - lcm_recall_1k: 0.5447 - lcm_recall_2k: 0.7860 - lcm_recall_3k: 0.8904 - lcm_recall_5k: 0.9584 - lcm_f1_1k: 0.6857 - lcm_f1_2k: 0.7610 - lcm_f1_3k: 0.7055 - lcm_f1_5k: 0.5559 - lcm_accuracy_1k: 0.9254 - lcm_accuracy_2k: 0.9801 - lcm_accuracy_3k: 0.9940 - lcm_accuracy_5k: 0.9975 - lcm_hamming_loss_k: 0.0255
Epoch 00018: val_loss improved from 0.53568 to 0.52949, saving model to logs/clwlku-labs-0602-221855/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.3701 - lcm_precision_1k: 0.9262 - lcm_precision_2k: 0.7366 - lcm_precision_3k: 0.5819 - lcm_precision_5k: 0.3889 - lcm_recall_1k: 0.5502 - lcm_recall_2k: 0.7906 - lcm_recall_3k: 0.8932 - lcm_recall_5k: 0.9589 - lcm_f1_1k: 0.6901 - lcm_f1_2k: 0.7625 - lcm_f1_3k: 0.7045 - lcm_f1_5k: 0.5533 - lcm_accuracy_1k: 0.9262 - lcm_accuracy_2k: 0.9813 - lcm_accuracy_3k: 0.9945 - lcm_accuracy_5k: 0.9977 - lcm_hamming_loss_k: 0.0251 - val_loss: 0.5295 - val_lcm_precision_1k: 0.8511 - val_lcm_precision_2k: 0.6778 - val_lcm_precision_3k: 0.5380 - val_lcm_precision_5k: 0.3598 - val_lcm_recall_1k: 0.4777 - val_lcm_recall_2k: 0.7024 - val_lcm_recall_3k: 0.8090 - val_lcm_recall_5k: 0.8831 - val_lcm_f1_1k: 0.6113 - val_lcm_f1_2k: 0.6891 - val_lcm_f1_3k: 0.6455 - val_lcm_f1_5k: 0.5108 - val_lcm_accuracy_1k: 0.8511 - val_lcm_accuracy_2k: 0.9270 - val_lcm_accuracy_3k: 0.9474 - val_lcm_accuracy_5k: 0.9689 - val_lcm_hamming_loss_k: 0.0285
Epoch 19/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3489 - lcm_precision_1k: 0.9297 - lcm_precision_2k: 0.7442 - lcm_precision_3k: 0.5883 - lcm_precision_5k: 0.3923 - lcm_recall_1k: 0.5498 - lcm_recall_2k: 0.7943 - lcm_recall_3k: 0.8980 - lcm_recall_5k: 0.9615 - lcm_f1_1k: 0.6908 - lcm_f1_2k: 0.7683 - lcm_f1_3k: 0.7108 - lcm_f1_5k: 0.5571 - lcm_accuracy_1k: 0.9297 - lcm_accuracy_2k: 0.9830 - lcm_accuracy_3k: 0.9947 - lcm_accuracy_5k: 0.9982 - lcm_hamming_loss_k: 0.0252
Epoch 00019: val_loss did not improve from 0.52949
12/12 [==============================] - 1s 69ms/step - loss: 0.3488 - lcm_precision_1k: 0.9290 - lcm_precision_2k: 0.7421 - lcm_precision_3k: 0.5867 - lcm_precision_5k: 0.3918 - lcm_recall_1k: 0.5512 - lcm_recall_2k: 0.7945 - lcm_recall_3k: 0.8979 - lcm_recall_5k: 0.9625 - lcm_f1_1k: 0.6917 - lcm_f1_2k: 0.7673 - lcm_f1_3k: 0.7096 - lcm_f1_5k: 0.5568 - lcm_accuracy_1k: 0.9290 - lcm_accuracy_2k: 0.9817 - lcm_accuracy_3k: 0.9946 - lcm_accuracy_5k: 0.9984 - lcm_hamming_loss_k: 0.0251 - val_loss: 0.5509 - val_lcm_precision_1k: 0.8419 - val_lcm_precision_2k: 0.6759 - val_lcm_precision_3k: 0.5310 - val_lcm_precision_5k: 0.3532 - val_lcm_recall_1k: 0.4703 - val_lcm_recall_2k: 0.7035 - val_lcm_recall_3k: 0.8009 - val_lcm_recall_5k: 0.8707 - val_lcm_f1_1k: 0.6027 - val_lcm_f1_2k: 0.6886 - val_lcm_f1_3k: 0.6378 - val_lcm_f1_5k: 0.5020 - val_lcm_accuracy_1k: 0.8419 - val_lcm_accuracy_2k: 0.9259 - val_lcm_accuracy_3k: 0.9470 - val_lcm_accuracy_5k: 0.9688 - val_lcm_hamming_loss_k: 0.0289
Epoch 20/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3346 - lcm_precision_1k: 0.9311 - lcm_precision_2k: 0.7527 - lcm_precision_3k: 0.5919 - lcm_precision_5k: 0.3933 - lcm_recall_1k: 0.5535 - lcm_recall_2k: 0.8054 - lcm_recall_3k: 0.9051 - lcm_recall_5k: 0.9656 - lcm_f1_1k: 0.6941 - lcm_f1_2k: 0.7780 - lcm_f1_3k: 0.7156 - lcm_f1_5k: 0.5588 - lcm_accuracy_1k: 0.9311 - lcm_accuracy_2k: 0.9837 - lcm_accuracy_3k: 0.9957 - lcm_accuracy_5k: 0.9979 - lcm_hamming_loss_k: 0.0249
Epoch 00020: val_loss did not improve from 0.52949
12/12 [==============================] - 1s 72ms/step - loss: 0.3362 - lcm_precision_1k: 0.9314 - lcm_precision_2k: 0.7531 - lcm_precision_3k: 0.5934 - lcm_precision_5k: 0.3945 - lcm_recall_1k: 0.5512 - lcm_recall_2k: 0.8026 - lcm_recall_3k: 0.9033 - lcm_recall_5k: 0.9642 - lcm_f1_1k: 0.6923 - lcm_f1_2k: 0.7769 - lcm_f1_3k: 0.7161 - lcm_f1_5k: 0.5598 - lcm_accuracy_1k: 0.9314 - lcm_accuracy_2k: 0.9829 - lcm_accuracy_3k: 0.9950 - lcm_accuracy_5k: 0.9981 - lcm_hamming_loss_k: 0.0251 - val_loss: 0.5367 - val_lcm_precision_1k: 0.8513 - val_lcm_precision_2k: 0.6798 - val_lcm_precision_3k: 0.5390 - val_lcm_precision_5k: 0.3593 - val_lcm_recall_1k: 0.4762 - val_lcm_recall_2k: 0.7008 - val_lcm_recall_3k: 0.8076 - val_lcm_recall_5k: 0.8797 - val_lcm_f1_1k: 0.6099 - val_lcm_f1_2k: 0.6893 - val_lcm_f1_3k: 0.6458 - val_lcm_f1_5k: 0.5098 - val_lcm_accuracy_1k: 0.8513 - val_lcm_accuracy_2k: 0.9192 - val_lcm_accuracy_3k: 0.9517 - val_lcm_accuracy_5k: 0.9679 - val_lcm_hamming_loss_k: 0.0285
Epoch 00020: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.4225 - lcm_precision_1k: 0.9127 - lcm_precision_2k: 0.7270 - lcm_precision_3k: 0.5713 - lcm_precision_5k: 0.3812 - lcm_recall_1k: 0.5223 - lcm_recall_2k: 0.7596 - lcm_recall_3k: 0.8588 - lcm_recall_5k: 0.9263 - lcm_f1_1k: 0.6634 - lcm_f1_2k: 0.7419 - lcm_f1_3k: 0.6851 - lcm_f1_5k: 0.5393 - lcm_accuracy_1k: 0.9127 - lcm_accuracy_2k: 0.9660 - lcm_accuracy_3k: 0.9796 - lcm_accuracy_5k: 0.9892 - lcm_hamming_loss_k: 0.0266
Best model result:  [0.42253032326698303, 0.9127383828163147, 0.7270281314849854, 0.571294903755188, 0.3811538517475128, 0.5222973823547363, 0.7596282362937927, 0.8587973117828369, 0.9262819290161133, 0.6633501052856445, 0.7419451475143433, 0.6851454377174377, 0.5393345952033997, 0.9127383828163147, 0.9659921526908875, 0.9796152114868164, 0.9892306923866272, 0.02662069723010063]
2970
742
1238
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_4 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_4[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_5 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_5[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Model: "model_5"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_4 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_4[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_5 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.__operators__.getitem_5 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_5[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 49, 1024)     308224      ['tf.__operators__.getitem_5[0][0
                                                                 ]']                              
                                                                                                  
 dot_2 (Dot)                    (None, 49)           0           ['label_lcm_emb[0][0]',          
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 49)           2450        ['dot_2[0][0]']                  
                                                                                                  
 concatenate_2 (Concatenate)    (None, 98)           0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 5,092,791
Trainable params: 4,028,391
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 1.5483 - lcm_precision_1k: 0.0685 - lcm_precision_2k: 0.0859 - lcm_precision_3k: 0.0900 - lcm_precision_5k: 0.0906 - lcm_recall_1k: 0.0338 - lcm_recall_2k: 0.0866 - lcm_recall_3k: 0.1371 - lcm_recall_5k: 0.2284 - lcm_f1_1k: 0.0450 - lcm_f1_2k: 0.0861 - lcm_f1_3k: 0.1086 - lcm_f1_5k: 0.1297 - lcm_accuracy_1k: 0.0685 - lcm_accuracy_2k: 0.1669 - lcm_accuracy_3k: 0.2496 - lcm_accuracy_5k: 0.3927 - lcm_hamming_loss_k: 0.1541
Epoch 00001: val_loss improved from inf to 1.49199, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 3s 109ms/step - loss: 1.5456 - lcm_precision_1k: 0.0644 - lcm_precision_2k: 0.0812 - lcm_precision_3k: 0.0853 - lcm_precision_5k: 0.0870 - lcm_recall_1k: 0.0315 - lcm_recall_2k: 0.0814 - lcm_recall_3k: 0.1291 - lcm_recall_5k: 0.2184 - lcm_f1_1k: 0.0421 - lcm_f1_2k: 0.0811 - lcm_f1_3k: 0.1026 - lcm_f1_5k: 0.1244 - lcm_accuracy_1k: 0.0644 - lcm_accuracy_2k: 0.1579 - lcm_accuracy_3k: 0.2375 - lcm_accuracy_5k: 0.3790 - lcm_hamming_loss_k: 0.1572 - val_loss: 1.4920 - val_lcm_precision_1k: 0.0408 - val_lcm_precision_2k: 0.0326 - val_lcm_precision_3k: 0.0345 - val_lcm_precision_5k: 0.0348 - val_lcm_recall_1k: 0.0193 - val_lcm_recall_2k: 0.0313 - val_lcm_recall_3k: 0.0545 - val_lcm_recall_5k: 0.0894 - val_lcm_f1_1k: 0.0261 - val_lcm_f1_2k: 0.0317 - val_lcm_f1_3k: 0.0420 - val_lcm_f1_5k: 0.0499 - val_lcm_accuracy_1k: 0.0408 - val_lcm_accuracy_2k: 0.0651 - val_lcm_accuracy_3k: 0.1009 - val_lcm_accuracy_5k: 0.1649 - val_lcm_hamming_loss_k: 0.1932
Epoch 2/100
12/12 [==============================] - ETA: 0s - loss: 1.4742 - lcm_precision_1k: 0.0429 - lcm_precision_2k: 0.0662 - lcm_precision_3k: 0.0540 - lcm_precision_5k: 0.0476 - lcm_recall_1k: 0.0214 - lcm_recall_2k: 0.0695 - lcm_recall_3k: 0.0846 - lcm_recall_5k: 0.1215 - lcm_f1_1k: 0.0285 - lcm_f1_2k: 0.0677 - lcm_f1_3k: 0.0658 - lcm_f1_5k: 0.0684 - lcm_accuracy_1k: 0.0429 - lcm_accuracy_2k: 0.1317 - lcm_accuracy_3k: 0.1609 - lcm_accuracy_5k: 0.2277 - lcm_hamming_loss_k: 0.1053
Epoch 00002: val_loss improved from 1.49199 to 1.46272, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 91ms/step - loss: 1.4742 - lcm_precision_1k: 0.0429 - lcm_precision_2k: 0.0662 - lcm_precision_3k: 0.0540 - lcm_precision_5k: 0.0476 - lcm_recall_1k: 0.0214 - lcm_recall_2k: 0.0695 - lcm_recall_3k: 0.0846 - lcm_recall_5k: 0.1215 - lcm_f1_1k: 0.0285 - lcm_f1_2k: 0.0677 - lcm_f1_3k: 0.0658 - lcm_f1_5k: 0.0684 - lcm_accuracy_1k: 0.0429 - lcm_accuracy_2k: 0.1317 - lcm_accuracy_3k: 0.1609 - lcm_accuracy_5k: 0.2277 - lcm_hamming_loss_k: 0.1053 - val_loss: 1.4627 - val_lcm_precision_1k: 0.0434 - val_lcm_precision_2k: 0.0770 - val_lcm_precision_3k: 0.0583 - val_lcm_precision_5k: 0.0505 - val_lcm_recall_1k: 0.0203 - val_lcm_recall_2k: 0.0799 - val_lcm_recall_3k: 0.0908 - val_lcm_recall_5k: 0.1254 - val_lcm_f1_1k: 0.0275 - val_lcm_f1_2k: 0.0782 - val_lcm_f1_3k: 0.0709 - val_lcm_f1_5k: 0.0719 - val_lcm_accuracy_1k: 0.0434 - val_lcm_accuracy_2k: 0.1514 - val_lcm_accuracy_3k: 0.1724 - val_lcm_accuracy_5k: 0.2358 - val_lcm_hamming_loss_k: 0.0614
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4388 - lcm_precision_1k: 0.0455 - lcm_precision_2k: 0.0536 - lcm_precision_3k: 0.0629 - lcm_precision_5k: 0.0582 - lcm_recall_1k: 0.0237 - lcm_recall_2k: 0.0551 - lcm_recall_3k: 0.0985 - lcm_recall_5k: 0.1484 - lcm_f1_1k: 0.0309 - lcm_f1_2k: 0.0542 - lcm_f1_3k: 0.0767 - lcm_f1_5k: 0.0835 - lcm_accuracy_1k: 0.0455 - lcm_accuracy_2k: 0.1072 - lcm_accuracy_3k: 0.1879 - lcm_accuracy_5k: 0.2709 - lcm_hamming_loss_k: 0.0612
Epoch 00003: val_loss improved from 1.46272 to 1.39794, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 1.4359 - lcm_precision_1k: 0.0460 - lcm_precision_2k: 0.0519 - lcm_precision_3k: 0.0672 - lcm_precision_5k: 0.0622 - lcm_recall_1k: 0.0230 - lcm_recall_2k: 0.0525 - lcm_recall_3k: 0.1063 - lcm_recall_5k: 0.1586 - lcm_f1_1k: 0.0303 - lcm_f1_2k: 0.0520 - lcm_f1_3k: 0.0823 - lcm_f1_5k: 0.0893 - lcm_accuracy_1k: 0.0460 - lcm_accuracy_2k: 0.1037 - lcm_accuracy_3k: 0.2003 - lcm_accuracy_5k: 0.2841 - lcm_hamming_loss_k: 0.0612 - val_loss: 1.3979 - val_lcm_precision_1k: 0.0476 - val_lcm_precision_2k: 0.0313 - val_lcm_precision_3k: 0.1105 - val_lcm_precision_5k: 0.1139 - val_lcm_recall_1k: 0.0212 - val_lcm_recall_2k: 0.0296 - val_lcm_recall_3k: 0.1663 - val_lcm_recall_5k: 0.2794 - val_lcm_f1_1k: 0.0291 - val_lcm_f1_2k: 0.0303 - val_lcm_f1_3k: 0.1323 - val_lcm_f1_5k: 0.1616 - val_lcm_accuracy_1k: 0.0476 - val_lcm_accuracy_2k: 0.0614 - val_lcm_accuracy_3k: 0.3288 - val_lcm_accuracy_5k: 0.4490 - val_lcm_hamming_loss_k: 0.0613
Epoch 4/100
11/12 [==========================>...] - ETA: 0s - loss: 1.3489 - lcm_precision_1k: 0.1481 - lcm_precision_2k: 0.1179 - lcm_precision_3k: 0.1391 - lcm_precision_5k: 0.1461 - lcm_recall_1k: 0.0783 - lcm_recall_2k: 0.1229 - lcm_recall_3k: 0.2117 - lcm_recall_5k: 0.3681 - lcm_f1_1k: 0.1024 - lcm_f1_2k: 0.1203 - lcm_f1_3k: 0.1678 - lcm_f1_5k: 0.2091 - lcm_accuracy_1k: 0.1481 - lcm_accuracy_2k: 0.2244 - lcm_accuracy_3k: 0.3977 - lcm_accuracy_5k: 0.5742 - lcm_hamming_loss_k: 0.0571
Epoch 00004: val_loss improved from 1.39794 to 1.27614, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 1.3455 - lcm_precision_1k: 0.1634 - lcm_precision_2k: 0.1289 - lcm_precision_3k: 0.1459 - lcm_precision_5k: 0.1498 - lcm_recall_1k: 0.0874 - lcm_recall_2k: 0.1371 - lcm_recall_3k: 0.2261 - lcm_recall_5k: 0.3806 - lcm_f1_1k: 0.1138 - lcm_f1_2k: 0.1327 - lcm_f1_3k: 0.1772 - lcm_f1_5k: 0.2150 - lcm_accuracy_1k: 0.1634 - lcm_accuracy_2k: 0.2458 - lcm_accuracy_3k: 0.4149 - lcm_accuracy_5k: 0.5891 - lcm_hamming_loss_k: 0.0564 - val_loss: 1.2761 - val_lcm_precision_1k: 0.3066 - val_lcm_precision_2k: 0.2214 - val_lcm_precision_3k: 0.1914 - val_lcm_precision_5k: 0.1964 - val_lcm_recall_1k: 0.1624 - val_lcm_recall_2k: 0.2248 - val_lcm_recall_3k: 0.2887 - val_lcm_recall_5k: 0.4919 - val_lcm_f1_1k: 0.2116 - val_lcm_f1_2k: 0.2227 - val_lcm_f1_3k: 0.2299 - val_lcm_f1_5k: 0.2805 - val_lcm_accuracy_1k: 0.3066 - val_lcm_accuracy_2k: 0.3824 - val_lcm_accuracy_3k: 0.4871 - val_lcm_accuracy_5k: 0.7358 - val_lcm_hamming_loss_k: 0.0507
Epoch 5/100
12/12 [==============================] - ETA: 0s - loss: 1.1939 - lcm_precision_1k: 0.3747 - lcm_precision_2k: 0.2997 - lcm_precision_3k: 0.2544 - lcm_precision_5k: 0.2137 - lcm_recall_1k: 0.2001 - lcm_recall_2k: 0.3114 - lcm_recall_3k: 0.3992 - lcm_recall_5k: 0.5490 - lcm_f1_1k: 0.2608 - lcm_f1_2k: 0.3053 - lcm_f1_3k: 0.3107 - lcm_f1_5k: 0.3075 - lcm_accuracy_1k: 0.3747 - lcm_accuracy_2k: 0.5084 - lcm_accuracy_3k: 0.6182 - lcm_accuracy_5k: 0.7750 - lcm_hamming_loss_k: 0.0477
Epoch 00005: val_loss improved from 1.27614 to 1.11399, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 1.1939 - lcm_precision_1k: 0.3747 - lcm_precision_2k: 0.2997 - lcm_precision_3k: 0.2544 - lcm_precision_5k: 0.2137 - lcm_recall_1k: 0.2001 - lcm_recall_2k: 0.3114 - lcm_recall_3k: 0.3992 - lcm_recall_5k: 0.5490 - lcm_f1_1k: 0.2608 - lcm_f1_2k: 0.3053 - lcm_f1_3k: 0.3107 - lcm_f1_5k: 0.3075 - lcm_accuracy_1k: 0.3747 - lcm_accuracy_2k: 0.5084 - lcm_accuracy_3k: 0.6182 - lcm_accuracy_5k: 0.7750 - lcm_hamming_loss_k: 0.0477 - val_loss: 1.1140 - val_lcm_precision_1k: 0.4427 - val_lcm_precision_2k: 0.3463 - val_lcm_precision_3k: 0.3208 - val_lcm_precision_5k: 0.2444 - val_lcm_recall_1k: 0.2162 - val_lcm_recall_2k: 0.3407 - val_lcm_recall_3k: 0.4800 - val_lcm_recall_5k: 0.6181 - val_lcm_f1_1k: 0.2893 - val_lcm_f1_2k: 0.3424 - val_lcm_f1_3k: 0.3842 - val_lcm_f1_5k: 0.3502 - val_lcm_accuracy_1k: 0.4427 - val_lcm_accuracy_2k: 0.5762 - val_lcm_accuracy_3k: 0.7182 - val_lcm_accuracy_5k: 0.8481 - val_lcm_hamming_loss_k: 0.0451
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 1.0276 - lcm_precision_1k: 0.4773 - lcm_precision_2k: 0.3936 - lcm_precision_3k: 0.3352 - lcm_precision_5k: 0.2619 - lcm_recall_1k: 0.2452 - lcm_recall_2k: 0.3946 - lcm_recall_3k: 0.5107 - lcm_recall_5k: 0.6604 - lcm_f1_1k: 0.3239 - lcm_f1_2k: 0.3941 - lcm_f1_3k: 0.4047 - lcm_f1_5k: 0.3750 - lcm_accuracy_1k: 0.4773 - lcm_accuracy_2k: 0.6460 - lcm_accuracy_3k: 0.7504 - lcm_accuracy_5k: 0.8509 - lcm_hamming_loss_k: 0.0436
Epoch 00006: val_loss improved from 1.11399 to 0.94822, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 1.0197 - lcm_precision_1k: 0.4840 - lcm_precision_2k: 0.3998 - lcm_precision_3k: 0.3407 - lcm_precision_5k: 0.2657 - lcm_recall_1k: 0.2499 - lcm_recall_2k: 0.4016 - lcm_recall_3k: 0.5198 - lcm_recall_5k: 0.6693 - lcm_f1_1k: 0.3295 - lcm_f1_2k: 0.4006 - lcm_f1_3k: 0.4115 - lcm_f1_5k: 0.3803 - lcm_accuracy_1k: 0.4840 - lcm_accuracy_2k: 0.6554 - lcm_accuracy_3k: 0.7603 - lcm_accuracy_5k: 0.8568 - lcm_hamming_loss_k: 0.0433 - val_loss: 0.9482 - val_lcm_precision_1k: 0.4895 - val_lcm_precision_2k: 0.3987 - val_lcm_precision_3k: 0.3548 - val_lcm_precision_5k: 0.2827 - val_lcm_recall_1k: 0.2401 - val_lcm_recall_2k: 0.3893 - val_lcm_recall_3k: 0.5356 - val_lcm_recall_5k: 0.6935 - val_lcm_f1_1k: 0.3209 - val_lcm_f1_2k: 0.3926 - val_lcm_f1_3k: 0.4266 - val_lcm_f1_5k: 0.4015 - val_lcm_accuracy_1k: 0.4895 - val_lcm_accuracy_2k: 0.6636 - val_lcm_accuracy_3k: 0.7873 - val_lcm_accuracy_5k: 0.8622 - val_lcm_hamming_loss_k: 0.0432
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.8610 - lcm_precision_1k: 0.5671 - lcm_precision_2k: 0.4584 - lcm_precision_3k: 0.3922 - lcm_precision_5k: 0.2966 - lcm_recall_1k: 0.3123 - lcm_recall_2k: 0.4857 - lcm_recall_3k: 0.6088 - lcm_recall_5k: 0.7426 - lcm_f1_1k: 0.4026 - lcm_f1_2k: 0.4715 - lcm_f1_3k: 0.4769 - lcm_f1_5k: 0.4238 - lcm_accuracy_1k: 0.5671 - lcm_accuracy_2k: 0.7379 - lcm_accuracy_3k: 0.8324 - lcm_accuracy_5k: 0.8988 - lcm_hamming_loss_k: 0.0399
Epoch 00007: val_loss improved from 0.94822 to 0.85032, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.8616 - lcm_precision_1k: 0.5680 - lcm_precision_2k: 0.4622 - lcm_precision_3k: 0.3927 - lcm_precision_5k: 0.2960 - lcm_recall_1k: 0.3121 - lcm_recall_2k: 0.4889 - lcm_recall_3k: 0.6093 - lcm_recall_5k: 0.7403 - lcm_f1_1k: 0.4027 - lcm_f1_2k: 0.4750 - lcm_f1_3k: 0.4774 - lcm_f1_5k: 0.4228 - lcm_accuracy_1k: 0.5680 - lcm_accuracy_2k: 0.7403 - lcm_accuracy_3k: 0.8317 - lcm_accuracy_5k: 0.8975 - lcm_hamming_loss_k: 0.0399 - val_loss: 0.8503 - val_lcm_precision_1k: 0.6107 - val_lcm_precision_2k: 0.4817 - val_lcm_precision_3k: 0.4057 - val_lcm_precision_5k: 0.2994 - val_lcm_recall_1k: 0.3345 - val_lcm_recall_2k: 0.4986 - val_lcm_recall_3k: 0.6170 - val_lcm_recall_5k: 0.7418 - val_lcm_f1_1k: 0.4321 - val_lcm_f1_2k: 0.4897 - val_lcm_f1_3k: 0.4891 - val_lcm_f1_5k: 0.4263 - val_lcm_accuracy_1k: 0.6107 - val_lcm_accuracy_2k: 0.7686 - val_lcm_accuracy_3k: 0.8295 - val_lcm_accuracy_5k: 0.8900 - val_lcm_hamming_loss_k: 0.0383
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.7741 - lcm_precision_1k: 0.6392 - lcm_precision_2k: 0.5243 - lcm_precision_3k: 0.4305 - lcm_precision_5k: 0.3148 - lcm_recall_1k: 0.3656 - lcm_recall_2k: 0.5624 - lcm_recall_3k: 0.6727 - lcm_recall_5k: 0.7950 - lcm_f1_1k: 0.4650 - lcm_f1_2k: 0.5426 - lcm_f1_3k: 0.5250 - lcm_f1_5k: 0.4510 - lcm_accuracy_1k: 0.6392 - lcm_accuracy_2k: 0.8057 - lcm_accuracy_3k: 0.8732 - lcm_accuracy_5k: 0.9297 - lcm_hamming_loss_k: 0.0368
Epoch 00008: val_loss improved from 0.85032 to 0.74970, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.7734 - lcm_precision_1k: 0.6428 - lcm_precision_2k: 0.5269 - lcm_precision_3k: 0.4332 - lcm_precision_5k: 0.3173 - lcm_recall_1k: 0.3640 - lcm_recall_2k: 0.5612 - lcm_recall_3k: 0.6716 - lcm_recall_5k: 0.7944 - lcm_f1_1k: 0.4646 - lcm_f1_2k: 0.5434 - lcm_f1_3k: 0.5266 - lcm_f1_5k: 0.4534 - lcm_accuracy_1k: 0.6428 - lcm_accuracy_2k: 0.8046 - lcm_accuracy_3k: 0.8730 - lcm_accuracy_5k: 0.9301 - lcm_hamming_loss_k: 0.0369 - val_loss: 0.7497 - val_lcm_precision_1k: 0.6566 - val_lcm_precision_2k: 0.5379 - val_lcm_precision_3k: 0.4495 - val_lcm_precision_5k: 0.3252 - val_lcm_recall_1k: 0.3699 - val_lcm_recall_2k: 0.5795 - val_lcm_recall_3k: 0.6967 - val_lcm_recall_5k: 0.8043 - val_lcm_f1_1k: 0.4731 - val_lcm_f1_2k: 0.5578 - val_lcm_f1_3k: 0.5463 - val_lcm_f1_5k: 0.4631 - val_lcm_accuracy_1k: 0.6566 - val_lcm_accuracy_2k: 0.8125 - val_lcm_accuracy_3k: 0.8868 - val_lcm_accuracy_5k: 0.9244 - val_lcm_hamming_loss_k: 0.0364
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6723 - lcm_precision_1k: 0.7060 - lcm_precision_2k: 0.5757 - lcm_precision_3k: 0.4711 - lcm_precision_5k: 0.3355 - lcm_recall_1k: 0.4012 - lcm_recall_2k: 0.6147 - lcm_recall_3k: 0.7290 - lcm_recall_5k: 0.8361 - lcm_f1_1k: 0.5115 - lcm_f1_2k: 0.5944 - lcm_f1_3k: 0.5723 - lcm_f1_5k: 0.4788 - lcm_accuracy_1k: 0.7060 - lcm_accuracy_2k: 0.8558 - lcm_accuracy_3k: 0.9091 - lcm_accuracy_5k: 0.9503 - lcm_hamming_loss_k: 0.0343
Epoch 00009: val_loss improved from 0.74970 to 0.67135, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.6706 - lcm_precision_1k: 0.7094 - lcm_precision_2k: 0.5767 - lcm_precision_3k: 0.4715 - lcm_precision_5k: 0.3354 - lcm_recall_1k: 0.4035 - lcm_recall_2k: 0.6165 - lcm_recall_3k: 0.7307 - lcm_recall_5k: 0.8376 - lcm_f1_1k: 0.5143 - lcm_f1_2k: 0.5958 - lcm_f1_3k: 0.5731 - lcm_f1_5k: 0.4789 - lcm_accuracy_1k: 0.7094 - lcm_accuracy_2k: 0.8559 - lcm_accuracy_3k: 0.9091 - lcm_accuracy_5k: 0.9501 - lcm_hamming_loss_k: 0.0341 - val_loss: 0.6713 - val_lcm_precision_1k: 0.7336 - val_lcm_precision_2k: 0.5868 - val_lcm_precision_3k: 0.4678 - val_lcm_precision_5k: 0.3346 - val_lcm_recall_1k: 0.4071 - val_lcm_recall_2k: 0.6163 - val_lcm_recall_3k: 0.7184 - val_lcm_recall_5k: 0.8327 - val_lcm_f1_1k: 0.5235 - val_lcm_f1_2k: 0.6009 - val_lcm_f1_3k: 0.5664 - val_lcm_f1_5k: 0.4772 - val_lcm_accuracy_1k: 0.7336 - val_lcm_accuracy_2k: 0.8429 - val_lcm_accuracy_3k: 0.8926 - val_lcm_accuracy_5k: 0.9537 - val_lcm_hamming_loss_k: 0.0333
Epoch 10/100
12/12 [==============================] - ETA: 0s - loss: 0.6068 - lcm_precision_1k: 0.7563 - lcm_precision_2k: 0.6180 - lcm_precision_3k: 0.4952 - lcm_precision_5k: 0.3479 - lcm_recall_1k: 0.4283 - lcm_recall_2k: 0.6563 - lcm_recall_3k: 0.7649 - lcm_recall_5k: 0.8673 - lcm_f1_1k: 0.5466 - lcm_f1_2k: 0.6364 - lcm_f1_3k: 0.6011 - lcm_f1_5k: 0.4965 - lcm_accuracy_1k: 0.7563 - lcm_accuracy_2k: 0.8827 - lcm_accuracy_3k: 0.9285 - lcm_accuracy_5k: 0.9659 - lcm_hamming_loss_k: 0.0321 ETA: 0s - loss: 0.6087 - lcm_precision_1k: 0.7511 - lcm_precision_2k: 0.6186 - lcm_precision_3k: 0.4931 - lcm_precision_5k: 0.3508 - lcm_recall_1k: 0.4197 - lcm_recall_2k: 0.6490 - lcm_recall_3k: 0.7560 - lcm_recall_5k: 0.8677 - lcm_f1_1k: 0.5382 - lcm_f1_2k: 0.6332 - lcm_f1_3k: 0.5967 - lcm_f1_5k: 0.4995 - lcm_accuracy_1k: 0.7511 - lcm_accuracy_2k: 0.8711 - lcm_accuracy_3k: 0.9224 - lcm_accuracy_5k: 0.9648 - lcm_hamming_loss_k
Epoch 00010: val_loss improved from 0.67135 to 0.63122, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.6068 - lcm_precision_1k: 0.7563 - lcm_precision_2k: 0.6180 - lcm_precision_3k: 0.4952 - lcm_precision_5k: 0.3479 - lcm_recall_1k: 0.4283 - lcm_recall_2k: 0.6563 - lcm_recall_3k: 0.7649 - lcm_recall_5k: 0.8673 - lcm_f1_1k: 0.5466 - lcm_f1_2k: 0.6364 - lcm_f1_3k: 0.6011 - lcm_f1_5k: 0.4965 - lcm_accuracy_1k: 0.7563 - lcm_accuracy_2k: 0.8827 - lcm_accuracy_3k: 0.9285 - lcm_accuracy_5k: 0.9659 - lcm_hamming_loss_k: 0.0321 - val_loss: 0.6312 - val_lcm_precision_1k: 0.7643 - val_lcm_precision_2k: 0.6125 - val_lcm_precision_3k: 0.4857 - val_lcm_precision_5k: 0.3376 - val_lcm_recall_1k: 0.4222 - val_lcm_recall_2k: 0.6447 - val_lcm_recall_3k: 0.7440 - val_lcm_recall_5k: 0.8375 - val_lcm_f1_1k: 0.5435 - val_lcm_f1_2k: 0.6278 - val_lcm_f1_3k: 0.5874 - val_lcm_f1_5k: 0.4810 - val_lcm_accuracy_1k: 0.7643 - val_lcm_accuracy_2k: 0.8730 - val_lcm_accuracy_3k: 0.9181 - val_lcm_accuracy_5k: 0.9483 - val_lcm_hamming_loss_k: 0.0320
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5496 - lcm_precision_1k: 0.8139 - lcm_precision_2k: 0.6431 - lcm_precision_3k: 0.5143 - lcm_precision_5k: 0.3586 - lcm_recall_1k: 0.4693 - lcm_recall_2k: 0.6852 - lcm_recall_3k: 0.7940 - lcm_recall_5k: 0.8932 - lcm_f1_1k: 0.5951 - lcm_f1_2k: 0.6633 - lcm_f1_3k: 0.6241 - lcm_f1_5k: 0.5117 - lcm_accuracy_1k: 0.8139 - lcm_accuracy_2k: 0.9070 - lcm_accuracy_3k: 0.9492 - lcm_accuracy_5k: 0.9791 - lcm_hamming_loss_k: 0.0297
Epoch 00011: val_loss improved from 0.63122 to 0.59555, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.5499 - lcm_precision_1k: 0.8110 - lcm_precision_2k: 0.6436 - lcm_precision_3k: 0.5162 - lcm_precision_5k: 0.3596 - lcm_recall_1k: 0.4671 - lcm_recall_2k: 0.6837 - lcm_recall_3k: 0.7942 - lcm_recall_5k: 0.8922 - lcm_f1_1k: 0.5926 - lcm_f1_2k: 0.6629 - lcm_f1_3k: 0.6256 - lcm_f1_5k: 0.5125 - lcm_accuracy_1k: 0.8110 - lcm_accuracy_2k: 0.9061 - lcm_accuracy_3k: 0.9491 - lcm_accuracy_5k: 0.9787 - lcm_hamming_loss_k: 0.0300 - val_loss: 0.5955 - val_lcm_precision_1k: 0.7824 - val_lcm_precision_2k: 0.6267 - val_lcm_precision_3k: 0.4965 - val_lcm_precision_5k: 0.3454 - val_lcm_recall_1k: 0.4418 - val_lcm_recall_2k: 0.6598 - val_lcm_recall_3k: 0.7619 - val_lcm_recall_5k: 0.8573 - val_lcm_f1_1k: 0.5646 - val_lcm_f1_2k: 0.6426 - val_lcm_f1_3k: 0.6010 - val_lcm_f1_5k: 0.4922 - val_lcm_accuracy_1k: 0.7824 - val_lcm_accuracy_2k: 0.8849 - val_lcm_accuracy_3k: 0.9217 - val_lcm_accuracy_5k: 0.9632 - val_lcm_hamming_loss_k: 0.0313
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5040 - lcm_precision_1k: 0.8427 - lcm_precision_2k: 0.6749 - lcm_precision_3k: 0.5352 - lcm_precision_5k: 0.3664 - lcm_recall_1k: 0.4862 - lcm_recall_2k: 0.7149 - lcm_recall_3k: 0.8216 - lcm_recall_5k: 0.9073 - lcm_f1_1k: 0.6165 - lcm_f1_2k: 0.6942 - lcm_f1_3k: 0.6480 - lcm_f1_5k: 0.5219 - lcm_accuracy_1k: 0.8427 - lcm_accuracy_2k: 0.9293 - lcm_accuracy_3k: 0.9592 - lcm_accuracy_5k: 0.9812 - lcm_hamming_loss_k: 0.0286
Epoch 00012: val_loss improved from 0.59555 to 0.57198, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.5038 - lcm_precision_1k: 0.8444 - lcm_precision_2k: 0.6771 - lcm_precision_3k: 0.5369 - lcm_precision_5k: 0.3670 - lcm_recall_1k: 0.4859 - lcm_recall_2k: 0.7160 - lcm_recall_3k: 0.8232 - lcm_recall_5k: 0.9078 - lcm_f1_1k: 0.6167 - lcm_f1_2k: 0.6959 - lcm_f1_3k: 0.6498 - lcm_f1_5k: 0.5226 - lcm_accuracy_1k: 0.8444 - lcm_accuracy_2k: 0.9320 - lcm_accuracy_3k: 0.9615 - lcm_accuracy_5k: 0.9822 - lcm_hamming_loss_k: 0.0286 - val_loss: 0.5720 - val_lcm_precision_1k: 0.8248 - val_lcm_precision_2k: 0.6456 - val_lcm_precision_3k: 0.5071 - val_lcm_precision_5k: 0.3517 - val_lcm_recall_1k: 0.4697 - val_lcm_recall_2k: 0.6830 - val_lcm_recall_3k: 0.7793 - val_lcm_recall_5k: 0.8746 - val_lcm_f1_1k: 0.5984 - val_lcm_f1_2k: 0.6635 - val_lcm_f1_3k: 0.6142 - val_lcm_f1_5k: 0.5014 - val_lcm_accuracy_1k: 0.8248 - val_lcm_accuracy_2k: 0.9118 - val_lcm_accuracy_3k: 0.9408 - val_lcm_accuracy_5k: 0.9728 - val_lcm_hamming_loss_k: 0.0296
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.4862 - lcm_precision_1k: 0.8475 - lcm_precision_2k: 0.6760 - lcm_precision_3k: 0.5451 - lcm_precision_5k: 0.3696 - lcm_recall_1k: 0.4926 - lcm_recall_2k: 0.7212 - lcm_recall_3k: 0.8369 - lcm_recall_5k: 0.9163 - lcm_f1_1k: 0.6229 - lcm_f1_2k: 0.6977 - lcm_f1_3k: 0.6601 - lcm_f1_5k: 0.5266 - lcm_accuracy_1k: 0.8475 - lcm_accuracy_2k: 0.9365 - lcm_accuracy_3k: 0.9688 - lcm_accuracy_5k: 0.9871 - lcm_hamming_loss_k: 0.0284
Epoch 00013: val_loss improved from 0.57198 to 0.56366, saving model to logs/twusnq-labs-0602-221920/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.4862 - lcm_precision_1k: 0.8475 - lcm_precision_2k: 0.6760 - lcm_precision_3k: 0.5451 - lcm_precision_5k: 0.3696 - lcm_recall_1k: 0.4926 - lcm_recall_2k: 0.7212 - lcm_recall_3k: 0.8369 - lcm_recall_5k: 0.9163 - lcm_f1_1k: 0.6229 - lcm_f1_2k: 0.6977 - lcm_f1_3k: 0.6601 - lcm_f1_5k: 0.5266 - lcm_accuracy_1k: 0.8475 - lcm_accuracy_2k: 0.9365 - lcm_accuracy_3k: 0.9688 - lcm_accuracy_5k: 0.9871 - lcm_hamming_loss_k: 0.0284 - val_loss: 0.5637 - val_lcm_precision_1k: 0.8122 - val_lcm_precision_2k: 0.6457 - val_lcm_precision_3k: 0.5113 - val_lcm_precision_5k: 0.3487 - val_lcm_recall_1k: 0.4627 - val_lcm_recall_2k: 0.6837 - val_lcm_recall_3k: 0.7828 - val_lcm_recall_5k: 0.8642 - val_lcm_f1_1k: 0.5893 - val_lcm_f1_2k: 0.6639 - val_lcm_f1_3k: 0.6184 - val_lcm_f1_5k: 0.4967 - val_lcm_accuracy_1k: 0.8122 - val_lcm_accuracy_2k: 0.9021 - val_lcm_accuracy_3k: 0.9436 - val_lcm_accuracy_5k: 0.9679 - val_lcm_hamming_loss_k: 0.0301
Epoch 14/100
12/12 [==============================] - ETA: 0s - loss: 0.4569 - lcm_precision_1k: 0.8704 - lcm_precision_2k: 0.6972 - lcm_precision_3k: 0.5545 - lcm_precision_5k: 0.3737 - lcm_recall_1k: 0.5088 - lcm_recall_2k: 0.7404 - lcm_recall_3k: 0.8498 - lcm_recall_5k: 0.9250 - lcm_f1_1k: 0.6420 - lcm_f1_2k: 0.7180 - lcm_f1_3k: 0.6710 - lcm_f1_5k: 0.5322 - lcm_accuracy_1k: 0.8704 - lcm_accuracy_2k: 0.9487 - lcm_accuracy_3k: 0.9759 - lcm_accuracy_5k: 0.9905 - lcm_hamming_loss_k: 0.0275
Epoch 00014: val_loss did not improve from 0.56366
12/12 [==============================] - 1s 64ms/step - loss: 0.4569 - lcm_precision_1k: 0.8704 - lcm_precision_2k: 0.6972 - lcm_precision_3k: 0.5545 - lcm_precision_5k: 0.3737 - lcm_recall_1k: 0.5088 - lcm_recall_2k: 0.7404 - lcm_recall_3k: 0.8498 - lcm_recall_5k: 0.9250 - lcm_f1_1k: 0.6420 - lcm_f1_2k: 0.7180 - lcm_f1_3k: 0.6710 - lcm_f1_5k: 0.5322 - lcm_accuracy_1k: 0.8704 - lcm_accuracy_2k: 0.9487 - lcm_accuracy_3k: 0.9759 - lcm_accuracy_5k: 0.9905 - lcm_hamming_loss_k: 0.0275 - val_loss: 0.5775 - val_lcm_precision_1k: 0.8080 - val_lcm_precision_2k: 0.6364 - val_lcm_precision_3k: 0.5090 - val_lcm_precision_5k: 0.3469 - val_lcm_recall_1k: 0.4616 - val_lcm_recall_2k: 0.6704 - val_lcm_recall_3k: 0.7816 - val_lcm_recall_5k: 0.8626 - val_lcm_f1_1k: 0.5874 - val_lcm_f1_2k: 0.6527 - val_lcm_f1_3k: 0.6162 - val_lcm_f1_5k: 0.4946 - val_lcm_accuracy_1k: 0.8080 - val_lcm_accuracy_2k: 0.8995 - val_lcm_accuracy_3k: 0.9366 - val_lcm_accuracy_5k: 0.9599 - val_lcm_hamming_loss_k: 0.0302
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4371 - lcm_precision_1k: 0.8814 - lcm_precision_2k: 0.7024 - lcm_precision_3k: 0.5584 - lcm_precision_5k: 0.3774 - lcm_recall_1k: 0.5147 - lcm_recall_2k: 0.7508 - lcm_recall_3k: 0.8583 - lcm_recall_5k: 0.9343 - lcm_f1_1k: 0.6497 - lcm_f1_2k: 0.7257 - lcm_f1_3k: 0.6764 - lcm_f1_5k: 0.5376 - lcm_accuracy_1k: 0.8814 - lcm_accuracy_2k: 0.9531 - lcm_accuracy_3k: 0.9784 - lcm_accuracy_5k: 0.9922 - lcm_hamming_loss_k: 0.0270 ETA: 0s - loss: 0.4365 - lcm_precision_1k: 0.8824 - lcm_precision_2k: 0.7039 - lcm_precision_3k: 0.5586 - lcm_precision_5k: 0.3768 - lcm_recall_1k: 0.5164 - lcm_recall_2k: 0.7538 - lcm_recall_3k: 0.8604 - lcm_recall_5k: 0.9350 - lcm_f1_1k: 0.6513 - lcm_f1_2k: 0.7278 - lcm_f1_3k: 0.6773 - lcm_f1_5k: 0.5371 - lcm_accuracy_1k: 0.8824 - lcm_accuracy_2k: 0.9551 - lcm_accuracy_3k: 0.9801 - lcm_accuracy_5k: 0.9930 - lcm_hamming_loss_k: 0.02
Epoch 00015: val_loss did not improve from 0.56366
12/12 [==============================] - 1s 69ms/step - loss: 0.4398 - lcm_precision_1k: 0.8794 - lcm_precision_2k: 0.7023 - lcm_precision_3k: 0.5594 - lcm_precision_5k: 0.3776 - lcm_recall_1k: 0.5125 - lcm_recall_2k: 0.7489 - lcm_recall_3k: 0.8579 - lcm_recall_5k: 0.9330 - lcm_f1_1k: 0.6474 - lcm_f1_2k: 0.7247 - lcm_f1_3k: 0.6771 - lcm_f1_5k: 0.5375 - lcm_accuracy_1k: 0.8794 - lcm_accuracy_2k: 0.9522 - lcm_accuracy_3k: 0.9780 - lcm_accuracy_5k: 0.9923 - lcm_hamming_loss_k: 0.0272 - val_loss: 0.5719 - val_lcm_precision_1k: 0.8071 - val_lcm_precision_2k: 0.6322 - val_lcm_precision_3k: 0.5080 - val_lcm_precision_5k: 0.3504 - val_lcm_recall_1k: 0.4596 - val_lcm_recall_2k: 0.6663 - val_lcm_recall_3k: 0.7744 - val_lcm_recall_5k: 0.8618 - val_lcm_f1_1k: 0.5855 - val_lcm_f1_2k: 0.6485 - val_lcm_f1_3k: 0.6132 - val_lcm_f1_5k: 0.4980 - val_lcm_accuracy_1k: 0.8071 - val_lcm_accuracy_2k: 0.8908 - val_lcm_accuracy_3k: 0.9346 - val_lcm_accuracy_5k: 0.9566 - val_lcm_hamming_loss_k: 0.0303
Epoch 00015: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.4862 - lcm_precision_1k: 0.8795 - lcm_precision_2k: 0.6974 - lcm_precision_3k: 0.5517 - lcm_precision_5k: 0.3703 - lcm_recall_1k: 0.5008 - lcm_recall_2k: 0.7299 - lcm_recall_3k: 0.8342 - lcm_recall_5k: 0.9043 - lcm_f1_1k: 0.6371 - lcm_f1_2k: 0.7122 - lcm_f1_3k: 0.6631 - lcm_f1_5k: 0.5247 - lcm_accuracy_1k: 0.8795 - lcm_accuracy_2k: 0.9488 - lcm_accuracy_3k: 0.9736 - lcm_accuracy_5k: 0.9864 - lcm_hamming_loss_k: 0.0280
Best model result:  [0.48620596528053284, 0.8795129060745239, 0.6974102258682251, 0.5516948699951172, 0.3703359067440033, 0.5007845163345337, 0.729858934879303, 0.8341717720031738, 0.9042972922325134, 0.6371349692344666, 0.7121678590774536, 0.6630915999412537, 0.5247405171394348, 0.8795129060745239, 0.948797345161438, 0.9736409187316895, 0.9863871335983276, 0.027976488694548607]
2970
742
1238
Model: "model_6"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_6 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_6[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_7 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_7[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Model: "model_7"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_6 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_6[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_7 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_7[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 49, 1024)     308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 dot_3 (Dot)                    (None, 49)           0           ['label_lcm_emb[0][0]',          
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 49)           2450        ['dot_3[0][0]']                  
                                                                                                  
 concatenate_3 (Concatenate)    (None, 98)           0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 5,092,791
Trainable params: 4,028,391
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 1.5547 - lcm_precision_1k: 0.0856 - lcm_precision_2k: 0.0883 - lcm_precision_3k: 0.0848 - lcm_precision_5k: 0.0810 - lcm_recall_1k: 0.0443 - lcm_recall_2k: 0.0976 - lcm_recall_3k: 0.1368 - lcm_recall_5k: 0.2135 - lcm_f1_1k: 0.0583 - lcm_f1_2k: 0.0925 - lcm_f1_3k: 0.1045 - lcm_f1_5k: 0.1174 - lcm_accuracy_1k: 0.0856 - lcm_accuracy_2k: 0.1708 - lcm_accuracy_3k: 0.2443 - lcm_accuracy_5k: 0.3722 - lcm_hamming_loss_k: 0.1744
Epoch 00001: val_loss improved from inf to 1.49103, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 3s 131ms/step - loss: 1.5516 - lcm_precision_1k: 0.0839 - lcm_precision_2k: 0.0863 - lcm_precision_3k: 0.0827 - lcm_precision_5k: 0.0786 - lcm_recall_1k: 0.0436 - lcm_recall_2k: 0.0962 - lcm_recall_3k: 0.1348 - lcm_recall_5k: 0.2087 - lcm_f1_1k: 0.0573 - lcm_f1_2k: 0.0908 - lcm_f1_3k: 0.1024 - lcm_f1_5k: 0.1141 - lcm_accuracy_1k: 0.0839 - lcm_accuracy_2k: 0.1674 - lcm_accuracy_3k: 0.2386 - lcm_accuracy_5k: 0.3612 - lcm_hamming_loss_k: 0.1772 - val_loss: 1.4910 - val_lcm_precision_1k: 0.0481 - val_lcm_precision_2k: 0.0775 - val_lcm_precision_3k: 0.0675 - val_lcm_precision_5k: 0.0627 - val_lcm_recall_1k: 0.0261 - val_lcm_recall_2k: 0.0936 - val_lcm_recall_3k: 0.1166 - val_lcm_recall_5k: 0.1720 - val_lcm_f1_1k: 0.0334 - val_lcm_f1_2k: 0.0845 - val_lcm_f1_3k: 0.0853 - val_lcm_f1_5k: 0.0917 - val_lcm_accuracy_1k: 0.0481 - val_lcm_accuracy_2k: 0.1520 - val_lcm_accuracy_3k: 0.1957 - val_lcm_accuracy_5k: 0.2964 - val_lcm_hamming_loss_k: 0.1899
Epoch 2/100
12/12 [==============================] - ETA: 0s - loss: 1.4681 - lcm_precision_1k: 0.1103 - lcm_precision_2k: 0.0980 - lcm_precision_3k: 0.0774 - lcm_precision_5k: 0.0767 - lcm_recall_1k: 0.0602 - lcm_recall_2k: 0.1055 - lcm_recall_3k: 0.1246 - lcm_recall_5k: 0.2053 - lcm_f1_1k: 0.0778 - lcm_f1_2k: 0.1014 - lcm_f1_3k: 0.0953 - lcm_f1_5k: 0.1115 - lcm_accuracy_1k: 0.1103 - lcm_accuracy_2k: 0.1923 - lcm_accuracy_3k: 0.2260 - lcm_accuracy_5k: 0.3583 - lcm_hamming_loss_k: 0.0949
Epoch 00002: val_loss improved from 1.49103 to 1.43278, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 1.4681 - lcm_precision_1k: 0.1103 - lcm_precision_2k: 0.0980 - lcm_precision_3k: 0.0774 - lcm_precision_5k: 0.0767 - lcm_recall_1k: 0.0602 - lcm_recall_2k: 0.1055 - lcm_recall_3k: 0.1246 - lcm_recall_5k: 0.2053 - lcm_f1_1k: 0.0778 - lcm_f1_2k: 0.1014 - lcm_f1_3k: 0.0953 - lcm_f1_5k: 0.1115 - lcm_accuracy_1k: 0.1103 - lcm_accuracy_2k: 0.1923 - lcm_accuracy_3k: 0.2260 - lcm_accuracy_5k: 0.3583 - lcm_hamming_loss_k: 0.0949 - val_loss: 1.4328 - val_lcm_precision_1k: 0.1333 - val_lcm_precision_2k: 0.1027 - val_lcm_precision_3k: 0.0814 - val_lcm_precision_5k: 0.0672 - val_lcm_recall_1k: 0.0694 - val_lcm_recall_2k: 0.1051 - val_lcm_recall_3k: 0.1253 - val_lcm_recall_5k: 0.1759 - val_lcm_f1_1k: 0.0909 - val_lcm_f1_2k: 0.1034 - val_lcm_f1_3k: 0.0982 - val_lcm_f1_5k: 0.0967 - val_lcm_accuracy_1k: 0.1333 - val_lcm_accuracy_2k: 0.1997 - val_lcm_accuracy_3k: 0.2341 - val_lcm_accuracy_5k: 0.3204 - val_lcm_hamming_loss_k: 0.0583
Epoch 3/100
12/12 [==============================] - ETA: 0s - loss: 1.4006 - lcm_precision_1k: 0.0687 - lcm_precision_2k: 0.0901 - lcm_precision_3k: 0.0993 - lcm_precision_5k: 0.0962 - lcm_recall_1k: 0.0376 - lcm_recall_2k: 0.0964 - lcm_recall_3k: 0.1509 - lcm_recall_5k: 0.2498 - lcm_f1_1k: 0.0485 - lcm_f1_2k: 0.0930 - lcm_f1_3k: 0.1196 - lcm_f1_5k: 0.1388 - lcm_accuracy_1k: 0.0687 - lcm_accuracy_2k: 0.1777 - lcm_accuracy_3k: 0.2876 - lcm_accuracy_5k: 0.4448 - lcm_hamming_loss_k: 0.0603
Epoch 00003: val_loss improved from 1.43278 to 1.36593, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 1.4006 - lcm_precision_1k: 0.0687 - lcm_precision_2k: 0.0901 - lcm_precision_3k: 0.0993 - lcm_precision_5k: 0.0962 - lcm_recall_1k: 0.0376 - lcm_recall_2k: 0.0964 - lcm_recall_3k: 0.1509 - lcm_recall_5k: 0.2498 - lcm_f1_1k: 0.0485 - lcm_f1_2k: 0.0930 - lcm_f1_3k: 0.1196 - lcm_f1_5k: 0.1388 - lcm_accuracy_1k: 0.0687 - lcm_accuracy_2k: 0.1777 - lcm_accuracy_3k: 0.2876 - lcm_accuracy_5k: 0.4448 - lcm_hamming_loss_k: 0.0603 - val_loss: 1.3659 - val_lcm_precision_1k: 0.0303 - val_lcm_precision_2k: 0.1472 - val_lcm_precision_3k: 0.1343 - val_lcm_precision_5k: 0.1223 - val_lcm_recall_1k: 0.0173 - val_lcm_recall_2k: 0.1510 - val_lcm_recall_3k: 0.2049 - val_lcm_recall_5k: 0.3166 - val_lcm_f1_1k: 0.0220 - val_lcm_f1_2k: 0.1489 - val_lcm_f1_3k: 0.1619 - val_lcm_f1_5k: 0.1760 - val_lcm_accuracy_1k: 0.0303 - val_lcm_accuracy_2k: 0.2890 - val_lcm_accuracy_3k: 0.3933 - val_lcm_accuracy_5k: 0.5487 - val_lcm_hamming_loss_k: 0.0624
Epoch 4/100
12/12 [==============================] - ETA: 0s - loss: 1.3103 - lcm_precision_1k: 0.2288 - lcm_precision_2k: 0.1859 - lcm_precision_3k: 0.1704 - lcm_precision_5k: 0.1542 - lcm_recall_1k: 0.1208 - lcm_recall_2k: 0.1924 - lcm_recall_3k: 0.2732 - lcm_recall_5k: 0.3955 - lcm_f1_1k: 0.1580 - lcm_f1_2k: 0.1891 - lcm_f1_3k: 0.2098 - lcm_f1_5k: 0.2218 - lcm_accuracy_1k: 0.2288 - lcm_accuracy_2k: 0.3611 - lcm_accuracy_3k: 0.4694 - lcm_accuracy_5k: 0.6487 - lcm_hamming_loss_k: 0.0537
Epoch 00004: val_loss improved from 1.36593 to 1.23494, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 1.3103 - lcm_precision_1k: 0.2288 - lcm_precision_2k: 0.1859 - lcm_precision_3k: 0.1704 - lcm_precision_5k: 0.1542 - lcm_recall_1k: 0.1208 - lcm_recall_2k: 0.1924 - lcm_recall_3k: 0.2732 - lcm_recall_5k: 0.3955 - lcm_f1_1k: 0.1580 - lcm_f1_2k: 0.1891 - lcm_f1_3k: 0.2098 - lcm_f1_5k: 0.2218 - lcm_accuracy_1k: 0.2288 - lcm_accuracy_2k: 0.3611 - lcm_accuracy_3k: 0.4694 - lcm_accuracy_5k: 0.6487 - lcm_hamming_loss_k: 0.0537 - val_loss: 1.2349 - val_lcm_precision_1k: 0.1433 - val_lcm_precision_2k: 0.1963 - val_lcm_precision_3k: 0.2021 - val_lcm_precision_5k: 0.1827 - val_lcm_recall_1k: 0.0759 - val_lcm_recall_2k: 0.2069 - val_lcm_recall_3k: 0.3206 - val_lcm_recall_5k: 0.4610 - val_lcm_f1_1k: 0.0990 - val_lcm_f1_2k: 0.2013 - val_lcm_f1_3k: 0.2476 - val_lcm_f1_5k: 0.2611 - val_lcm_accuracy_1k: 0.1433 - val_lcm_accuracy_2k: 0.3848 - val_lcm_accuracy_3k: 0.5500 - val_lcm_accuracy_5k: 0.7313 - val_lcm_hamming_loss_k: 0.0578
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 1.1539 - lcm_precision_1k: 0.0820 - lcm_precision_2k: 0.2278 - lcm_precision_3k: 0.2362 - lcm_precision_5k: 0.2070 - lcm_recall_1k: 0.0448 - lcm_recall_2k: 0.2367 - lcm_recall_3k: 0.3737 - lcm_recall_5k: 0.5321 - lcm_f1_1k: 0.0578 - lcm_f1_2k: 0.2321 - lcm_f1_3k: 0.2893 - lcm_f1_5k: 0.2979 - lcm_accuracy_1k: 0.0820 - lcm_accuracy_2k: 0.4425 - lcm_accuracy_3k: 0.6105 - lcm_accuracy_5k: 0.7642 - lcm_hamming_loss_k: 0.0595
Epoch 00005: val_loss improved from 1.23494 to 1.08872, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 1.1529 - lcm_precision_1k: 0.0774 - lcm_precision_2k: 0.2294 - lcm_precision_3k: 0.2381 - lcm_precision_5k: 0.2088 - lcm_recall_1k: 0.0419 - lcm_recall_2k: 0.2379 - lcm_recall_3k: 0.3743 - lcm_recall_5k: 0.5335 - lcm_f1_1k: 0.0541 - lcm_f1_2k: 0.2335 - lcm_f1_3k: 0.2909 - lcm_f1_5k: 0.3000 - lcm_accuracy_1k: 0.0774 - lcm_accuracy_2k: 0.4456 - lcm_accuracy_3k: 0.6142 - lcm_accuracy_5k: 0.7687 - lcm_hamming_loss_k: 0.0599 - val_loss: 1.0887 - val_lcm_precision_1k: 0.1181 - val_lcm_precision_2k: 0.2420 - val_lcm_precision_3k: 0.2666 - val_lcm_precision_5k: 0.2360 - val_lcm_recall_1k: 0.0708 - val_lcm_recall_2k: 0.2544 - val_lcm_recall_3k: 0.3991 - val_lcm_recall_5k: 0.5852 - val_lcm_f1_1k: 0.0883 - val_lcm_f1_2k: 0.2477 - val_lcm_f1_3k: 0.3191 - val_lcm_f1_5k: 0.3359 - val_lcm_accuracy_1k: 0.1181 - val_lcm_accuracy_2k: 0.4800 - val_lcm_accuracy_3k: 0.6421 - val_lcm_accuracy_5k: 0.8015 - val_lcm_hamming_loss_k: 0.0588
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.9832 - lcm_precision_1k: 0.3697 - lcm_precision_2k: 0.3381 - lcm_precision_3k: 0.3251 - lcm_precision_5k: 0.2663 - lcm_recall_1k: 0.1968 - lcm_recall_2k: 0.3514 - lcm_recall_3k: 0.4998 - lcm_recall_5k: 0.6707 - lcm_f1_1k: 0.2567 - lcm_f1_2k: 0.3446 - lcm_f1_3k: 0.3939 - lcm_f1_5k: 0.3812 - lcm_accuracy_1k: 0.3697 - lcm_accuracy_2k: 0.6058 - lcm_accuracy_3k: 0.7369 - lcm_accuracy_5k: 0.8629 - lcm_hamming_loss_k: 0.0480
Epoch 00006: val_loss improved from 1.08872 to 0.95159, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.9796 - lcm_precision_1k: 0.3773 - lcm_precision_2k: 0.3456 - lcm_precision_3k: 0.3284 - lcm_precision_5k: 0.2676 - lcm_recall_1k: 0.2019 - lcm_recall_2k: 0.3630 - lcm_recall_3k: 0.5084 - lcm_recall_5k: 0.6788 - lcm_f1_1k: 0.2629 - lcm_f1_2k: 0.3540 - lcm_f1_3k: 0.3990 - lcm_f1_5k: 0.3838 - lcm_accuracy_1k: 0.3773 - lcm_accuracy_2k: 0.6138 - lcm_accuracy_3k: 0.7409 - lcm_accuracy_5k: 0.8673 - lcm_hamming_loss_k: 0.0475 - val_loss: 0.9516 - val_lcm_precision_1k: 0.5193 - val_lcm_precision_2k: 0.4132 - val_lcm_precision_3k: 0.3589 - val_lcm_precision_5k: 0.2781 - val_lcm_recall_1k: 0.2729 - val_lcm_recall_2k: 0.4346 - val_lcm_recall_3k: 0.5586 - val_lcm_recall_5k: 0.6976 - val_lcm_f1_1k: 0.3571 - val_lcm_f1_2k: 0.4232 - val_lcm_f1_3k: 0.4367 - val_lcm_f1_5k: 0.3973 - val_lcm_accuracy_1k: 0.5193 - val_lcm_accuracy_2k: 0.6680 - val_lcm_accuracy_3k: 0.7776 - val_lcm_accuracy_5k: 0.8649 - val_lcm_hamming_loss_k: 0.0425
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.8344 - lcm_precision_1k: 0.5952 - lcm_precision_2k: 0.5053 - lcm_precision_3k: 0.4155 - lcm_precision_5k: 0.3046 - lcm_recall_1k: 0.3241 - lcm_recall_2k: 0.5405 - lcm_recall_3k: 0.6506 - lcm_recall_5k: 0.7695 - lcm_f1_1k: 0.4196 - lcm_f1_2k: 0.5222 - lcm_f1_3k: 0.5071 - lcm_f1_5k: 0.4364 - lcm_accuracy_1k: 0.5952 - lcm_accuracy_2k: 0.7788 - lcm_accuracy_3k: 0.8548 - lcm_accuracy_5k: 0.9208 - lcm_hamming_loss_k: 0.0387
Epoch 00007: val_loss improved from 0.95159 to 0.77263, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.8309 - lcm_precision_1k: 0.6002 - lcm_precision_2k: 0.5049 - lcm_precision_3k: 0.4162 - lcm_precision_5k: 0.3053 - lcm_recall_1k: 0.3293 - lcm_recall_2k: 0.5418 - lcm_recall_3k: 0.6535 - lcm_recall_5k: 0.7726 - lcm_f1_1k: 0.4252 - lcm_f1_2k: 0.5226 - lcm_f1_3k: 0.5085 - lcm_f1_5k: 0.4377 - lcm_accuracy_1k: 0.6002 - lcm_accuracy_2k: 0.7793 - lcm_accuracy_3k: 0.8560 - lcm_accuracy_5k: 0.9220 - lcm_hamming_loss_k: 0.0384 - val_loss: 0.7726 - val_lcm_precision_1k: 0.5966 - val_lcm_precision_2k: 0.5049 - val_lcm_precision_3k: 0.4253 - val_lcm_precision_5k: 0.3186 - val_lcm_recall_1k: 0.3338 - val_lcm_recall_2k: 0.5331 - val_lcm_recall_3k: 0.6432 - val_lcm_recall_5k: 0.7813 - val_lcm_f1_1k: 0.4278 - val_lcm_f1_2k: 0.5183 - val_lcm_f1_3k: 0.5116 - val_lcm_f1_5k: 0.4522 - val_lcm_accuracy_1k: 0.5966 - val_lcm_accuracy_2k: 0.7965 - val_lcm_accuracy_3k: 0.8523 - val_lcm_accuracy_5k: 0.9198 - val_lcm_hamming_loss_k: 0.0393
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6885 - lcm_precision_1k: 0.7131 - lcm_precision_2k: 0.5716 - lcm_precision_3k: 0.4621 - lcm_precision_5k: 0.3288 - lcm_recall_1k: 0.4085 - lcm_recall_2k: 0.6146 - lcm_recall_3k: 0.7178 - lcm_recall_5k: 0.8246 - lcm_f1_1k: 0.5191 - lcm_f1_2k: 0.5921 - lcm_f1_3k: 0.5622 - lcm_f1_5k: 0.4701 - lcm_accuracy_1k: 0.7131 - lcm_accuracy_2k: 0.8562 - lcm_accuracy_3k: 0.9016 - lcm_accuracy_5k: 0.9457 - lcm_hamming_loss_k: 0.0339
Epoch 00008: val_loss improved from 0.77263 to 0.70441, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.6866 - lcm_precision_1k: 0.7137 - lcm_precision_2k: 0.5713 - lcm_precision_3k: 0.4635 - lcm_precision_5k: 0.3295 - lcm_recall_1k: 0.4096 - lcm_recall_2k: 0.6147 - lcm_recall_3k: 0.7210 - lcm_recall_5k: 0.8279 - lcm_f1_1k: 0.5203 - lcm_f1_2k: 0.5920 - lcm_f1_3k: 0.5642 - lcm_f1_5k: 0.4713 - lcm_accuracy_1k: 0.7137 - lcm_accuracy_2k: 0.8568 - lcm_accuracy_3k: 0.9039 - lcm_accuracy_5k: 0.9480 - lcm_hamming_loss_k: 0.0338 - val_loss: 0.7044 - val_lcm_precision_1k: 0.6832 - val_lcm_precision_2k: 0.5577 - val_lcm_precision_3k: 0.4597 - val_lcm_precision_5k: 0.3292 - val_lcm_recall_1k: 0.3913 - val_lcm_recall_2k: 0.5826 - val_lcm_recall_3k: 0.6977 - val_lcm_recall_5k: 0.8089 - val_lcm_f1_1k: 0.4972 - val_lcm_f1_2k: 0.5696 - val_lcm_f1_3k: 0.5538 - val_lcm_f1_5k: 0.4676 - val_lcm_accuracy_1k: 0.6832 - val_lcm_accuracy_2k: 0.8214 - val_lcm_accuracy_3k: 0.8831 - val_lcm_accuracy_5k: 0.9322 - val_lcm_hamming_loss_k: 0.0358
Epoch 9/100
12/12 [==============================] - ETA: 0s - loss: 0.6057 - lcm_precision_1k: 0.7785 - lcm_precision_2k: 0.6214 - lcm_precision_3k: 0.5008 - lcm_precision_5k: 0.3487 - lcm_recall_1k: 0.4496 - lcm_recall_2k: 0.6638 - lcm_recall_3k: 0.7756 - lcm_recall_5k: 0.8721 - lcm_f1_1k: 0.5698 - lcm_f1_2k: 0.6417 - lcm_f1_3k: 0.6084 - lcm_f1_5k: 0.4981 - lcm_accuracy_1k: 0.7785 - lcm_accuracy_2k: 0.8960 - lcm_accuracy_3k: 0.9410 - lcm_accuracy_5k: 0.9703 - lcm_hamming_loss_k: 0.0311 ETA: 0s - loss: 0.6386 - lcm_precision_1k: 0.7735 - lcm_precision_2k: 0.6280 - lcm_precision_3k: 0.4981 - lcm_precision_5k: 0.3532 - lcm_recall_1k: 0.4347 - lcm_recall_2k: 0.6554 - lcm_recall_3k: 0.7493 - lcm_recall_5k: 0.8601 - lcm_f1_1k: 0.5565 - lcm_f1_2k: 0.6414 - lcm_f1_3k: 0.5983 - lcm_f1_5k: 0.5007 - lcm_accuracy_1k: 0.7735 - lcm_accuracy_2k: 0.9003 - lcm_accuracy_3k: 0.9375 - lcm_accuracy_5k: 0.9649 - lcm_hamming_
Epoch 00009: val_loss improved from 0.70441 to 0.64718, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.6057 - lcm_precision_1k: 0.7785 - lcm_precision_2k: 0.6214 - lcm_precision_3k: 0.5008 - lcm_precision_5k: 0.3487 - lcm_recall_1k: 0.4496 - lcm_recall_2k: 0.6638 - lcm_recall_3k: 0.7756 - lcm_recall_5k: 0.8721 - lcm_f1_1k: 0.5698 - lcm_f1_2k: 0.6417 - lcm_f1_3k: 0.6084 - lcm_f1_5k: 0.4981 - lcm_accuracy_1k: 0.7785 - lcm_accuracy_2k: 0.8960 - lcm_accuracy_3k: 0.9410 - lcm_accuracy_5k: 0.9703 - lcm_hamming_loss_k: 0.0311 - val_loss: 0.6472 - val_lcm_precision_1k: 0.7561 - val_lcm_precision_2k: 0.5991 - val_lcm_precision_3k: 0.4866 - val_lcm_precision_5k: 0.3398 - val_lcm_recall_1k: 0.4270 - val_lcm_recall_2k: 0.6235 - val_lcm_recall_3k: 0.7329 - val_lcm_recall_5k: 0.8321 - val_lcm_f1_1k: 0.5451 - val_lcm_f1_2k: 0.6102 - val_lcm_f1_3k: 0.5842 - val_lcm_f1_5k: 0.4821 - val_lcm_accuracy_1k: 0.7561 - val_lcm_accuracy_2k: 0.8668 - val_lcm_accuracy_3k: 0.9024 - val_lcm_accuracy_5k: 0.9404 - val_lcm_hamming_loss_k: 0.0328
Epoch 10/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5651 - lcm_precision_1k: 0.8150 - lcm_precision_2k: 0.6413 - lcm_precision_3k: 0.5108 - lcm_precision_5k: 0.3529 - lcm_recall_1k: 0.4725 - lcm_recall_2k: 0.6900 - lcm_recall_3k: 0.7939 - lcm_recall_5k: 0.8842 - lcm_f1_1k: 0.5981 - lcm_f1_2k: 0.6647 - lcm_f1_3k: 0.6216 - lcm_f1_5k: 0.5044 - lcm_accuracy_1k: 0.8150 - lcm_accuracy_2k: 0.9173 - lcm_accuracy_3k: 0.9577 - lcm_accuracy_5k: 0.9798 - lcm_hamming_loss_k: 0.0297
Epoch 00010: val_loss did not improve from 0.64718
12/12 [==============================] - 1s 71ms/step - loss: 0.5652 - lcm_precision_1k: 0.8147 - lcm_precision_2k: 0.6404 - lcm_precision_3k: 0.5113 - lcm_precision_5k: 0.3530 - lcm_recall_1k: 0.4719 - lcm_recall_2k: 0.6885 - lcm_recall_3k: 0.7938 - lcm_recall_5k: 0.8840 - lcm_f1_1k: 0.5976 - lcm_f1_2k: 0.6635 - lcm_f1_3k: 0.6219 - lcm_f1_5k: 0.5045 - lcm_accuracy_1k: 0.8147 - lcm_accuracy_2k: 0.9150 - lcm_accuracy_3k: 0.9553 - lcm_accuracy_5k: 0.9788 - lcm_hamming_loss_k: 0.0297 - val_loss: 0.6510 - val_lcm_precision_1k: 0.7485 - val_lcm_precision_2k: 0.6000 - val_lcm_precision_3k: 0.4789 - val_lcm_precision_5k: 0.3365 - val_lcm_recall_1k: 0.4260 - val_lcm_recall_2k: 0.6264 - val_lcm_recall_3k: 0.7212 - val_lcm_recall_5k: 0.8239 - val_lcm_f1_1k: 0.5424 - val_lcm_f1_2k: 0.6121 - val_lcm_f1_3k: 0.5750 - val_lcm_f1_5k: 0.4774 - val_lcm_accuracy_1k: 0.7485 - val_lcm_accuracy_2k: 0.8532 - val_lcm_accuracy_3k: 0.8914 - val_lcm_accuracy_5k: 0.9344 - val_lcm_hamming_loss_k: 0.0331
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5158 - lcm_precision_1k: 0.8374 - lcm_precision_2k: 0.6603 - lcm_precision_3k: 0.5304 - lcm_precision_5k: 0.3621 - lcm_recall_1k: 0.4901 - lcm_recall_2k: 0.7105 - lcm_recall_3k: 0.8223 - lcm_recall_5k: 0.9057 - lcm_f1_1k: 0.6182 - lcm_f1_2k: 0.6843 - lcm_f1_3k: 0.6447 - lcm_f1_5k: 0.5172 - lcm_accuracy_1k: 0.8374 - lcm_accuracy_2k: 0.9347 - lcm_accuracy_3k: 0.9680 - lcm_accuracy_5k: 0.9855 - lcm_hamming_loss_k: 0.0286
Epoch 00011: val_loss improved from 0.64718 to 0.60318, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 90ms/step - loss: 0.5187 - lcm_precision_1k: 0.8336 - lcm_precision_2k: 0.6611 - lcm_precision_3k: 0.5302 - lcm_precision_5k: 0.3630 - lcm_recall_1k: 0.4841 - lcm_recall_2k: 0.7062 - lcm_recall_3k: 0.8165 - lcm_recall_5k: 0.9015 - lcm_f1_1k: 0.6122 - lcm_f1_2k: 0.6827 - lcm_f1_3k: 0.6427 - lcm_f1_5k: 0.5174 - lcm_accuracy_1k: 0.8336 - lcm_accuracy_2k: 0.9309 - lcm_accuracy_3k: 0.9653 - lcm_accuracy_5k: 0.9840 - lcm_hamming_loss_k: 0.0291 - val_loss: 0.6032 - val_lcm_precision_1k: 0.7611 - val_lcm_precision_2k: 0.6072 - val_lcm_precision_3k: 0.4957 - val_lcm_precision_5k: 0.3489 - val_lcm_recall_1k: 0.4341 - val_lcm_recall_2k: 0.6338 - val_lcm_recall_3k: 0.7490 - val_lcm_recall_5k: 0.8532 - val_lcm_f1_1k: 0.5521 - val_lcm_f1_2k: 0.6195 - val_lcm_f1_3k: 0.5960 - val_lcm_f1_5k: 0.4948 - val_lcm_accuracy_1k: 0.7611 - val_lcm_accuracy_2k: 0.8707 - val_lcm_accuracy_3k: 0.9047 - val_lcm_accuracy_5k: 0.9550 - val_lcm_hamming_loss_k: 0.0326
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4970 - lcm_precision_1k: 0.8533 - lcm_precision_2k: 0.6703 - lcm_precision_3k: 0.5369 - lcm_precision_5k: 0.3676 - lcm_recall_1k: 0.4971 - lcm_recall_2k: 0.7169 - lcm_recall_3k: 0.8288 - lcm_recall_5k: 0.9134 - lcm_f1_1k: 0.6282 - lcm_f1_2k: 0.6927 - lcm_f1_3k: 0.6516 - lcm_f1_5k: 0.5241 - lcm_accuracy_1k: 0.8533 - lcm_accuracy_2k: 0.9386 - lcm_accuracy_3k: 0.9716 - lcm_accuracy_5k: 0.9872 - lcm_hamming_loss_k: 0.0282
Epoch 00012: val_loss did not improve from 0.60318
12/12 [==============================] - 1s 70ms/step - loss: 0.4966 - lcm_precision_1k: 0.8526 - lcm_precision_2k: 0.6707 - lcm_precision_3k: 0.5360 - lcm_precision_5k: 0.3666 - lcm_recall_1k: 0.4975 - lcm_recall_2k: 0.7185 - lcm_recall_3k: 0.8293 - lcm_recall_5k: 0.9136 - lcm_f1_1k: 0.6282 - lcm_f1_2k: 0.6937 - lcm_f1_3k: 0.6510 - lcm_f1_5k: 0.5232 - lcm_accuracy_1k: 0.8526 - lcm_accuracy_2k: 0.9377 - lcm_accuracy_3k: 0.9718 - lcm_accuracy_5k: 0.9878 - lcm_hamming_loss_k: 0.0281 - val_loss: 0.6128 - val_lcm_precision_1k: 0.7657 - val_lcm_precision_2k: 0.6013 - val_lcm_precision_3k: 0.4924 - val_lcm_precision_5k: 0.3441 - val_lcm_recall_1k: 0.4397 - val_lcm_recall_2k: 0.6369 - val_lcm_recall_3k: 0.7478 - val_lcm_recall_5k: 0.8464 - val_lcm_f1_1k: 0.5581 - val_lcm_f1_2k: 0.6177 - val_lcm_f1_3k: 0.5930 - val_lcm_f1_5k: 0.4888 - val_lcm_accuracy_1k: 0.7657 - val_lcm_accuracy_2k: 0.8640 - val_lcm_accuracy_3k: 0.9089 - val_lcm_accuracy_5k: 0.9512 - val_lcm_hamming_loss_k: 0.0324
Epoch 13/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4675 - lcm_precision_1k: 0.8714 - lcm_precision_2k: 0.6928 - lcm_precision_3k: 0.5468 - lcm_precision_5k: 0.3722 - lcm_recall_1k: 0.5117 - lcm_recall_2k: 0.7395 - lcm_recall_3k: 0.8436 - lcm_recall_5k: 0.9241 - lcm_f1_1k: 0.6447 - lcm_f1_2k: 0.7153 - lcm_f1_3k: 0.6634 - lcm_f1_5k: 0.5305 - lcm_accuracy_1k: 0.8714 - lcm_accuracy_2k: 0.9510 - lcm_accuracy_3k: 0.9770 - lcm_accuracy_5k: 0.9901 - lcm_hamming_loss_k: 0.0273
Epoch 00013: val_loss improved from 0.60318 to 0.57133, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.4675 - lcm_precision_1k: 0.8676 - lcm_precision_2k: 0.6924 - lcm_precision_3k: 0.5476 - lcm_precision_5k: 0.3732 - lcm_recall_1k: 0.5077 - lcm_recall_2k: 0.7379 - lcm_recall_3k: 0.8440 - lcm_recall_5k: 0.9251 - lcm_f1_1k: 0.6405 - lcm_f1_2k: 0.7143 - lcm_f1_3k: 0.6641 - lcm_f1_5k: 0.5317 - lcm_accuracy_1k: 0.8676 - lcm_accuracy_2k: 0.9507 - lcm_accuracy_3k: 0.9783 - lcm_accuracy_5k: 0.9904 - lcm_hamming_loss_k: 0.0276 - val_loss: 0.5713 - val_lcm_precision_1k: 0.7927 - val_lcm_precision_2k: 0.6314 - val_lcm_precision_3k: 0.5079 - val_lcm_precision_5k: 0.3563 - val_lcm_recall_1k: 0.4531 - val_lcm_recall_2k: 0.6581 - val_lcm_recall_3k: 0.7690 - val_lcm_recall_5k: 0.8713 - val_lcm_f1_1k: 0.5757 - val_lcm_f1_2k: 0.6438 - val_lcm_f1_3k: 0.6111 - val_lcm_f1_5k: 0.5053 - val_lcm_accuracy_1k: 0.7927 - val_lcm_accuracy_2k: 0.8908 - val_lcm_accuracy_3k: 0.9260 - val_lcm_accuracy_5k: 0.9595 - val_lcm_hamming_loss_k: 0.0313
Epoch 14/100
12/12 [==============================] - ETA: 0s - loss: 0.4398 - lcm_precision_1k: 0.8852 - lcm_precision_2k: 0.7065 - lcm_precision_3k: 0.5597 - lcm_precision_5k: 0.3794 - lcm_recall_1k: 0.5166 - lcm_recall_2k: 0.7526 - lcm_recall_3k: 0.8597 - lcm_recall_5k: 0.9386 - lcm_f1_1k: 0.6524 - lcm_f1_2k: 0.7287 - lcm_f1_3k: 0.6779 - lcm_f1_5k: 0.5403 - lcm_accuracy_1k: 0.8852 - lcm_accuracy_2k: 0.9571 - lcm_accuracy_3k: 0.9831 - lcm_accuracy_5k: 0.9940 - lcm_hamming_loss_k: 0.0268
Epoch 00014: val_loss improved from 0.57133 to 0.56964, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.4398 - lcm_precision_1k: 0.8852 - lcm_precision_2k: 0.7065 - lcm_precision_3k: 0.5597 - lcm_precision_5k: 0.3794 - lcm_recall_1k: 0.5166 - lcm_recall_2k: 0.7526 - lcm_recall_3k: 0.8597 - lcm_recall_5k: 0.9386 - lcm_f1_1k: 0.6524 - lcm_f1_2k: 0.7287 - lcm_f1_3k: 0.6779 - lcm_f1_5k: 0.5403 - lcm_accuracy_1k: 0.8852 - lcm_accuracy_2k: 0.9571 - lcm_accuracy_3k: 0.9831 - lcm_accuracy_5k: 0.9940 - lcm_hamming_loss_k: 0.0268 - val_loss: 0.5696 - val_lcm_precision_1k: 0.7928 - val_lcm_precision_2k: 0.6301 - val_lcm_precision_3k: 0.5143 - val_lcm_precision_5k: 0.3547 - val_lcm_recall_1k: 0.4519 - val_lcm_recall_2k: 0.6588 - val_lcm_recall_3k: 0.7775 - val_lcm_recall_5k: 0.8623 - val_lcm_f1_1k: 0.5750 - val_lcm_f1_2k: 0.6434 - val_lcm_f1_3k: 0.6185 - val_lcm_f1_5k: 0.5023 - val_lcm_accuracy_1k: 0.7928 - val_lcm_accuracy_2k: 0.8943 - val_lcm_accuracy_3k: 0.9227 - val_lcm_accuracy_5k: 0.9489 - val_lcm_hamming_loss_k: 0.0313
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4249 - lcm_precision_1k: 0.8998 - lcm_precision_2k: 0.7122 - lcm_precision_3k: 0.5611 - lcm_precision_5k: 0.3770 - lcm_recall_1k: 0.5291 - lcm_recall_2k: 0.7630 - lcm_recall_3k: 0.8642 - lcm_recall_5k: 0.9355 - lcm_f1_1k: 0.6662 - lcm_f1_2k: 0.7366 - lcm_f1_3k: 0.6803 - lcm_f1_5k: 0.5373 - lcm_accuracy_1k: 0.8998 - lcm_accuracy_2k: 0.9673 - lcm_accuracy_3k: 0.9844 - lcm_accuracy_5k: 0.9950 - lcm_hamming_loss_k: 0.0262
Epoch 00015: val_loss improved from 0.56964 to 0.56104, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.4257 - lcm_precision_1k: 0.9001 - lcm_precision_2k: 0.7118 - lcm_precision_3k: 0.5619 - lcm_precision_5k: 0.3778 - lcm_recall_1k: 0.5297 - lcm_recall_2k: 0.7616 - lcm_recall_3k: 0.8640 - lcm_recall_5k: 0.9355 - lcm_f1_1k: 0.6668 - lcm_f1_2k: 0.7358 - lcm_f1_3k: 0.6809 - lcm_f1_5k: 0.5382 - lcm_accuracy_1k: 0.9001 - lcm_accuracy_2k: 0.9657 - lcm_accuracy_3k: 0.9846 - lcm_accuracy_5k: 0.9955 - lcm_hamming_loss_k: 0.0263 - val_loss: 0.5610 - val_lcm_precision_1k: 0.8119 - val_lcm_precision_2k: 0.6298 - val_lcm_precision_3k: 0.5120 - val_lcm_precision_5k: 0.3585 - val_lcm_recall_1k: 0.4599 - val_lcm_recall_2k: 0.6567 - val_lcm_recall_3k: 0.7730 - val_lcm_recall_5k: 0.8728 - val_lcm_f1_1k: 0.5865 - val_lcm_f1_2k: 0.6424 - val_lcm_f1_3k: 0.6154 - val_lcm_f1_5k: 0.5079 - val_lcm_accuracy_1k: 0.8119 - val_lcm_accuracy_2k: 0.8856 - val_lcm_accuracy_3k: 0.9215 - val_lcm_accuracy_5k: 0.9567 - val_lcm_hamming_loss_k: 0.0305
Epoch 16/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4026 - lcm_precision_1k: 0.9073 - lcm_precision_2k: 0.7232 - lcm_precision_3k: 0.5747 - lcm_precision_5k: 0.3866 - lcm_recall_1k: 0.5294 - lcm_recall_2k: 0.7674 - lcm_recall_3k: 0.8775 - lcm_recall_5k: 0.9486 - lcm_f1_1k: 0.6686 - lcm_f1_2k: 0.7446 - lcm_f1_3k: 0.6945 - lcm_f1_5k: 0.5493 - lcm_accuracy_1k: 0.9073 - lcm_accuracy_2k: 0.9670 - lcm_accuracy_3k: 0.9894 - lcm_accuracy_5k: 0.9957 - lcm_hamming_loss_k: 0.0261
Epoch 00016: val_loss improved from 0.56104 to 0.55895, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.4011 - lcm_precision_1k: 0.9086 - lcm_precision_2k: 0.7225 - lcm_precision_3k: 0.5722 - lcm_precision_5k: 0.3848 - lcm_recall_1k: 0.5326 - lcm_recall_2k: 0.7704 - lcm_recall_3k: 0.8787 - lcm_recall_5k: 0.9495 - lcm_f1_1k: 0.6715 - lcm_f1_2k: 0.7456 - lcm_f1_3k: 0.6930 - lcm_f1_5k: 0.5476 - lcm_accuracy_1k: 0.9086 - lcm_accuracy_2k: 0.9681 - lcm_accuracy_3k: 0.9897 - lcm_accuracy_5k: 0.9961 - lcm_hamming_loss_k: 0.0258 - val_loss: 0.5590 - val_lcm_precision_1k: 0.8048 - val_lcm_precision_2k: 0.6400 - val_lcm_precision_3k: 0.5119 - val_lcm_precision_5k: 0.3620 - val_lcm_recall_1k: 0.4608 - val_lcm_recall_2k: 0.6714 - val_lcm_recall_3k: 0.7745 - val_lcm_recall_5k: 0.8817 - val_lcm_f1_1k: 0.5852 - val_lcm_f1_2k: 0.6548 - val_lcm_f1_3k: 0.6158 - val_lcm_f1_5k: 0.5128 - val_lcm_accuracy_1k: 0.8048 - val_lcm_accuracy_2k: 0.9013 - val_lcm_accuracy_3k: 0.9285 - val_lcm_accuracy_5k: 0.9618 - val_lcm_hamming_loss_k: 0.0308
Epoch 17/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3745 - lcm_precision_1k: 0.9279 - lcm_precision_2k: 0.7402 - lcm_precision_3k: 0.5844 - lcm_precision_5k: 0.3891 - lcm_recall_1k: 0.5451 - lcm_recall_2k: 0.7868 - lcm_recall_3k: 0.8914 - lcm_recall_5k: 0.9550 - lcm_f1_1k: 0.6867 - lcm_f1_2k: 0.7627 - lcm_f1_3k: 0.7058 - lcm_f1_5k: 0.5529 - lcm_accuracy_1k: 0.9279 - lcm_accuracy_2k: 0.9777 - lcm_accuracy_3k: 0.9922 - lcm_accuracy_5k: 0.9972 - lcm_hamming_loss_k: 0.0252
Epoch 00017: val_loss improved from 0.55895 to 0.55384, saving model to logs/iaaswo-labs-0602-221940/model/checkpoint_labs.h5
12/12 [==============================] - 1s 102ms/step - loss: 0.3756 - lcm_precision_1k: 0.9236 - lcm_precision_2k: 0.7378 - lcm_precision_3k: 0.5831 - lcm_precision_5k: 0.3870 - lcm_recall_1k: 0.5443 - lcm_recall_2k: 0.7875 - lcm_recall_3k: 0.8934 - lcm_recall_5k: 0.9545 - lcm_f1_1k: 0.6848 - lcm_f1_2k: 0.7617 - lcm_f1_3k: 0.7055 - lcm_f1_5k: 0.5506 - lcm_accuracy_1k: 0.9236 - lcm_accuracy_2k: 0.9773 - lcm_accuracy_3k: 0.9923 - lcm_accuracy_5k: 0.9969 - lcm_hamming_loss_k: 0.0252 - val_loss: 0.5538 - val_lcm_precision_1k: 0.8067 - val_lcm_precision_2k: 0.6477 - val_lcm_precision_3k: 0.5220 - val_lcm_precision_5k: 0.3603 - val_lcm_recall_1k: 0.4605 - val_lcm_recall_2k: 0.6801 - val_lcm_recall_3k: 0.7923 - val_lcm_recall_5k: 0.8787 - val_lcm_f1_1k: 0.5854 - val_lcm_f1_2k: 0.6626 - val_lcm_f1_3k: 0.6285 - val_lcm_f1_5k: 0.5106 - val_lcm_accuracy_1k: 0.8067 - val_lcm_accuracy_2k: 0.9081 - val_lcm_accuracy_3k: 0.9430 - val_lcm_accuracy_5k: 0.9622 - val_lcm_hamming_loss_k: 0.0307
Epoch 18/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3550 - lcm_precision_1k: 0.9290 - lcm_precision_2k: 0.7422 - lcm_precision_3k: 0.5877 - lcm_precision_5k: 0.3912 - lcm_recall_1k: 0.5460 - lcm_recall_2k: 0.7893 - lcm_recall_3k: 0.8957 - lcm_recall_5k: 0.9592 - lcm_f1_1k: 0.6877 - lcm_f1_2k: 0.7649 - lcm_f1_3k: 0.7096 - lcm_f1_5k: 0.5556 - lcm_accuracy_1k: 0.9290 - lcm_accuracy_2k: 0.9787 - lcm_accuracy_3k: 0.9940 - lcm_accuracy_5k: 0.9979 - lcm_hamming_loss_k: 0.0251
Epoch 00018: val_loss did not improve from 0.55384
12/12 [==============================] - 1s 69ms/step - loss: 0.3545 - lcm_precision_1k: 0.9311 - lcm_precision_2k: 0.7420 - lcm_precision_3k: 0.5873 - lcm_precision_5k: 0.3904 - lcm_recall_1k: 0.5487 - lcm_recall_2k: 0.7910 - lcm_recall_3k: 0.8966 - lcm_recall_5k: 0.9598 - lcm_f1_1k: 0.6904 - lcm_f1_2k: 0.7656 - lcm_f1_3k: 0.7096 - lcm_f1_5k: 0.5549 - lcm_accuracy_1k: 0.9311 - lcm_accuracy_2k: 0.9800 - lcm_accuracy_3k: 0.9939 - lcm_accuracy_5k: 0.9981 - lcm_hamming_loss_k: 0.0249 - val_loss: 0.5598 - val_lcm_precision_1k: 0.8251 - val_lcm_precision_2k: 0.6492 - val_lcm_precision_3k: 0.5162 - val_lcm_precision_5k: 0.3614 - val_lcm_recall_1k: 0.4723 - val_lcm_recall_2k: 0.6818 - val_lcm_recall_3k: 0.7874 - val_lcm_recall_5k: 0.8808 - val_lcm_f1_1k: 0.5997 - val_lcm_f1_2k: 0.6641 - val_lcm_f1_3k: 0.6228 - val_lcm_f1_5k: 0.5120 - val_lcm_accuracy_1k: 0.8251 - val_lcm_accuracy_2k: 0.9063 - val_lcm_accuracy_3k: 0.9390 - val_lcm_accuracy_5k: 0.9622 - val_lcm_hamming_loss_k: 0.0300
Epoch 19/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3478 - lcm_precision_1k: 0.9332 - lcm_precision_2k: 0.7488 - lcm_precision_3k: 0.5901 - lcm_precision_5k: 0.3923 - lcm_recall_1k: 0.5513 - lcm_recall_2k: 0.8003 - lcm_recall_3k: 0.9024 - lcm_recall_5k: 0.9658 - lcm_f1_1k: 0.6929 - lcm_f1_2k: 0.7735 - lcm_f1_3k: 0.7134 - lcm_f1_5k: 0.5578 - lcm_accuracy_1k: 0.9332 - lcm_accuracy_2k: 0.9844 - lcm_accuracy_3k: 0.9957 - lcm_accuracy_5k: 0.9989 - lcm_hamming_loss_k: 0.0247
Epoch 00019: val_loss did not improve from 0.55384
12/12 [==============================] - 1s 69ms/step - loss: 0.3489 - lcm_precision_1k: 0.9307 - lcm_precision_2k: 0.7497 - lcm_precision_3k: 0.5914 - lcm_precision_5k: 0.3949 - lcm_recall_1k: 0.5477 - lcm_recall_2k: 0.7970 - lcm_recall_3k: 0.9002 - lcm_recall_5k: 0.9657 - lcm_f1_1k: 0.6894 - lcm_f1_2k: 0.7724 - lcm_f1_3k: 0.7136 - lcm_f1_5k: 0.5604 - lcm_accuracy_1k: 0.9307 - lcm_accuracy_2k: 0.9824 - lcm_accuracy_3k: 0.9956 - lcm_accuracy_5k: 0.9990 - lcm_hamming_loss_k: 0.0251 - val_loss: 0.5618 - val_lcm_precision_1k: 0.8067 - val_lcm_precision_2k: 0.6422 - val_lcm_precision_3k: 0.5212 - val_lcm_precision_5k: 0.3577 - val_lcm_recall_1k: 0.4580 - val_lcm_recall_2k: 0.6708 - val_lcm_recall_3k: 0.7877 - val_lcm_recall_5k: 0.8699 - val_lcm_f1_1k: 0.5836 - val_lcm_f1_2k: 0.6555 - val_lcm_f1_3k: 0.6266 - val_lcm_f1_5k: 0.5064 - val_lcm_accuracy_1k: 0.8067 - val_lcm_accuracy_2k: 0.8985 - val_lcm_accuracy_3k: 0.9254 - val_lcm_accuracy_5k: 0.9499 - val_lcm_hamming_loss_k: 0.0307
Epoch 00019: early stopping
39/39 [==============================] - 1s 19ms/step - loss: 0.4067 - lcm_precision_1k: 0.9135 - lcm_precision_2k: 0.7330 - lcm_precision_3k: 0.5784 - lcm_precision_5k: 0.3847 - lcm_recall_1k: 0.5247 - lcm_recall_2k: 0.7631 - lcm_recall_3k: 0.8692 - lcm_recall_5k: 0.9337 - lcm_f1_1k: 0.6656 - lcm_f1_2k: 0.7467 - lcm_f1_3k: 0.6935 - lcm_f1_5k: 0.5441 - lcm_accuracy_1k: 0.9135 - lcm_accuracy_2k: 0.9648 - lcm_accuracy_3k: 0.9832 - lcm_accuracy_5k: 0.9928 - lcm_hamming_loss_k: 0.0266
Best model result:  [0.4066724479198456, 0.9135257005691528, 0.7329948544502258, 0.5783615112304688, 0.38466668128967285, 0.5246948003768921, 0.7630845904350281, 0.8692308068275452, 0.9337307810783386, 0.6655612587928772, 0.7466842532157898, 0.6934881210327148, 0.5441005229949951, 0.9135257005691528, 0.9647537469863892, 0.9831794500350952, 0.9927948117256165, 0.026587987318634987]
2970
742
1238
Model: "model_8"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_8 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_8[0][0
                                                                 ]']                              
                                                                                                  
 permute_4 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_8 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_4[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Model: "model_9"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_8 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_8[0][0
                                                                 ]']                              
                                                                                                  
 permute_4 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_8 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_4[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 49, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 dot_4 (Dot)                    (None, 49)           0           ['label_lcm_emb[0][0]',          
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 49)           2450        ['dot_4[0][0]']                  
                                                                                                  
 concatenate_4 (Concatenate)    (None, 98)           0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 5,092,791
Trainable params: 4,028,391
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 1.5549 - lcm_precision_1k: 0.0991 - lcm_precision_2k: 0.1188 - lcm_precision_3k: 0.1220 - lcm_precision_5k: 0.1124 - lcm_recall_1k: 0.0488 - lcm_recall_2k: 0.1156 - lcm_recall_3k: 0.1735 - lcm_recall_5k: 0.2694 - lcm_f1_1k: 0.0652 - lcm_f1_2k: 0.1171 - lcm_f1_3k: 0.1432 - lcm_f1_5k: 0.1586 - lcm_accuracy_1k: 0.0991 - lcm_accuracy_2k: 0.2315 - lcm_accuracy_3k: 0.3334 - lcm_accuracy_5k: 0.4879 - lcm_hamming_loss_k: 0.1397 ETA: 0s - loss: 1.5748 - lcm_precision_1k: 0.1159 - lcm_precision_2k: 0.1369 - lcm_precision_3k: 0.1324 - lcm_precision_5k: 0.1164 - lcm_recall_1k: 0.0561 - lcm_recall_2k: 0.1324 - lcm_recall_3k: 0.1875 - lcm_recall_5k: 0.2768 - lcm_f1_1k: 0.0755 - lcm_f1_2k: 0.1346 - lcm_f1_3k: 0.1551 - lcm_f1_5k: 0.1638 - lcm_accuracy_1k: 0.1159 - lcm_accuracy_2k: 0.2665 - lcm_accuracy_3k: 0.3607 - lcm_accuracy_5k: 0.5000 - lcm_hamming_loss_k: 0.
Epoch 00001: val_loss improved from inf to 1.44096, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 3s 112ms/step - loss: 1.5507 - lcm_precision_1k: 0.0930 - lcm_precision_2k: 0.1124 - lcm_precision_3k: 0.1169 - lcm_precision_5k: 0.1105 - lcm_recall_1k: 0.0460 - lcm_recall_2k: 0.1093 - lcm_recall_3k: 0.1660 - lcm_recall_5k: 0.2643 - lcm_f1_1k: 0.0614 - lcm_f1_2k: 0.1108 - lcm_f1_3k: 0.1371 - lcm_f1_5k: 0.1558 - lcm_accuracy_1k: 0.0930 - lcm_accuracy_2k: 0.2193 - lcm_accuracy_3k: 0.3203 - lcm_accuracy_5k: 0.4814 - lcm_hamming_loss_k: 0.1479 - val_loss: 1.4410 - val_lcm_precision_1k: 0.0484 - val_lcm_precision_2k: 0.0625 - val_lcm_precision_3k: 0.0767 - val_lcm_precision_5k: 0.0915 - val_lcm_recall_1k: 0.0257 - val_lcm_recall_2k: 0.0631 - val_lcm_recall_3k: 0.1148 - val_lcm_recall_5k: 0.2384 - val_lcm_f1_1k: nan - val_lcm_f1_2k: 0.0625 - val_lcm_f1_3k: 0.0916 - val_lcm_f1_5k: 0.1319 - val_lcm_accuracy_1k: 0.0484 - val_lcm_accuracy_2k: 0.1222 - val_lcm_accuracy_3k: 0.2163 - val_lcm_accuracy_5k: 0.4192 - val_lcm_hamming_loss_k: 0.2134
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4381 - lcm_precision_1k: 0.0373 - lcm_precision_2k: 0.1211 - lcm_precision_3k: 0.1039 - lcm_precision_5k: 0.1011 - lcm_recall_1k: 0.0186 - lcm_recall_2k: 0.1196 - lcm_recall_3k: 0.1522 - lcm_recall_5k: 0.2446 - lcm_f1_1k: 0.0248 - lcm_f1_2k: 0.1202 - lcm_f1_3k: 0.1234 - lcm_f1_5k: 0.1431 - lcm_accuracy_1k: 0.0373 - lcm_accuracy_2k: 0.2408 - lcm_accuracy_3k: 0.3019 - lcm_accuracy_5k: 0.4574 - lcm_hamming_loss_k: 0.1350
Epoch 00002: val_loss improved from 1.44096 to 1.41748, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 1.4369 - lcm_precision_1k: 0.0396 - lcm_precision_2k: 0.1240 - lcm_precision_3k: 0.1057 - lcm_precision_5k: 0.1017 - lcm_recall_1k: 0.0193 - lcm_recall_2k: 0.1221 - lcm_recall_3k: 0.1557 - lcm_recall_5k: 0.2474 - lcm_f1_1k: 0.0258 - lcm_f1_2k: 0.1229 - lcm_f1_3k: 0.1258 - lcm_f1_5k: 0.1441 - lcm_accuracy_1k: 0.0396 - lcm_accuracy_2k: 0.2467 - lcm_accuracy_3k: 0.3075 - lcm_accuracy_5k: 0.4609 - lcm_hamming_loss_k: 0.1297 - val_loss: 1.4175 - val_lcm_precision_1k: 0.0527 - val_lcm_precision_2k: 0.1473 - val_lcm_precision_3k: 0.1147 - val_lcm_precision_5k: 0.1041 - val_lcm_recall_1k: 0.0277 - val_lcm_recall_2k: 0.1587 - val_lcm_recall_3k: 0.1850 - val_lcm_recall_5k: 0.2791 - val_lcm_f1_1k: nan - val_lcm_f1_2k: 0.1521 - val_lcm_f1_3k: 0.1410 - val_lcm_f1_5k: 0.1514 - val_lcm_accuracy_1k: 0.0527 - val_lcm_accuracy_2k: 0.2946 - val_lcm_accuracy_3k: 0.3373 - val_lcm_accuracy_5k: 0.4765 - val_lcm_hamming_loss_k: 0.0697
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 1.4095 - lcm_precision_1k: 0.1179 - lcm_precision_2k: 0.1632 - lcm_precision_3k: 0.1340 - lcm_precision_5k: 0.1121 - lcm_recall_1k: 0.0608 - lcm_recall_2k: 0.1605 - lcm_recall_3k: 0.1962 - lcm_recall_5k: 0.2768 - lcm_f1_1k: 0.0800 - lcm_f1_2k: 0.1617 - lcm_f1_3k: 0.1591 - lcm_f1_5k: 0.1595 - lcm_accuracy_1k: 0.1179 - lcm_accuracy_2k: 0.3253 - lcm_accuracy_3k: 0.3899 - lcm_accuracy_5k: 0.5035 - lcm_hamming_loss_k: 0.0638
Epoch 00003: val_loss improved from 1.41748 to 1.39351, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 1.4096 - lcm_precision_1k: 0.1184 - lcm_precision_2k: 0.1642 - lcm_precision_3k: 0.1362 - lcm_precision_5k: 0.1132 - lcm_recall_1k: 0.0607 - lcm_recall_2k: 0.1611 - lcm_recall_3k: 0.1990 - lcm_recall_5k: 0.2784 - lcm_f1_1k: 0.0801 - lcm_f1_2k: 0.1625 - lcm_f1_3k: 0.1616 - lcm_f1_5k: 0.1610 - lcm_accuracy_1k: 0.1184 - lcm_accuracy_2k: 0.3274 - lcm_accuracy_3k: 0.3964 - lcm_accuracy_5k: 0.5081 - lcm_hamming_loss_k: 0.0640 - val_loss: 1.3935 - val_lcm_precision_1k: 0.0973 - val_lcm_precision_2k: 0.1473 - val_lcm_precision_3k: 0.1379 - val_lcm_precision_5k: 0.1094 - val_lcm_recall_1k: 0.0559 - val_lcm_recall_2k: 0.1587 - val_lcm_recall_3k: 0.2178 - val_lcm_recall_5k: 0.2976 - val_lcm_f1_1k: 0.0707 - val_lcm_f1_2k: 0.1521 - val_lcm_f1_3k: 0.1681 - val_lcm_f1_5k: 0.1597 - val_lcm_accuracy_1k: 0.0973 - val_lcm_accuracy_2k: 0.2946 - val_lcm_accuracy_3k: 0.4054 - val_lcm_accuracy_5k: 0.4988 - val_lcm_hamming_loss_k: 0.0647
Epoch 4/100
11/12 [==========================>...] - ETA: 0s - loss: 1.3813 - lcm_precision_1k: 0.0625 - lcm_precision_2k: 0.1623 - lcm_precision_3k: 0.1487 - lcm_precision_5k: 0.1173 - lcm_recall_1k: 0.0328 - lcm_recall_2k: 0.1602 - lcm_recall_3k: 0.2159 - lcm_recall_5k: 0.2900 - lcm_f1_1k: 0.0429 - lcm_f1_2k: 0.1612 - lcm_f1_3k: 0.1760 - lcm_f1_5k: 0.1670 - lcm_accuracy_1k: 0.0625 - lcm_accuracy_2k: 0.3235 - lcm_accuracy_3k: 0.4332 - lcm_accuracy_5k: 0.5167 - lcm_hamming_loss_k: 0.0662
Epoch 00004: val_loss improved from 1.39351 to 1.36215, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 1.3796 - lcm_precision_1k: 0.0638 - lcm_precision_2k: 0.1647 - lcm_precision_3k: 0.1509 - lcm_precision_5k: 0.1185 - lcm_recall_1k: 0.0331 - lcm_recall_2k: 0.1613 - lcm_recall_3k: 0.2182 - lcm_recall_5k: 0.2928 - lcm_f1_1k: 0.0434 - lcm_f1_2k: 0.1629 - lcm_f1_3k: 0.1784 - lcm_f1_5k: 0.1687 - lcm_accuracy_1k: 0.0638 - lcm_accuracy_2k: 0.3285 - lcm_accuracy_3k: 0.4383 - lcm_accuracy_5k: 0.5223 - lcm_hamming_loss_k: 0.0659 - val_loss: 1.3621 - val_lcm_precision_1k: 0.1468 - val_lcm_precision_2k: 0.1473 - val_lcm_precision_3k: 0.1477 - val_lcm_precision_5k: 0.1136 - val_lcm_recall_1k: 0.0814 - val_lcm_recall_2k: 0.1587 - val_lcm_recall_3k: 0.2334 - val_lcm_recall_5k: 0.3075 - val_lcm_f1_1k: 0.1043 - val_lcm_f1_2k: 0.1521 - val_lcm_f1_3k: 0.1803 - val_lcm_f1_5k: 0.1657 - val_lcm_accuracy_1k: 0.1468 - val_lcm_accuracy_2k: 0.2946 - val_lcm_accuracy_3k: 0.4324 - val_lcm_accuracy_5k: 0.5079 - val_lcm_hamming_loss_k: 0.0621
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 1.3481 - lcm_precision_1k: 0.2780 - lcm_precision_2k: 0.1655 - lcm_precision_3k: 0.1547 - lcm_precision_5k: 0.1219 - lcm_recall_1k: 0.1389 - lcm_recall_2k: 0.1630 - lcm_recall_3k: 0.2246 - lcm_recall_5k: 0.3017 - lcm_f1_1k: 0.1851 - lcm_f1_2k: 0.1641 - lcm_f1_3k: 0.1831 - lcm_f1_5k: 0.1736 - lcm_accuracy_1k: 0.2780 - lcm_accuracy_2k: 0.3299 - lcm_accuracy_3k: 0.4478 - lcm_accuracy_5k: 0.5320 - lcm_hamming_loss_k: 0.0542
Epoch 00005: val_loss improved from 1.36215 to 1.34024, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 1.3482 - lcm_precision_1k: 0.2771 - lcm_precision_2k: 0.1631 - lcm_precision_3k: 0.1541 - lcm_precision_5k: 0.1210 - lcm_recall_1k: 0.1374 - lcm_recall_2k: 0.1597 - lcm_recall_3k: 0.2230 - lcm_recall_5k: 0.2985 - lcm_f1_1k: 0.1835 - lcm_f1_2k: 0.1612 - lcm_f1_3k: 0.1822 - lcm_f1_5k: 0.1721 - lcm_accuracy_1k: 0.2771 - lcm_accuracy_2k: 0.3251 - lcm_accuracy_3k: 0.4457 - lcm_accuracy_5k: 0.5271 - lcm_hamming_loss_k: 0.0542 - val_loss: 1.3402 - val_lcm_precision_1k: 0.2800 - val_lcm_precision_2k: 0.1479 - val_lcm_precision_3k: 0.1483 - val_lcm_precision_5k: 0.1143 - val_lcm_recall_1k: 0.1523 - val_lcm_recall_2k: 0.1591 - val_lcm_recall_3k: 0.2359 - val_lcm_recall_5k: 0.3118 - val_lcm_f1_1k: 0.1964 - val_lcm_f1_2k: 0.1526 - val_lcm_f1_3k: 0.1814 - val_lcm_f1_5k: 0.1671 - val_lcm_accuracy_1k: 0.2800 - val_lcm_accuracy_2k: 0.2959 - val_lcm_accuracy_3k: 0.4277 - val_lcm_accuracy_5k: 0.5102 - val_lcm_hamming_loss_k: 0.0508
Epoch 6/100
12/12 [==============================] - ETA: 0s - loss: 1.3059 - lcm_precision_1k: 0.3052 - lcm_precision_2k: 0.1670 - lcm_precision_3k: 0.1513 - lcm_precision_5k: 0.1270 - lcm_recall_1k: 0.1510 - lcm_recall_2k: 0.1639 - lcm_recall_3k: 0.2191 - lcm_recall_5k: 0.3144 - lcm_f1_1k: 0.2019 - lcm_f1_2k: 0.1654 - lcm_f1_3k: 0.1789 - lcm_f1_5k: 0.1809 - lcm_accuracy_1k: 0.3052 - lcm_accuracy_2k: 0.3327 - lcm_accuracy_3k: 0.4375 - lcm_accuracy_5k: 0.5392 - lcm_hamming_loss_k: 0.0510
Epoch 00006: val_loss improved from 1.34024 to 1.27214, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 1.3059 - lcm_precision_1k: 0.3052 - lcm_precision_2k: 0.1670 - lcm_precision_3k: 0.1513 - lcm_precision_5k: 0.1270 - lcm_recall_1k: 0.1510 - lcm_recall_2k: 0.1639 - lcm_recall_3k: 0.2191 - lcm_recall_5k: 0.3144 - lcm_f1_1k: 0.2019 - lcm_f1_2k: 0.1654 - lcm_f1_3k: 0.1789 - lcm_f1_5k: 0.1809 - lcm_accuracy_1k: 0.3052 - lcm_accuracy_2k: 0.3327 - lcm_accuracy_3k: 0.4375 - lcm_accuracy_5k: 0.5392 - lcm_hamming_loss_k: 0.0510 - val_loss: 1.2721 - val_lcm_precision_1k: 0.2823 - val_lcm_precision_2k: 0.1486 - val_lcm_precision_3k: 0.1427 - val_lcm_precision_5k: 0.1303 - val_lcm_recall_1k: 0.1528 - val_lcm_recall_2k: 0.1604 - val_lcm_recall_3k: 0.2249 - val_lcm_recall_5k: 0.3495 - val_lcm_f1_1k: 0.1973 - val_lcm_f1_2k: 0.1536 - val_lcm_f1_3k: 0.1739 - val_lcm_f1_5k: 0.1895 - val_lcm_accuracy_1k: 0.2823 - val_lcm_accuracy_2k: 0.2973 - val_lcm_accuracy_3k: 0.4199 - val_lcm_accuracy_5k: 0.5512 - val_lcm_hamming_loss_k: 0.0505
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 1.2220 - lcm_precision_1k: 0.3029 - lcm_precision_2k: 0.1848 - lcm_precision_3k: 0.1689 - lcm_precision_5k: 0.1751 - lcm_recall_1k: 0.1504 - lcm_recall_2k: 0.1839 - lcm_recall_3k: 0.2526 - lcm_recall_5k: 0.4456 - lcm_f1_1k: 0.2010 - lcm_f1_2k: 0.1843 - lcm_f1_3k: 0.2024 - lcm_f1_5k: 0.2513 - lcm_accuracy_1k: 0.3029 - lcm_accuracy_2k: 0.3679 - lcm_accuracy_3k: 0.4787 - lcm_accuracy_5k: 0.6768 - lcm_hamming_loss_k: 0.0508
Epoch 00007: val_loss improved from 1.27214 to 1.09489, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 95ms/step - loss: 1.2157 - lcm_precision_1k: 0.2896 - lcm_precision_2k: 0.1886 - lcm_precision_3k: 0.1772 - lcm_precision_5k: 0.1811 - lcm_recall_1k: 0.1440 - lcm_recall_2k: 0.1873 - lcm_recall_3k: 0.2625 - lcm_recall_5k: 0.4556 - lcm_f1_1k: 0.1923 - lcm_f1_2k: 0.1879 - lcm_f1_3k: 0.2115 - lcm_f1_5k: 0.2590 - lcm_accuracy_1k: 0.2896 - lcm_accuracy_2k: 0.3757 - lcm_accuracy_3k: 0.4935 - lcm_accuracy_5k: 0.6897 - lcm_hamming_loss_k: 0.0517 - val_loss: 1.0949 - val_lcm_precision_1k: 0.0686 - val_lcm_precision_2k: 0.1925 - val_lcm_precision_3k: 0.2341 - val_lcm_precision_5k: 0.2346 - val_lcm_recall_1k: 0.0361 - val_lcm_recall_2k: 0.2064 - val_lcm_recall_3k: 0.3758 - val_lcm_recall_5k: 0.6189 - val_lcm_f1_1k: 0.0470 - val_lcm_f1_2k: 0.1984 - val_lcm_f1_3k: 0.2876 - val_lcm_f1_5k: 0.3398 - val_lcm_accuracy_1k: 0.0686 - val_lcm_accuracy_2k: 0.3821 - val_lcm_accuracy_3k: 0.5872 - val_lcm_accuracy_5k: 0.8299 - val_lcm_hamming_loss_k: 0.0592
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 1.0195 - lcm_precision_1k: 0.0604 - lcm_precision_2k: 0.2626 - lcm_precision_3k: 0.2770 - lcm_precision_5k: 0.2544 - lcm_recall_1k: 0.0322 - lcm_recall_2k: 0.2757 - lcm_recall_3k: 0.4290 - lcm_recall_5k: 0.6395 - lcm_f1_1k: 0.0418 - lcm_f1_2k: 0.2689 - lcm_f1_3k: 0.3366 - lcm_f1_5k: 0.3639 - lcm_accuracy_1k: 0.0604 - lcm_accuracy_2k: 0.5192 - lcm_accuracy_3k: 0.6804 - lcm_accuracy_5k: 0.8455 - lcm_hamming_loss_k: 0.0609
Epoch 00008: val_loss improved from 1.09489 to 0.94374, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 94ms/step - loss: 1.0162 - lcm_precision_1k: 0.0646 - lcm_precision_2k: 0.2659 - lcm_precision_3k: 0.2808 - lcm_precision_5k: 0.2569 - lcm_recall_1k: 0.0342 - lcm_recall_2k: 0.2806 - lcm_recall_3k: 0.4345 - lcm_recall_5k: 0.6442 - lcm_f1_1k: 0.0445 - lcm_f1_2k: 0.2729 - lcm_f1_3k: 0.3411 - lcm_f1_5k: 0.3673 - lcm_accuracy_1k: 0.0646 - lcm_accuracy_2k: 0.5257 - lcm_accuracy_3k: 0.6838 - lcm_accuracy_5k: 0.8481 - lcm_hamming_loss_k: 0.0607 - val_loss: 0.9437 - val_lcm_precision_1k: 0.0940 - val_lcm_precision_2k: 0.3054 - val_lcm_precision_3k: 0.3178 - val_lcm_precision_5k: 0.2784 - val_lcm_recall_1k: 0.0556 - val_lcm_recall_2k: 0.3501 - val_lcm_recall_3k: 0.5180 - val_lcm_recall_5k: 0.7193 - val_lcm_f1_1k: 0.0692 - val_lcm_f1_2k: 0.3254 - val_lcm_f1_3k: 0.3933 - val_lcm_f1_5k: 0.4010 - val_lcm_accuracy_1k: 0.0940 - val_lcm_accuracy_2k: 0.5998 - val_lcm_accuracy_3k: 0.7545 - val_lcm_accuracy_5k: 0.8929 - val_lcm_hamming_loss_k: 0.0582
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.8621 - lcm_precision_1k: 0.3665 - lcm_precision_2k: 0.4087 - lcm_precision_3k: 0.3815 - lcm_precision_5k: 0.3056 - lcm_recall_1k: 0.2055 - lcm_recall_2k: 0.4432 - lcm_recall_3k: 0.5950 - lcm_recall_5k: 0.7680 - lcm_f1_1k: 0.2631 - lcm_f1_2k: 0.4250 - lcm_f1_3k: 0.4648 - lcm_f1_5k: 0.4371 - lcm_accuracy_1k: 0.3665 - lcm_accuracy_2k: 0.7102 - lcm_accuracy_3k: 0.8260 - lcm_accuracy_5k: 0.9254 - lcm_hamming_loss_k: 0.0483
Epoch 00009: val_loss improved from 0.94374 to 0.78800, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.8629 - lcm_precision_1k: 0.3846 - lcm_precision_2k: 0.4180 - lcm_precision_3k: 0.3827 - lcm_precision_5k: 0.3058 - lcm_recall_1k: 0.2124 - lcm_recall_2k: 0.4480 - lcm_recall_3k: 0.5926 - lcm_recall_5k: 0.7653 - lcm_f1_1k: 0.2734 - lcm_f1_2k: 0.4321 - lcm_f1_3k: 0.4649 - lcm_f1_5k: 0.4369 - lcm_accuracy_1k: 0.3846 - lcm_accuracy_2k: 0.7138 - lcm_accuracy_3k: 0.8248 - lcm_accuracy_5k: 0.9241 - lcm_hamming_loss_k: 0.0477 - val_loss: 0.7880 - val_lcm_precision_1k: 0.6172 - val_lcm_precision_2k: 0.5065 - val_lcm_precision_3k: 0.4222 - val_lcm_precision_5k: 0.3085 - val_lcm_recall_1k: 0.3509 - val_lcm_recall_2k: 0.5554 - val_lcm_recall_3k: 0.6718 - val_lcm_recall_5k: 0.7877 - val_lcm_f1_1k: 0.4464 - val_lcm_f1_2k: 0.5289 - val_lcm_f1_3k: 0.5179 - val_lcm_f1_5k: 0.4429 - val_lcm_accuracy_1k: 0.6172 - val_lcm_accuracy_2k: 0.7913 - val_lcm_accuracy_3k: 0.8713 - val_lcm_accuracy_5k: 0.9192 - val_lcm_hamming_loss_k: 0.0368
Epoch 10/100
12/12 [==============================] - ETA: 0s - loss: 0.7061 - lcm_precision_1k: 0.6993 - lcm_precision_2k: 0.5674 - lcm_precision_3k: 0.4589 - lcm_precision_5k: 0.3337 - lcm_recall_1k: 0.3919 - lcm_recall_2k: 0.6012 - lcm_recall_3k: 0.7060 - lcm_recall_5k: 0.8269 - lcm_f1_1k: 0.5022 - lcm_f1_2k: 0.5837 - lcm_f1_3k: 0.5562 - lcm_f1_5k: 0.4755 - lcm_accuracy_1k: 0.6993 - lcm_accuracy_2k: 0.8366 - lcm_accuracy_3k: 0.8996 - lcm_accuracy_5k: 0.9479 - lcm_hamming_loss_k: 0.0348
Epoch 00010: val_loss improved from 0.78800 to 0.68640, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.7061 - lcm_precision_1k: 0.6993 - lcm_precision_2k: 0.5674 - lcm_precision_3k: 0.4589 - lcm_precision_5k: 0.3337 - lcm_recall_1k: 0.3919 - lcm_recall_2k: 0.6012 - lcm_recall_3k: 0.7060 - lcm_recall_5k: 0.8269 - lcm_f1_1k: 0.5022 - lcm_f1_2k: 0.5837 - lcm_f1_3k: 0.5562 - lcm_f1_5k: 0.4755 - lcm_accuracy_1k: 0.6993 - lcm_accuracy_2k: 0.8366 - lcm_accuracy_3k: 0.8996 - lcm_accuracy_5k: 0.9479 - lcm_hamming_loss_k: 0.0348 - val_loss: 0.6864 - val_lcm_precision_1k: 0.6780 - val_lcm_precision_2k: 0.5582 - val_lcm_precision_3k: 0.4486 - val_lcm_precision_5k: 0.3240 - val_lcm_recall_1k: 0.4131 - val_lcm_recall_2k: 0.6200 - val_lcm_recall_3k: 0.7223 - val_lcm_recall_5k: 0.8322 - val_lcm_f1_1k: 0.5128 - val_lcm_f1_2k: 0.5868 - val_lcm_f1_3k: 0.5528 - val_lcm_f1_5k: 0.4660 - val_lcm_accuracy_1k: 0.6780 - val_lcm_accuracy_2k: 0.8630 - val_lcm_accuracy_3k: 0.9119 - val_lcm_accuracy_5k: 0.9513 - val_lcm_hamming_loss_k: 0.0343
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.6162 - lcm_precision_1k: 0.7496 - lcm_precision_2k: 0.6101 - lcm_precision_3k: 0.4957 - lcm_precision_5k: 0.3494 - lcm_recall_1k: 0.4251 - lcm_recall_2k: 0.6434 - lcm_recall_3k: 0.7602 - lcm_recall_5k: 0.8684 - lcm_f1_1k: 0.5425 - lcm_f1_2k: 0.6262 - lcm_f1_3k: 0.6001 - lcm_f1_5k: 0.4983 - lcm_accuracy_1k: 0.7496 - lcm_accuracy_2k: 0.8786 - lcm_accuracy_3k: 0.9244 - lcm_accuracy_5k: 0.9684 - lcm_hamming_loss_k: 0.0327
Epoch 00011: val_loss improved from 0.68640 to 0.61881, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.6148 - lcm_precision_1k: 0.7570 - lcm_precision_2k: 0.6117 - lcm_precision_3k: 0.4965 - lcm_precision_5k: 0.3503 - lcm_recall_1k: 0.4290 - lcm_recall_2k: 0.6447 - lcm_recall_3k: 0.7609 - lcm_recall_5k: 0.8687 - lcm_f1_1k: 0.5475 - lcm_f1_2k: 0.6277 - lcm_f1_3k: 0.6008 - lcm_f1_5k: 0.4992 - lcm_accuracy_1k: 0.7570 - lcm_accuracy_2k: 0.8811 - lcm_accuracy_3k: 0.9258 - lcm_accuracy_5k: 0.9678 - lcm_hamming_loss_k: 0.0325 - val_loss: 0.6188 - val_lcm_precision_1k: 0.7746 - val_lcm_precision_2k: 0.5947 - val_lcm_precision_3k: 0.4717 - val_lcm_precision_5k: 0.3373 - val_lcm_recall_1k: 0.4658 - val_lcm_recall_2k: 0.6559 - val_lcm_recall_3k: 0.7532 - val_lcm_recall_5k: 0.8621 - val_lcm_f1_1k: 0.5811 - val_lcm_f1_2k: 0.6231 - val_lcm_f1_3k: 0.5794 - val_lcm_f1_5k: 0.4844 - val_lcm_accuracy_1k: 0.7746 - val_lcm_accuracy_2k: 0.8951 - val_lcm_accuracy_3k: 0.9344 - val_lcm_accuracy_5k: 0.9642 - val_lcm_hamming_loss_k: 0.0304
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.5581 - lcm_precision_1k: 0.8022 - lcm_precision_2k: 0.6406 - lcm_precision_3k: 0.5176 - lcm_precision_5k: 0.3592 - lcm_recall_1k: 0.4570 - lcm_recall_2k: 0.6741 - lcm_recall_3k: 0.7892 - lcm_recall_5k: 0.8843 - lcm_f1_1k: 0.5822 - lcm_f1_2k: 0.6569 - lcm_f1_3k: 0.6251 - lcm_f1_5k: 0.5108 - lcm_accuracy_1k: 0.8022 - lcm_accuracy_2k: 0.9063 - lcm_accuracy_3k: 0.9457 - lcm_accuracy_5k: 0.9730 - lcm_hamming_loss_k: 0.0307
Epoch 00012: val_loss improved from 0.61881 to 0.58022, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.5560 - lcm_precision_1k: 0.8041 - lcm_precision_2k: 0.6438 - lcm_precision_3k: 0.5194 - lcm_precision_5k: 0.3600 - lcm_recall_1k: 0.4596 - lcm_recall_2k: 0.6787 - lcm_recall_3k: 0.7933 - lcm_recall_5k: 0.8879 - lcm_f1_1k: 0.5848 - lcm_f1_2k: 0.6607 - lcm_f1_3k: 0.6277 - lcm_f1_5k: 0.5122 - lcm_accuracy_1k: 0.8041 - lcm_accuracy_2k: 0.9092 - lcm_accuracy_3k: 0.9475 - lcm_accuracy_5k: 0.9753 - lcm_hamming_loss_k: 0.0305 - val_loss: 0.5802 - val_lcm_precision_1k: 0.7954 - val_lcm_precision_2k: 0.6124 - val_lcm_precision_3k: 0.4874 - val_lcm_precision_5k: 0.3433 - val_lcm_recall_1k: 0.4723 - val_lcm_recall_2k: 0.6703 - val_lcm_recall_3k: 0.7757 - val_lcm_recall_5k: 0.8751 - val_lcm_f1_1k: 0.5918 - val_lcm_f1_2k: 0.6392 - val_lcm_f1_3k: 0.5978 - val_lcm_f1_5k: 0.4926 - val_lcm_accuracy_1k: 0.7954 - val_lcm_accuracy_2k: 0.9043 - val_lcm_accuracy_3k: 0.9385 - val_lcm_accuracy_5k: 0.9683 - val_lcm_hamming_loss_k: 0.0295
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.5176 - lcm_precision_1k: 0.8405 - lcm_precision_2k: 0.6655 - lcm_precision_3k: 0.5325 - lcm_precision_5k: 0.3651 - lcm_recall_1k: 0.4825 - lcm_recall_2k: 0.7019 - lcm_recall_3k: 0.8115 - lcm_recall_5k: 0.8991 - lcm_f1_1k: 0.6130 - lcm_f1_2k: 0.6831 - lcm_f1_3k: 0.6430 - lcm_f1_5k: 0.5193 - lcm_accuracy_1k: 0.8405 - lcm_accuracy_2k: 0.9249 - lcm_accuracy_3k: 0.9516 - lcm_accuracy_5k: 0.9788 - lcm_hamming_loss_k: 0.0290
Epoch 00013: val_loss improved from 0.58022 to 0.54910, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.5176 - lcm_precision_1k: 0.8405 - lcm_precision_2k: 0.6655 - lcm_precision_3k: 0.5325 - lcm_precision_5k: 0.3651 - lcm_recall_1k: 0.4825 - lcm_recall_2k: 0.7019 - lcm_recall_3k: 0.8115 - lcm_recall_5k: 0.8991 - lcm_f1_1k: 0.6130 - lcm_f1_2k: 0.6831 - lcm_f1_3k: 0.6430 - lcm_f1_5k: 0.5193 - lcm_accuracy_1k: 0.8405 - lcm_accuracy_2k: 0.9249 - lcm_accuracy_3k: 0.9516 - lcm_accuracy_5k: 0.9788 - lcm_hamming_loss_k: 0.0290 - val_loss: 0.5491 - val_lcm_precision_1k: 0.8277 - val_lcm_precision_2k: 0.6319 - val_lcm_precision_3k: 0.5006 - val_lcm_precision_5k: 0.3504 - val_lcm_recall_1k: 0.4896 - val_lcm_recall_2k: 0.6862 - val_lcm_recall_3k: 0.7912 - val_lcm_recall_5k: 0.8915 - val_lcm_f1_1k: 0.6142 - val_lcm_f1_2k: 0.6568 - val_lcm_f1_3k: 0.6124 - val_lcm_f1_5k: 0.5026 - val_lcm_accuracy_1k: 0.8277 - val_lcm_accuracy_2k: 0.9007 - val_lcm_accuracy_3k: 0.9427 - val_lcm_accuracy_5k: 0.9768 - val_lcm_hamming_loss_k: 0.0282
Epoch 14/100
12/12 [==============================] - ETA: 0s - loss: 0.4855 - lcm_precision_1k: 0.8612 - lcm_precision_2k: 0.6871 - lcm_precision_3k: 0.5467 - lcm_precision_5k: 0.3710 - lcm_recall_1k: 0.4933 - lcm_recall_2k: 0.7248 - lcm_recall_3k: 0.8361 - lcm_recall_5k: 0.9161 - lcm_f1_1k: 0.6271 - lcm_f1_2k: 0.7054 - lcm_f1_3k: 0.6610 - lcm_f1_5k: 0.5280 - lcm_accuracy_1k: 0.8612 - lcm_accuracy_2k: 0.9390 - lcm_accuracy_3k: 0.9709 - lcm_accuracy_5k: 0.9874 - lcm_hamming_loss_k: 0.0281
Epoch 00014: val_loss improved from 0.54910 to 0.54627, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.4855 - lcm_precision_1k: 0.8612 - lcm_precision_2k: 0.6871 - lcm_precision_3k: 0.5467 - lcm_precision_5k: 0.3710 - lcm_recall_1k: 0.4933 - lcm_recall_2k: 0.7248 - lcm_recall_3k: 0.8361 - lcm_recall_5k: 0.9161 - lcm_f1_1k: 0.6271 - lcm_f1_2k: 0.7054 - lcm_f1_3k: 0.6610 - lcm_f1_5k: 0.5280 - lcm_accuracy_1k: 0.8612 - lcm_accuracy_2k: 0.9390 - lcm_accuracy_3k: 0.9709 - lcm_accuracy_5k: 0.9874 - lcm_hamming_loss_k: 0.0281 - val_loss: 0.5463 - val_lcm_precision_1k: 0.8148 - val_lcm_precision_2k: 0.6294 - val_lcm_precision_3k: 0.5068 - val_lcm_precision_5k: 0.3487 - val_lcm_recall_1k: 0.4795 - val_lcm_recall_2k: 0.6953 - val_lcm_recall_3k: 0.8052 - val_lcm_recall_5k: 0.8916 - val_lcm_f1_1k: 0.6026 - val_lcm_f1_2k: 0.6599 - val_lcm_f1_3k: 0.6213 - val_lcm_f1_5k: 0.5009 - val_lcm_accuracy_1k: 0.8148 - val_lcm_accuracy_2k: 0.9185 - val_lcm_accuracy_3k: 0.9595 - val_lcm_accuracy_5k: 0.9796 - val_lcm_hamming_loss_k: 0.0288
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4676 - lcm_precision_1k: 0.8558 - lcm_precision_2k: 0.6857 - lcm_precision_3k: 0.5541 - lcm_precision_5k: 0.3767 - lcm_recall_1k: 0.4926 - lcm_recall_2k: 0.7239 - lcm_recall_3k: 0.8468 - lcm_recall_5k: 0.9267 - lcm_f1_1k: 0.6252 - lcm_f1_2k: 0.7042 - lcm_f1_3k: 0.6698 - lcm_f1_5k: 0.5356 - lcm_accuracy_1k: 0.8558 - lcm_accuracy_2k: 0.9375 - lcm_accuracy_3k: 0.9755 - lcm_accuracy_5k: 0.9908 - lcm_hamming_loss_k: 0.0283
Epoch 00015: val_loss improved from 0.54627 to 0.53047, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.4683 - lcm_precision_1k: 0.8554 - lcm_precision_2k: 0.6887 - lcm_precision_3k: 0.5546 - lcm_precision_5k: 0.3776 - lcm_recall_1k: 0.4913 - lcm_recall_2k: 0.7241 - lcm_recall_3k: 0.8439 - lcm_recall_5k: 0.9240 - lcm_f1_1k: 0.6240 - lcm_f1_2k: 0.7058 - lcm_f1_3k: 0.6692 - lcm_f1_5k: 0.5360 - lcm_accuracy_1k: 0.8554 - lcm_accuracy_2k: 0.9389 - lcm_accuracy_3k: 0.9754 - lcm_accuracy_5k: 0.9905 - lcm_hamming_loss_k: 0.0285 - val_loss: 0.5305 - val_lcm_precision_1k: 0.8283 - val_lcm_precision_2k: 0.6407 - val_lcm_precision_3k: 0.5087 - val_lcm_precision_5k: 0.3496 - val_lcm_recall_1k: 0.4976 - val_lcm_recall_2k: 0.7010 - val_lcm_recall_3k: 0.8054 - val_lcm_recall_5k: 0.8915 - val_lcm_f1_1k: 0.6211 - val_lcm_f1_2k: 0.6687 - val_lcm_f1_3k: 0.6227 - val_lcm_f1_5k: 0.5017 - val_lcm_accuracy_1k: 0.8283 - val_lcm_accuracy_2k: 0.9264 - val_lcm_accuracy_3k: 0.9560 - val_lcm_accuracy_5k: 0.9764 - val_lcm_hamming_loss_k: 0.0282
Epoch 16/100
12/12 [==============================] - ETA: 0s - loss: 0.4520 - lcm_precision_1k: 0.8679 - lcm_precision_2k: 0.6991 - lcm_precision_3k: 0.5599 - lcm_precision_5k: 0.3812 - lcm_recall_1k: 0.4993 - lcm_recall_2k: 0.7349 - lcm_recall_3k: 0.8485 - lcm_recall_5k: 0.9306 - lcm_f1_1k: 0.6337 - lcm_f1_2k: 0.7163 - lcm_f1_3k: 0.6744 - lcm_f1_5k: 0.5407 - lcm_accuracy_1k: 0.8679 - lcm_accuracy_2k: 0.9481 - lcm_accuracy_3k: 0.9741 - lcm_accuracy_5k: 0.9892 - lcm_hamming_loss_k: 0.0280
Epoch 00016: val_loss improved from 0.53047 to 0.52077, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.4520 - lcm_precision_1k: 0.8679 - lcm_precision_2k: 0.6991 - lcm_precision_3k: 0.5599 - lcm_precision_5k: 0.3812 - lcm_recall_1k: 0.4993 - lcm_recall_2k: 0.7349 - lcm_recall_3k: 0.8485 - lcm_recall_5k: 0.9306 - lcm_f1_1k: 0.6337 - lcm_f1_2k: 0.7163 - lcm_f1_3k: 0.6744 - lcm_f1_5k: 0.5407 - lcm_accuracy_1k: 0.8679 - lcm_accuracy_2k: 0.9481 - lcm_accuracy_3k: 0.9741 - lcm_accuracy_5k: 0.9892 - lcm_hamming_loss_k: 0.0280 - val_loss: 0.5208 - val_lcm_precision_1k: 0.8280 - val_lcm_precision_2k: 0.6466 - val_lcm_precision_3k: 0.5207 - val_lcm_precision_5k: 0.3533 - val_lcm_recall_1k: 0.4854 - val_lcm_recall_2k: 0.7005 - val_lcm_recall_3k: 0.8181 - val_lcm_recall_5k: 0.9021 - val_lcm_f1_1k: 0.6109 - val_lcm_f1_2k: 0.6711 - val_lcm_f1_3k: 0.6356 - val_lcm_f1_5k: 0.5073 - val_lcm_accuracy_1k: 0.8280 - val_lcm_accuracy_2k: 0.9073 - val_lcm_accuracy_3k: 0.9560 - val_lcm_accuracy_5k: 0.9838 - val_lcm_hamming_loss_k: 0.0282
Epoch 17/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4208 - lcm_precision_1k: 0.8888 - lcm_precision_2k: 0.7141 - lcm_precision_3k: 0.5709 - lcm_precision_5k: 0.3862 - lcm_recall_1k: 0.5127 - lcm_recall_2k: 0.7509 - lcm_recall_3k: 0.8653 - lcm_recall_5k: 0.9423 - lcm_f1_1k: 0.6502 - lcm_f1_2k: 0.7320 - lcm_f1_3k: 0.6878 - lcm_f1_5k: 0.5478 - lcm_accuracy_1k: 0.8888 - lcm_accuracy_2k: 0.9574 - lcm_accuracy_3k: 0.9840 - lcm_accuracy_5k: 0.9950 - lcm_hamming_loss_k: 0.0272
Epoch 00017: val_loss did not improve from 0.52077
12/12 [==============================] - 1s 69ms/step - loss: 0.4220 - lcm_precision_1k: 0.8851 - lcm_precision_2k: 0.7136 - lcm_precision_3k: 0.5697 - lcm_precision_5k: 0.3851 - lcm_recall_1k: 0.5100 - lcm_recall_2k: 0.7510 - lcm_recall_3k: 0.8652 - lcm_recall_5k: 0.9422 - lcm_f1_1k: 0.6470 - lcm_f1_2k: 0.7317 - lcm_f1_3k: 0.6869 - lcm_f1_5k: 0.5467 - lcm_accuracy_1k: 0.8851 - lcm_accuracy_2k: 0.9566 - lcm_accuracy_3k: 0.9827 - lcm_accuracy_5k: 0.9944 - lcm_hamming_loss_k: 0.0272 - val_loss: 0.5319 - val_lcm_precision_1k: 0.8235 - val_lcm_precision_2k: 0.6447 - val_lcm_precision_3k: 0.5136 - val_lcm_precision_5k: 0.3497 - val_lcm_recall_1k: 0.4838 - val_lcm_recall_2k: 0.7011 - val_lcm_recall_3k: 0.8105 - val_lcm_recall_5k: 0.8871 - val_lcm_f1_1k: 0.6084 - val_lcm_f1_2k: 0.6704 - val_lcm_f1_3k: 0.6278 - val_lcm_f1_5k: 0.5011 - val_lcm_accuracy_1k: 0.8235 - val_lcm_accuracy_2k: 0.9167 - val_lcm_accuracy_3k: 0.9560 - val_lcm_accuracy_5k: 0.9780 - val_lcm_hamming_loss_k: 0.0284
Epoch 18/100
11/12 [==========================>...] - ETA: 0s - loss: 0.4038 - lcm_precision_1k: 0.9006 - lcm_precision_2k: 0.7212 - lcm_precision_3k: 0.5760 - lcm_precision_5k: 0.3883 - lcm_recall_1k: 0.5241 - lcm_recall_2k: 0.7594 - lcm_recall_3k: 0.8711 - lcm_recall_5k: 0.9461 - lcm_f1_1k: 0.6624 - lcm_f1_2k: 0.7397 - lcm_f1_3k: 0.6933 - lcm_f1_5k: 0.5505 - lcm_accuracy_1k: 0.9006 - lcm_accuracy_2k: 0.9627 - lcm_accuracy_3k: 0.9801 - lcm_accuracy_5k: 0.9943 - lcm_hamming_loss_k: 0.0267
Epoch 00018: val_loss improved from 0.52077 to 0.50787, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.4017 - lcm_precision_1k: 0.9040 - lcm_precision_2k: 0.7239 - lcm_precision_3k: 0.5765 - lcm_precision_5k: 0.3879 - lcm_recall_1k: 0.5269 - lcm_recall_2k: 0.7632 - lcm_recall_3k: 0.8731 - lcm_recall_5k: 0.9469 - lcm_f1_1k: 0.6656 - lcm_f1_2k: 0.7429 - lcm_f1_3k: 0.6943 - lcm_f1_5k: 0.5502 - lcm_accuracy_1k: 0.9040 - lcm_accuracy_2k: 0.9647 - lcm_accuracy_3k: 0.9818 - lcm_accuracy_5k: 0.9948 - lcm_hamming_loss_k: 0.0264 - val_loss: 0.5079 - val_lcm_precision_1k: 0.8491 - val_lcm_precision_2k: 0.6540 - val_lcm_precision_3k: 0.5217 - val_lcm_precision_5k: 0.3557 - val_lcm_recall_1k: 0.5100 - val_lcm_recall_2k: 0.7229 - val_lcm_recall_3k: 0.8206 - val_lcm_recall_5k: 0.9011 - val_lcm_f1_1k: 0.6366 - val_lcm_f1_2k: 0.6857 - val_lcm_f1_3k: 0.6370 - val_lcm_f1_5k: 0.5096 - val_lcm_accuracy_1k: 0.8491 - val_lcm_accuracy_2k: 0.9383 - val_lcm_accuracy_3k: 0.9655 - val_lcm_accuracy_5k: 0.9767 - val_lcm_hamming_loss_k: 0.0274
Epoch 19/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3805 - lcm_precision_1k: 0.9130 - lcm_precision_2k: 0.7351 - lcm_precision_3k: 0.5852 - lcm_precision_5k: 0.3920 - lcm_recall_1k: 0.5309 - lcm_recall_2k: 0.7746 - lcm_recall_3k: 0.8842 - lcm_recall_5k: 0.9541 - lcm_f1_1k: 0.6712 - lcm_f1_2k: 0.7542 - lcm_f1_3k: 0.7042 - lcm_f1_5k: 0.5557 - lcm_accuracy_1k: 0.9130 - lcm_accuracy_2k: 0.9709 - lcm_accuracy_3k: 0.9869 - lcm_accuracy_5k: 0.9968 - lcm_hamming_loss_k: 0.0262
Epoch 00019: val_loss improved from 0.50787 to 0.50217, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.3814 - lcm_precision_1k: 0.9132 - lcm_precision_2k: 0.7347 - lcm_precision_3k: 0.5839 - lcm_precision_5k: 0.3911 - lcm_recall_1k: 0.5312 - lcm_recall_2k: 0.7745 - lcm_recall_3k: 0.8836 - lcm_recall_5k: 0.9544 - lcm_f1_1k: 0.6715 - lcm_f1_2k: 0.7540 - lcm_f1_3k: 0.7031 - lcm_f1_5k: 0.5548 - lcm_accuracy_1k: 0.9132 - lcm_accuracy_2k: 0.9690 - lcm_accuracy_3k: 0.9858 - lcm_accuracy_5k: 0.9971 - lcm_hamming_loss_k: 0.0260 - val_loss: 0.5022 - val_lcm_precision_1k: 0.8509 - val_lcm_precision_2k: 0.6582 - val_lcm_precision_3k: 0.5248 - val_lcm_precision_5k: 0.3578 - val_lcm_recall_1k: 0.5055 - val_lcm_recall_2k: 0.7209 - val_lcm_recall_3k: 0.8279 - val_lcm_recall_5k: 0.9092 - val_lcm_f1_1k: 0.6330 - val_lcm_f1_2k: 0.6867 - val_lcm_f1_3k: 0.6415 - val_lcm_f1_5k: 0.5130 - val_lcm_accuracy_1k: 0.8509 - val_lcm_accuracy_2k: 0.9303 - val_lcm_accuracy_3k: 0.9677 - val_lcm_accuracy_5k: 0.9834 - val_lcm_hamming_loss_k: 0.0273
Epoch 20/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3643 - lcm_precision_1k: 0.9197 - lcm_precision_2k: 0.7433 - lcm_precision_3k: 0.5885 - lcm_precision_5k: 0.3938 - lcm_recall_1k: 0.5342 - lcm_recall_2k: 0.7821 - lcm_recall_3k: 0.8898 - lcm_recall_5k: 0.9589 - lcm_f1_1k: 0.6758 - lcm_f1_2k: 0.7622 - lcm_f1_3k: 0.7084 - lcm_f1_5k: 0.5583 - lcm_accuracy_1k: 0.9197 - lcm_accuracy_2k: 0.9744 - lcm_accuracy_3k: 0.9897 - lcm_accuracy_5k: 0.9968 - lcm_hamming_loss_k: 0.0258
Epoch 00020: val_loss did not improve from 0.50217
12/12 [==============================] - 1s 70ms/step - loss: 0.3623 - lcm_precision_1k: 0.9216 - lcm_precision_2k: 0.7441 - lcm_precision_3k: 0.5900 - lcm_precision_5k: 0.3941 - lcm_recall_1k: 0.5358 - lcm_recall_2k: 0.7834 - lcm_recall_3k: 0.8921 - lcm_recall_5k: 0.9599 - lcm_f1_1k: 0.6776 - lcm_f1_2k: 0.7632 - lcm_f1_3k: 0.7102 - lcm_f1_5k: 0.5588 - lcm_accuracy_1k: 0.9216 - lcm_accuracy_2k: 0.9739 - lcm_accuracy_3k: 0.9906 - lcm_accuracy_5k: 0.9971 - lcm_hamming_loss_k: 0.0257 - val_loss: 0.5074 - val_lcm_precision_1k: 0.8447 - val_lcm_precision_2k: 0.6499 - val_lcm_precision_3k: 0.5168 - val_lcm_precision_5k: 0.3528 - val_lcm_recall_1k: 0.5055 - val_lcm_recall_2k: 0.7084 - val_lcm_recall_3k: 0.8170 - val_lcm_recall_5k: 0.8981 - val_lcm_f1_1k: 0.6313 - val_lcm_f1_2k: 0.6767 - val_lcm_f1_3k: 0.6324 - val_lcm_f1_5k: 0.5061 - val_lcm_accuracy_1k: 0.8447 - val_lcm_accuracy_2k: 0.9172 - val_lcm_accuracy_3k: 0.9579 - val_lcm_accuracy_5k: 0.9729 - val_lcm_hamming_loss_k: 0.0275
Epoch 21/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3589 - lcm_precision_1k: 0.9162 - lcm_precision_2k: 0.7417 - lcm_precision_3k: 0.5892 - lcm_precision_5k: 0.3948 - lcm_recall_1k: 0.5338 - lcm_recall_2k: 0.7825 - lcm_recall_3k: 0.8913 - lcm_recall_5k: 0.9590 - lcm_f1_1k: 0.6744 - lcm_f1_2k: 0.7614 - lcm_f1_3k: 0.7093 - lcm_f1_5k: 0.5593 - lcm_accuracy_1k: 0.9162 - lcm_accuracy_2k: 0.9720 - lcm_accuracy_3k: 0.9876 - lcm_accuracy_5k: 0.9957 - lcm_hamming_loss_k: 0.0260
Epoch 00021: val_loss improved from 0.50217 to 0.48951, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 102ms/step - loss: 0.3577 - lcm_precision_1k: 0.9178 - lcm_precision_2k: 0.7421 - lcm_precision_3k: 0.5910 - lcm_precision_5k: 0.3953 - lcm_recall_1k: 0.5344 - lcm_recall_2k: 0.7825 - lcm_recall_3k: 0.8933 - lcm_recall_5k: 0.9600 - lcm_f1_1k: 0.6753 - lcm_f1_2k: 0.7616 - lcm_f1_3k: 0.7113 - lcm_f1_5k: 0.5599 - lcm_accuracy_1k: 0.9178 - lcm_accuracy_2k: 0.9732 - lcm_accuracy_3k: 0.9886 - lcm_accuracy_5k: 0.9961 - lcm_hamming_loss_k: 0.0259 - val_loss: 0.4895 - val_lcm_precision_1k: 0.8509 - val_lcm_precision_2k: 0.6586 - val_lcm_precision_3k: 0.5310 - val_lcm_precision_5k: 0.3593 - val_lcm_recall_1k: 0.5069 - val_lcm_recall_2k: 0.7177 - val_lcm_recall_3k: 0.8332 - val_lcm_recall_5k: 0.9108 - val_lcm_f1_1k: 0.6342 - val_lcm_f1_2k: 0.6856 - val_lcm_f1_3k: 0.6476 - val_lcm_f1_5k: 0.5148 - val_lcm_accuracy_1k: 0.8509 - val_lcm_accuracy_2k: 0.9325 - val_lcm_accuracy_3k: 0.9682 - val_lcm_accuracy_5k: 0.9822 - val_lcm_hamming_loss_k: 0.0273
Epoch 22/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3360 - lcm_precision_1k: 0.9354 - lcm_precision_2k: 0.7529 - lcm_precision_3k: 0.5991 - lcm_precision_5k: 0.3967 - lcm_recall_1k: 0.5460 - lcm_recall_2k: 0.7949 - lcm_recall_3k: 0.9042 - lcm_recall_5k: 0.9653 - lcm_f1_1k: 0.6894 - lcm_f1_2k: 0.7732 - lcm_f1_3k: 0.7206 - lcm_f1_5k: 0.5623 - lcm_accuracy_1k: 0.9354 - lcm_accuracy_2k: 0.9798 - lcm_accuracy_3k: 0.9922 - lcm_accuracy_5k: 0.9982 - lcm_hamming_loss_k: 0.0251
Epoch 00022: val_loss did not improve from 0.48951
12/12 [==============================] - 1s 72ms/step - loss: 0.3384 - lcm_precision_1k: 0.9305 - lcm_precision_2k: 0.7510 - lcm_precision_3k: 0.5977 - lcm_precision_5k: 0.3971 - lcm_recall_1k: 0.5443 - lcm_recall_2k: 0.7934 - lcm_recall_3k: 0.9018 - lcm_recall_5k: 0.9649 - lcm_f1_1k: 0.6868 - lcm_f1_2k: 0.7715 - lcm_f1_3k: 0.7188 - lcm_f1_5k: 0.5626 - lcm_accuracy_1k: 0.9305 - lcm_accuracy_2k: 0.9787 - lcm_accuracy_3k: 0.9912 - lcm_accuracy_5k: 0.9984 - lcm_hamming_loss_k: 0.0254 - val_loss: 0.5025 - val_lcm_precision_1k: 0.8342 - val_lcm_precision_2k: 0.6637 - val_lcm_precision_3k: 0.5241 - val_lcm_precision_5k: 0.3537 - val_lcm_recall_1k: 0.4956 - val_lcm_recall_2k: 0.7252 - val_lcm_recall_3k: 0.8276 - val_lcm_recall_5k: 0.8991 - val_lcm_f1_1k: 0.6207 - val_lcm_f1_2k: 0.6918 - val_lcm_f1_3k: 0.6407 - val_lcm_f1_5k: 0.5072 - val_lcm_accuracy_1k: 0.8342 - val_lcm_accuracy_2k: 0.9379 - val_lcm_accuracy_3k: 0.9669 - val_lcm_accuracy_5k: 0.9793 - val_lcm_hamming_loss_k: 0.0280
Epoch 23/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3403 - lcm_precision_1k: 0.9244 - lcm_precision_2k: 0.7557 - lcm_precision_3k: 0.5956 - lcm_precision_5k: 0.3969 - lcm_recall_1k: 0.5369 - lcm_recall_2k: 0.7951 - lcm_recall_3k: 0.9003 - lcm_recall_5k: 0.9649 - lcm_f1_1k: 0.6792 - lcm_f1_2k: 0.7748 - lcm_f1_3k: 0.7169 - lcm_f1_5k: 0.5624 - lcm_accuracy_1k: 0.9244 - lcm_accuracy_2k: 0.9769 - lcm_accuracy_3k: 0.9915 - lcm_accuracy_5k: 0.9979 - lcm_hamming_loss_k: 0.0256
Epoch 00023: val_loss improved from 0.48951 to 0.47471, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.3390 - lcm_precision_1k: 0.9253 - lcm_precision_2k: 0.7587 - lcm_precision_3k: 0.5992 - lcm_precision_5k: 0.3981 - lcm_recall_1k: 0.5350 - lcm_recall_2k: 0.7950 - lcm_recall_3k: 0.9019 - lcm_recall_5k: 0.9652 - lcm_f1_1k: 0.6779 - lcm_f1_2k: 0.7763 - lcm_f1_3k: 0.7199 - lcm_f1_5k: 0.5636 - lcm_accuracy_1k: 0.9253 - lcm_accuracy_2k: 0.9767 - lcm_accuracy_3k: 0.9917 - lcm_accuracy_5k: 0.9980 - lcm_hamming_loss_k: 0.0256 - val_loss: 0.4747 - val_lcm_precision_1k: 0.8655 - val_lcm_precision_2k: 0.6763 - val_lcm_precision_3k: 0.5325 - val_lcm_precision_5k: 0.3572 - val_lcm_recall_1k: 0.5240 - val_lcm_recall_2k: 0.7419 - val_lcm_recall_3k: 0.8438 - val_lcm_recall_5k: 0.9081 - val_lcm_f1_1k: 0.6518 - val_lcm_f1_2k: 0.7065 - val_lcm_f1_3k: 0.6521 - val_lcm_f1_5k: 0.5122 - val_lcm_accuracy_1k: 0.8655 - val_lcm_accuracy_2k: 0.9467 - val_lcm_accuracy_3k: 0.9768 - val_lcm_accuracy_5k: 0.9848 - val_lcm_hamming_loss_k: 0.0267
Epoch 24/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3068 - lcm_precision_1k: 0.9471 - lcm_precision_2k: 0.7679 - lcm_precision_3k: 0.6055 - lcm_precision_5k: 0.4011 - lcm_recall_1k: 0.5546 - lcm_recall_2k: 0.8114 - lcm_recall_3k: 0.9147 - lcm_recall_5k: 0.9754 - lcm_f1_1k: 0.6993 - lcm_f1_2k: 0.7888 - lcm_f1_3k: 0.7284 - lcm_f1_5k: 0.5683 - lcm_accuracy_1k: 0.9471 - lcm_accuracy_2k: 0.9837 - lcm_accuracy_3k: 0.9957 - lcm_accuracy_5k: 0.9989 - lcm_hamming_loss_k: 0.0245
Epoch 00024: val_loss improved from 0.47471 to 0.46764, saving model to logs/hbajex-labs-0602-222004/model/checkpoint_labs.h5
12/12 [==============================] - 1s 103ms/step - loss: 0.3071 - lcm_precision_1k: 0.9461 - lcm_precision_2k: 0.7716 - lcm_precision_3k: 0.6093 - lcm_precision_5k: 0.4036 - lcm_recall_1k: 0.5497 - lcm_recall_2k: 0.8090 - lcm_recall_3k: 0.9138 - lcm_recall_5k: 0.9743 - lcm_f1_1k: 0.6950 - lcm_f1_2k: 0.7895 - lcm_f1_3k: 0.7308 - lcm_f1_5k: 0.5706 - lcm_accuracy_1k: 0.9461 - lcm_accuracy_2k: 0.9834 - lcm_accuracy_3k: 0.9961 - lcm_accuracy_5k: 0.9990 - lcm_hamming_loss_k: 0.0249 - val_loss: 0.4676 - val_lcm_precision_1k: 0.8733 - val_lcm_precision_2k: 0.6776 - val_lcm_precision_3k: 0.5366 - val_lcm_precision_5k: 0.3594 - val_lcm_recall_1k: 0.5287 - val_lcm_recall_2k: 0.7440 - val_lcm_recall_3k: 0.8442 - val_lcm_recall_5k: 0.9100 - val_lcm_f1_1k: 0.6578 - val_lcm_f1_2k: 0.7083 - val_lcm_f1_3k: 0.6552 - val_lcm_f1_5k: 0.5147 - val_lcm_accuracy_1k: 0.8733 - val_lcm_accuracy_2k: 0.9511 - val_lcm_accuracy_3k: 0.9726 - val_lcm_accuracy_5k: 0.9808 - val_lcm_hamming_loss_k: 0.0264
Epoch 25/100
11/12 [==========================>...] - ETA: 0s - loss: 0.2871 - lcm_precision_1k: 0.9503 - lcm_precision_2k: 0.7802 - lcm_precision_3k: 0.6131 - lcm_precision_5k: 0.4047 - lcm_recall_1k: 0.5564 - lcm_recall_2k: 0.8189 - lcm_recall_3k: 0.9195 - lcm_recall_5k: 0.9779 - lcm_f1_1k: 0.7018 - lcm_f1_2k: 0.7990 - lcm_f1_3k: 0.7356 - lcm_f1_5k: 0.5724 - lcm_accuracy_1k: 0.9503 - lcm_accuracy_2k: 0.9862 - lcm_accuracy_3k: 0.9950 - lcm_accuracy_5k: 0.9993 - lcm_hamming_loss_k: 0.0246
Epoch 00025: val_loss did not improve from 0.46764
12/12 [==============================] - 1s 67ms/step - loss: 0.2867 - lcm_precision_1k: 0.9517 - lcm_precision_2k: 0.7820 - lcm_precision_3k: 0.6132 - lcm_precision_5k: 0.4045 - lcm_recall_1k: 0.5572 - lcm_recall_2k: 0.8202 - lcm_recall_3k: 0.9192 - lcm_recall_5k: 0.9774 - lcm_f1_1k: 0.7028 - lcm_f1_2k: 0.8006 - lcm_f1_3k: 0.7356 - lcm_f1_5k: 0.5722 - lcm_accuracy_1k: 0.9517 - lcm_accuracy_2k: 0.9868 - lcm_accuracy_3k: 0.9955 - lcm_accuracy_5k: 0.9994 - lcm_hamming_loss_k: 0.0245 - val_loss: 0.4729 - val_lcm_precision_1k: 0.8743 - val_lcm_precision_2k: 0.6659 - val_lcm_precision_3k: 0.5338 - val_lcm_precision_5k: 0.3535 - val_lcm_recall_1k: 0.5231 - val_lcm_recall_2k: 0.7278 - val_lcm_recall_3k: 0.8396 - val_lcm_recall_5k: 0.8975 - val_lcm_f1_1k: 0.6535 - val_lcm_f1_2k: 0.6942 - val_lcm_f1_3k: 0.6517 - val_lcm_f1_5k: 0.5066 - val_lcm_accuracy_1k: 0.8743 - val_lcm_accuracy_2k: 0.9379 - val_lcm_accuracy_3k: 0.9684 - val_lcm_accuracy_5k: 0.9738 - val_lcm_hamming_loss_k: 0.0263
Epoch 26/100
11/12 [==========================>...] - ETA: 0s - loss: 0.2797 - lcm_precision_1k: 0.9521 - lcm_precision_2k: 0.7848 - lcm_precision_3k: 0.6185 - lcm_precision_5k: 0.4054 - lcm_recall_1k: 0.5560 - lcm_recall_2k: 0.8224 - lcm_recall_3k: 0.9249 - lcm_recall_5k: 0.9779 - lcm_f1_1k: 0.7020 - lcm_f1_2k: 0.8031 - lcm_f1_3k: 0.7412 - lcm_f1_5k: 0.5731 - lcm_accuracy_1k: 0.9521 - lcm_accuracy_2k: 0.9883 - lcm_accuracy_3k: 0.9968 - lcm_accuracy_5k: 0.9993 - lcm_hamming_loss_k: 0.0246
Epoch 00026: val_loss did not improve from 0.46764
12/12 [==============================] - 1s 69ms/step - loss: 0.2791 - lcm_precision_1k: 0.9533 - lcm_precision_2k: 0.7860 - lcm_precision_3k: 0.6180 - lcm_precision_5k: 0.4051 - lcm_recall_1k: 0.5577 - lcm_recall_2k: 0.8246 - lcm_recall_3k: 0.9261 - lcm_recall_5k: 0.9790 - lcm_f1_1k: 0.7037 - lcm_f1_2k: 0.8047 - lcm_f1_3k: 0.7413 - lcm_f1_5k: 0.5730 - lcm_accuracy_1k: 0.9533 - lcm_accuracy_2k: 0.9882 - lcm_accuracy_3k: 0.9971 - lcm_accuracy_5k: 0.9994 - lcm_hamming_loss_k: 0.0244 - val_loss: 0.5020 - val_lcm_precision_1k: 0.8574 - val_lcm_precision_2k: 0.6621 - val_lcm_precision_3k: 0.5278 - val_lcm_precision_5k: 0.3512 - val_lcm_recall_1k: 0.5129 - val_lcm_recall_2k: 0.7194 - val_lcm_recall_3k: 0.8283 - val_lcm_recall_5k: 0.8909 - val_lcm_f1_1k: 0.6406 - val_lcm_f1_2k: 0.6881 - val_lcm_f1_3k: 0.6437 - val_lcm_f1_5k: 0.5032 - val_lcm_accuracy_1k: 0.8574 - val_lcm_accuracy_2k: 0.9251 - val_lcm_accuracy_3k: 0.9613 - val_lcm_accuracy_5k: 0.9719 - val_lcm_hamming_loss_k: 0.0270
Epoch 00026: early stopping
39/39 [==============================] - 1s 23ms/step - loss: 0.3495 - lcm_precision_1k: 0.9336 - lcm_precision_2k: 0.7647 - lcm_precision_3k: 0.5992 - lcm_precision_5k: 0.3972 - lcm_recall_1k: 0.5367 - lcm_recall_2k: 0.7934 - lcm_recall_3k: 0.8945 - lcm_recall_5k: 0.9540 - lcm_f1_1k: 0.6806 - lcm_f1_2k: 0.7778 - lcm_f1_3k: 0.7167 - lcm_f1_5k: 0.5601 - lcm_accuracy_1k: 0.9336 - lcm_accuracy_2k: 0.9772 - lcm_accuracy_3k: 0.9916 - lcm_accuracy_5k: 0.9948 - lcm_hamming_loss_k: 0.0258 0s - loss: 0.3600 - lcm_precision_1k: 0.9347 - lcm_precision_2k: 0.7770 - lcm_precision_3k: 0.5975 - lcm_precision_5k: 0.4023 - lcm_recall_1k: 0.5306 - lcm_recall_2k: 0.8007 - lcm_recall_3k: 0.8839 - lcm_recall_5k: 0.9548 - lcm_f1_1k: 0.6762 - lcm_f1_2k: 0.7879 - lcm_f1_3k: 0.7123 - lcm_f1_5k: 0.5655 - lcm_accuracy_1k: 0.9347 - lcm_accuracy_2k: 0.9801 - lcm_accuracy_3k: 0.9915 - lcm_accuracy_5k: 0.9972 - lcm_hamm
Best model result:  [0.3495256006717682, 0.9336410164833069, 0.7646846175193787, 0.5991922616958618, 0.3971666693687439, 0.5366512537002563, 0.7934256196022034, 0.8945308327674866, 0.9540460109710693, 0.6805673241615295, 0.7778409123420715, 0.716676652431488, 0.560126781463623, 0.9336410164833069, 0.9772178530693054, 0.9916306734085083, 0.99483323097229, 0.025767384096980095]
fold_result:  [[0.4135601818561554, 0.9235796928405762, 0.7292384505271912, 0.5730154514312744, 0.3843333125114441, 0.5264948010444641, 0.7603538632392883, 0.8633204698562622, 0.9324718117713928, 0.6697353720664978, 0.7433788776397705, 0.6878405809402466, 0.5436285734176636, 0.9235796928405762, 0.9675999283790588, 0.9879896640777588, 0.9951999187469482, 0.02617768757045269], [0.42253032326698303, 0.9127383828163147, 0.7270281314849854, 0.571294903755188, 0.3811538517475128, 0.5222973823547363, 0.7596282362937927, 0.8587973117828369, 0.9262819290161133, 0.6633501052856445, 0.7419451475143433, 0.6851454377174377, 0.5393345952033997, 0.9127383828163147, 0.9659921526908875, 0.9796152114868164, 0.9892306923866272, 0.02662069723010063], [0.48620596528053284, 0.8795129060745239, 0.6974102258682251, 0.5516948699951172, 0.3703359067440033, 0.5007845163345337, 0.729858934879303, 0.8341717720031738, 0.9042972922325134, 0.6371349692344666, 0.7121678590774536, 0.6630915999412537, 0.5247405171394348, 0.8795129060745239, 0.948797345161438, 0.9736409187316895, 0.9863871335983276, 0.027976488694548607], [0.4066724479198456, 0.9135257005691528, 0.7329948544502258, 0.5783615112304688, 0.38466668128967285, 0.5246948003768921, 0.7630845904350281, 0.8692308068275452, 0.9337307810783386, 0.6655612587928772, 0.7466842532157898, 0.6934881210327148, 0.5441005229949951, 0.9135257005691528, 0.9647537469863892, 0.9831794500350952, 0.9927948117256165, 0.026587987318634987], [0.3495256006717682, 0.9336410164833069, 0.7646846175193787, 0.5991922616958618, 0.3971666693687439, 0.5366512537002563, 0.7934256196022034, 0.8945308327674866, 0.9540460109710693, 0.6805673241615295, 0.7778409123420715, 0.716676652431488, 0.560126781463623, 0.9336410164833069, 0.9772178530693054, 0.9916306734085083, 0.99483323097229, 0.025767384096980095]]
average_result:  [0.415698903799057, 0.9125995397567749, 0.7302712559700012, 0.574711799621582, 0.3835312843322754, 0.5221845507621765, 0.7612702488899231, 0.864010238647461, 0.9301655650138855, 0.6632698059082032, 0.7444034099578858, 0.6892484784126282, 0.5423861980438233, 0.9125995397567749, 0.9648722052574158, 0.9832111835479737, 0.9916891574859619, 0.0266260489821434]
2024-06-02 22:20:36,641 : INFO : =======End=======
