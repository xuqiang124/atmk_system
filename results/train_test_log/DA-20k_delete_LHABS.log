/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2024-06-04 15:57:04,116 : INFO : Loading config...
2024-06-04 15:57:04,117 : INFO : {'cache_file_h5py': '../file_data/a30/math_data_delete.h5', 'cache_file_pickle': '../file_data/a32/vocab_label.pkl', 'embeddings': '../file_data/a32/embeddings.pkl', 'maxlen': 150, 'emb_size': 300, 'epochs': 16, 'batch_size': 512, 'alpha': 4, 'hidden_size': 512, 'num_classes_list': [15, 427], 'l_patience': 2, 'b_patience': 10}
2024-06-04 15:57:04,117 : INFO : Loading data...
2024-06-04 15:57:04,760 : INFO : Loading embeddings...
2024-06-04 15:57:04,833 : INFO : model name labs
TOTAL: 22498 TRAIN: [[ 126    3 1315 ...    0    0    0]
 [  44    3   17 ...    0    0    0]
 [ 216    3   11 ...    0    0    0]
 ...
 [ 130    3   78 ...    0    0    0]
 [ 238    3  134 ...    0    0    0]
 [  59    3   78 ...    0    0    0]] 16873 TEST: [[ 105    3 1490 ...    0    0    0]
 [ 238    3 2235 ...    0    0    0]
 [ 172    3  134 ...    0    0    0]
 ...
 [ 161    3 2144 ...    0    0    0]
 [ 161    3 4753 ...    0    0    0]
 [ 189    3   17 ...    0    0    0]] 5625
2024-06-04 15:57:04,868 : INFO : =====Start final=====
13498
3375
5625
2024-06-04 15:57:05.300881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.324148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.324257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.324502: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-04 15:57:05.326525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.326629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.326690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.652554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.652682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.652755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 15:57:05.652824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22102 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 15, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean (TFOpLambd  (None, 1024)        0           ['BiLSTM[0][0]']                 
 a)                                                                                               
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat (TFOpLambda)         (None, 2048)         0           ['tf.math.reduce_mean[0][0]',    
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense (Dense)                  (None, 1024)         2098176     ['tf.concat[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 15)           15375       ['dense[0][0]']                  
                                                                                                  
 tf.nn.softmax (TFOpLambda)     (None, 15)           0           ['dense_1[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 15, 1)        0           ['tf.nn.softmax[0][0]']          
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims[0][0]']         
 (Dense)                                                                                          
                                                                                                  
 permute_1 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15, 150)      22650       ['lambda_2[0][0]']               
                                                                                                  
 tf.math.reduce_mean_1 (TFOpLam  (None, 150)         0           ['dense_2[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_1[0][0]']  
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply (TFOpLambda)  (None, 150, 1024)    0           ['BiLSTM[0][0]',                 
                                                                  'tf.expand_dims_1[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply[0][0]']       
                                                                                                  
 lambda_3 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_3[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_4 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply[0][0]']       
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_4[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 15, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean (TFOpLambd  (None, 1024)        0           ['BiLSTM[0][0]']                 
 a)                                                                                               
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat (TFOpLambda)         (None, 2048)         0           ['tf.math.reduce_mean[0][0]',    
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense (Dense)                  (None, 1024)         2098176     ['tf.concat[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 15)           15375       ['dense[0][0]']                  
                                                                                                  
 tf.nn.softmax (TFOpLambda)     (None, 15)           0           ['dense_1[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 15, 1)        0           ['tf.nn.softmax[0][0]']          
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims[0][0]']         
 (Dense)                                                                                          
                                                                                                  
 permute_1 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15, 150)      22650       ['lambda_2[0][0]']               
                                                                                                  
 tf.math.reduce_mean_1 (TFOpLam  (None, 150)         0           ['dense_2[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_1[0][0]']  
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply (TFOpLambda)  (None, 150, 1024)    0           ['BiLSTM[0][0]',                 
                                                                  'tf.expand_dims_1[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply[0][0]']       
                                                                                                  
 lambda_3 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_3[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_4 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply[0][0]']       
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_4[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_2[0][0
                                                                 ]']                              
                                                                                                  
 dot (Dot)                      (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot[0][0]']                    
                                                                                                  
 concatenate (Concatenate)      (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/16
2024-06-04 15:57:08.663073: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8906
2024-06-04 15:57:09.208932: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/dzq/k12/atmk_system-master/MathByte/models/evaluation_metrics.py:260: RuntimeWarning: invalid value encountered in true_divide
  return (2 * p_k * r_k) / (p_k + r_k)
27/27 [==============================] - ETA: 0s - loss: 0.4932 - lcm_precision_1k: 0.2000 - lcm_precision_2k: 0.1650 - lcm_precision_3k: 0.1444 - lcm_precision_5k: 0.1162 - lcm_recall_1k: 0.1082 - lcm_recall_2k: 0.1755 - lcm_recall_3k: 0.2283 - lcm_recall_5k: 0.3048 - lcm_f1_1k: nan - lcm_f1_2k: 0.1700 - lcm_f1_3k: 0.1768 - lcm_f1_5k: 0.1683 - lcm_accuracy_1k: 0.2000 - lcm_accuracy_2k: 0.2900 - lcm_accuracy_3k: 0.3547 - lcm_accuracy_5k: 0.4397 - lcm_hamming_loss_k: 0.0059
Epoch 00001: val_loss improved from inf to 0.42733, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 14s 416ms/step - loss: 0.4932 - lcm_precision_1k: 0.2000 - lcm_precision_2k: 0.1650 - lcm_precision_3k: 0.1444 - lcm_precision_5k: 0.1162 - lcm_recall_1k: 0.1082 - lcm_recall_2k: 0.1755 - lcm_recall_3k: 0.2283 - lcm_recall_5k: 0.3048 - lcm_f1_1k: nan - lcm_f1_2k: 0.1700 - lcm_f1_3k: 0.1768 - lcm_f1_5k: 0.1683 - lcm_accuracy_1k: 0.2000 - lcm_accuracy_2k: 0.2900 - lcm_accuracy_3k: 0.3547 - lcm_accuracy_5k: 0.4397 - lcm_hamming_loss_k: 0.0059 - val_loss: 0.4273 - val_lcm_precision_1k: 0.2935 - val_lcm_precision_2k: 0.2386 - val_lcm_precision_3k: 0.1995 - val_lcm_precision_5k: 0.1582 - val_lcm_recall_1k: 0.1741 - val_lcm_recall_2k: 0.2752 - val_lcm_recall_3k: 0.3369 - val_lcm_recall_5k: 0.4346 - val_lcm_f1_1k: 0.2185 - val_lcm_f1_2k: 0.2555 - val_lcm_f1_3k: 0.2505 - val_lcm_f1_5k: 0.2319 - val_lcm_accuracy_1k: 0.2935 - val_lcm_accuracy_2k: 0.4235 - val_lcm_accuracy_3k: 0.4966 - val_lcm_accuracy_5k: 0.5927 - val_lcm_hamming_loss_k: 0.0054
Epoch 2/16
27/27 [==============================] - ETA: 0s - loss: 0.3794 - lcm_precision_1k: 0.3792 - lcm_precision_2k: 0.3149 - lcm_precision_3k: 0.2642 - lcm_precision_5k: 0.2050 - lcm_recall_1k: 0.2242 - lcm_recall_2k: 0.3557 - lcm_recall_3k: 0.4387 - lcm_recall_5k: 0.5540 - lcm_f1_1k: 0.2816 - lcm_f1_2k: 0.3340 - lcm_f1_3k: 0.3297 - lcm_f1_5k: 0.2992 - lcm_accuracy_1k: 0.3792 - lcm_accuracy_2k: 0.5293 - lcm_accuracy_3k: 0.6081 - lcm_accuracy_5k: 0.7049 - lcm_hamming_loss_k: 0.0050
Epoch 00002: val_loss improved from 0.42733 to 0.35700, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.3794 - lcm_precision_1k: 0.3792 - lcm_precision_2k: 0.3149 - lcm_precision_3k: 0.2642 - lcm_precision_5k: 0.2050 - lcm_recall_1k: 0.2242 - lcm_recall_2k: 0.3557 - lcm_recall_3k: 0.4387 - lcm_recall_5k: 0.5540 - lcm_f1_1k: 0.2816 - lcm_f1_2k: 0.3340 - lcm_f1_3k: 0.3297 - lcm_f1_5k: 0.2992 - lcm_accuracy_1k: 0.3792 - lcm_accuracy_2k: 0.5293 - lcm_accuracy_3k: 0.6081 - lcm_accuracy_5k: 0.7049 - lcm_hamming_loss_k: 0.0050 - val_loss: 0.3570 - val_lcm_precision_1k: 0.4116 - val_lcm_precision_2k: 0.3386 - val_lcm_precision_3k: 0.2841 - val_lcm_precision_5k: 0.2152 - val_lcm_recall_1k: 0.2577 - val_lcm_recall_2k: 0.3972 - val_lcm_recall_3k: 0.4895 - val_lcm_recall_5k: 0.5994 - val_lcm_f1_1k: 0.3168 - val_lcm_f1_2k: 0.3654 - val_lcm_f1_3k: 0.3594 - val_lcm_f1_5k: 0.3167 - val_lcm_accuracy_1k: 0.4116 - val_lcm_accuracy_2k: 0.5760 - val_lcm_accuracy_3k: 0.6607 - val_lcm_accuracy_5k: 0.7458 - val_lcm_hamming_loss_k: 0.0048
Epoch 3/16
27/27 [==============================] - ETA: 0s - loss: 0.3287 - lcm_precision_1k: 0.4780 - lcm_precision_2k: 0.3852 - lcm_precision_3k: 0.3196 - lcm_precision_5k: 0.2385 - lcm_recall_1k: 0.2906 - lcm_recall_2k: 0.4429 - lcm_recall_3k: 0.5389 - lcm_recall_5k: 0.6519 - lcm_f1_1k: 0.3614 - lcm_f1_2k: 0.4120 - lcm_f1_3k: 0.4012 - lcm_f1_5k: 0.3492 - lcm_accuracy_1k: 0.4780 - lcm_accuracy_2k: 0.6339 - lcm_accuracy_3k: 0.7145 - lcm_accuracy_5k: 0.7984 - lcm_hamming_loss_k: 0.0046
Epoch 00003: val_loss improved from 0.35700 to 0.33197, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 423ms/step - loss: 0.3287 - lcm_precision_1k: 0.4780 - lcm_precision_2k: 0.3852 - lcm_precision_3k: 0.3196 - lcm_precision_5k: 0.2385 - lcm_recall_1k: 0.2906 - lcm_recall_2k: 0.4429 - lcm_recall_3k: 0.5389 - lcm_recall_5k: 0.6519 - lcm_f1_1k: 0.3614 - lcm_f1_2k: 0.4120 - lcm_f1_3k: 0.4012 - lcm_f1_5k: 0.3492 - lcm_accuracy_1k: 0.4780 - lcm_accuracy_2k: 0.6339 - lcm_accuracy_3k: 0.7145 - lcm_accuracy_5k: 0.7984 - lcm_hamming_loss_k: 0.0046 - val_loss: 0.3320 - val_lcm_precision_1k: 0.4826 - val_lcm_precision_2k: 0.3796 - val_lcm_precision_3k: 0.3088 - val_lcm_precision_5k: 0.2273 - val_lcm_recall_1k: 0.2987 - val_lcm_recall_2k: 0.4452 - val_lcm_recall_3k: 0.5304 - val_lcm_recall_5k: 0.6335 - val_lcm_f1_1k: 0.3689 - val_lcm_f1_2k: 0.4097 - val_lcm_f1_3k: 0.3903 - val_lcm_f1_5k: 0.3345 - val_lcm_accuracy_1k: 0.4826 - val_lcm_accuracy_2k: 0.6257 - val_lcm_accuracy_3k: 0.6970 - val_lcm_accuracy_5k: 0.7829 - val_lcm_hamming_loss_k: 0.0045
Epoch 4/16
27/27 [==============================] - ETA: 0s - loss: 0.3060 - lcm_precision_1k: 0.5261 - lcm_precision_2k: 0.4198 - lcm_precision_3k: 0.3457 - lcm_precision_5k: 0.2544 - lcm_recall_1k: 0.3244 - lcm_recall_2k: 0.4874 - lcm_recall_3k: 0.5852 - lcm_recall_5k: 0.6965 - lcm_f1_1k: 0.4013 - lcm_f1_2k: 0.4510 - lcm_f1_3k: 0.4346 - lcm_f1_5k: 0.3726 - lcm_accuracy_1k: 0.5261 - lcm_accuracy_2k: 0.6811 - lcm_accuracy_3k: 0.7593 - lcm_accuracy_5k: 0.8363 - lcm_hamming_loss_k: 0.0044
Epoch 00004: val_loss improved from 0.33197 to 0.31622, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 12s 431ms/step - loss: 0.3060 - lcm_precision_1k: 0.5261 - lcm_precision_2k: 0.4198 - lcm_precision_3k: 0.3457 - lcm_precision_5k: 0.2544 - lcm_recall_1k: 0.3244 - lcm_recall_2k: 0.4874 - lcm_recall_3k: 0.5852 - lcm_recall_5k: 0.6965 - lcm_f1_1k: 0.4013 - lcm_f1_2k: 0.4510 - lcm_f1_3k: 0.4346 - lcm_f1_5k: 0.3726 - lcm_accuracy_1k: 0.5261 - lcm_accuracy_2k: 0.6811 - lcm_accuracy_3k: 0.7593 - lcm_accuracy_5k: 0.8363 - lcm_hamming_loss_k: 0.0044 - val_loss: 0.3162 - val_lcm_precision_1k: 0.4951 - val_lcm_precision_2k: 0.3928 - val_lcm_precision_3k: 0.3266 - val_lcm_precision_5k: 0.2400 - val_lcm_recall_1k: 0.3075 - val_lcm_recall_2k: 0.4663 - val_lcm_recall_3k: 0.5650 - val_lcm_recall_5k: 0.6722 - val_lcm_f1_1k: 0.3793 - val_lcm_f1_2k: 0.4263 - val_lcm_f1_3k: 0.4138 - val_lcm_f1_5k: 0.3537 - val_lcm_accuracy_1k: 0.4951 - val_lcm_accuracy_2k: 0.6534 - val_lcm_accuracy_3k: 0.7385 - val_lcm_accuracy_5k: 0.8140 - val_lcm_hamming_loss_k: 0.0044
Epoch 5/16
27/27 [==============================] - ETA: 0s - loss: 0.2918 - lcm_precision_1k: 0.5534 - lcm_precision_2k: 0.4365 - lcm_precision_3k: 0.3621 - lcm_precision_5k: 0.2644 - lcm_recall_1k: 0.3417 - lcm_recall_2k: 0.5073 - lcm_recall_3k: 0.6152 - lcm_recall_5k: 0.7242 - lcm_f1_1k: 0.4224 - lcm_f1_2k: 0.4692 - lcm_f1_3k: 0.4558 - lcm_f1_5k: 0.3873 - lcm_accuracy_1k: 0.5534 - lcm_accuracy_2k: 0.7060 - lcm_accuracy_3k: 0.7897 - lcm_accuracy_5k: 0.8591 - lcm_hamming_loss_k: 0.0042
Epoch 00005: val_loss improved from 0.31622 to 0.30869, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2918 - lcm_precision_1k: 0.5534 - lcm_precision_2k: 0.4365 - lcm_precision_3k: 0.3621 - lcm_precision_5k: 0.2644 - lcm_recall_1k: 0.3417 - lcm_recall_2k: 0.5073 - lcm_recall_3k: 0.6152 - lcm_recall_5k: 0.7242 - lcm_f1_1k: 0.4224 - lcm_f1_2k: 0.4692 - lcm_f1_3k: 0.4558 - lcm_f1_5k: 0.3873 - lcm_accuracy_1k: 0.5534 - lcm_accuracy_2k: 0.7060 - lcm_accuracy_3k: 0.7897 - lcm_accuracy_5k: 0.8591 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.3087 - val_lcm_precision_1k: 0.5134 - val_lcm_precision_2k: 0.4057 - val_lcm_precision_3k: 0.3348 - val_lcm_precision_5k: 0.2426 - val_lcm_recall_1k: 0.3211 - val_lcm_recall_2k: 0.4821 - val_lcm_recall_3k: 0.5830 - val_lcm_recall_5k: 0.6803 - val_lcm_f1_1k: 0.3949 - val_lcm_f1_2k: 0.4405 - val_lcm_f1_3k: 0.4252 - val_lcm_f1_5k: 0.3576 - val_lcm_accuracy_1k: 0.5134 - val_lcm_accuracy_2k: 0.6660 - val_lcm_accuracy_3k: 0.7558 - val_lcm_accuracy_5k: 0.8213 - val_lcm_hamming_loss_k: 0.0043
Epoch 6/16
27/27 [==============================] - ETA: 0s - loss: 0.2804 - lcm_precision_1k: 0.5776 - lcm_precision_2k: 0.4566 - lcm_precision_3k: 0.3735 - lcm_precision_5k: 0.2712 - lcm_recall_1k: 0.3582 - lcm_recall_2k: 0.5336 - lcm_recall_3k: 0.6347 - lcm_recall_5k: 0.7432 - lcm_f1_1k: 0.4422 - lcm_f1_2k: 0.4920 - lcm_f1_3k: 0.4703 - lcm_f1_5k: 0.3974 - lcm_accuracy_1k: 0.5776 - lcm_accuracy_2k: 0.7326 - lcm_accuracy_3k: 0.8091 - lcm_accuracy_5k: 0.8727 - lcm_hamming_loss_k: 0.0041
Epoch 00006: val_loss improved from 0.30869 to 0.30148, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 421ms/step - loss: 0.2804 - lcm_precision_1k: 0.5776 - lcm_precision_2k: 0.4566 - lcm_precision_3k: 0.3735 - lcm_precision_5k: 0.2712 - lcm_recall_1k: 0.3582 - lcm_recall_2k: 0.5336 - lcm_recall_3k: 0.6347 - lcm_recall_5k: 0.7432 - lcm_f1_1k: 0.4422 - lcm_f1_2k: 0.4920 - lcm_f1_3k: 0.4703 - lcm_f1_5k: 0.3974 - lcm_accuracy_1k: 0.5776 - lcm_accuracy_2k: 0.7326 - lcm_accuracy_3k: 0.8091 - lcm_accuracy_5k: 0.8727 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.3015 - val_lcm_precision_1k: 0.5278 - val_lcm_precision_2k: 0.4225 - val_lcm_precision_3k: 0.3455 - val_lcm_precision_5k: 0.2475 - val_lcm_recall_1k: 0.3293 - val_lcm_recall_2k: 0.5013 - val_lcm_recall_3k: 0.5978 - val_lcm_recall_5k: 0.6918 - val_lcm_f1_1k: 0.4054 - val_lcm_f1_2k: 0.4584 - val_lcm_f1_3k: 0.4377 - val_lcm_f1_5k: 0.3645 - val_lcm_accuracy_1k: 0.5278 - val_lcm_accuracy_2k: 0.6909 - val_lcm_accuracy_3k: 0.7663 - val_lcm_accuracy_5k: 0.8286 - val_lcm_hamming_loss_k: 0.0043
Epoch 7/16
27/27 [==============================] - ETA: 0s - loss: 0.2693 - lcm_precision_1k: 0.5995 - lcm_precision_2k: 0.4704 - lcm_precision_3k: 0.3838 - lcm_precision_5k: 0.2772 - lcm_recall_1k: 0.3720 - lcm_recall_2k: 0.5521 - lcm_recall_3k: 0.6540 - lcm_recall_5k: 0.7593 - lcm_f1_1k: 0.4590 - lcm_f1_2k: 0.5079 - lcm_f1_3k: 0.4836 - lcm_f1_5k: 0.4061 - lcm_accuracy_1k: 0.5995 - lcm_accuracy_2k: 0.7517 - lcm_accuracy_3k: 0.8243 - lcm_accuracy_5k: 0.8862 - lcm_hamming_loss_k: 0.0040
Epoch 00007: val_loss improved from 0.30148 to 0.29651, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.2693 - lcm_precision_1k: 0.5995 - lcm_precision_2k: 0.4704 - lcm_precision_3k: 0.3838 - lcm_precision_5k: 0.2772 - lcm_recall_1k: 0.3720 - lcm_recall_2k: 0.5521 - lcm_recall_3k: 0.6540 - lcm_recall_5k: 0.7593 - lcm_f1_1k: 0.4590 - lcm_f1_2k: 0.5079 - lcm_f1_3k: 0.4836 - lcm_f1_5k: 0.4061 - lcm_accuracy_1k: 0.5995 - lcm_accuracy_2k: 0.7517 - lcm_accuracy_3k: 0.8243 - lcm_accuracy_5k: 0.8862 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2965 - val_lcm_precision_1k: 0.5364 - val_lcm_precision_2k: 0.4274 - val_lcm_precision_3k: 0.3476 - val_lcm_precision_5k: 0.2502 - val_lcm_recall_1k: 0.3373 - val_lcm_recall_2k: 0.5058 - val_lcm_recall_3k: 0.6028 - val_lcm_recall_5k: 0.7004 - val_lcm_f1_1k: 0.4140 - val_lcm_f1_2k: 0.4632 - val_lcm_f1_3k: 0.4408 - val_lcm_f1_5k: 0.3686 - val_lcm_accuracy_1k: 0.5364 - val_lcm_accuracy_2k: 0.6957 - val_lcm_accuracy_3k: 0.7749 - val_lcm_accuracy_5k: 0.8370 - val_lcm_hamming_loss_k: 0.0042
Epoch 8/16
27/27 [==============================] - ETA: 0s - loss: 0.2617 - lcm_precision_1k: 0.6147 - lcm_precision_2k: 0.4824 - lcm_precision_3k: 0.3907 - lcm_precision_5k: 0.2820 - lcm_recall_1k: 0.3840 - lcm_recall_2k: 0.5653 - lcm_recall_3k: 0.6646 - lcm_recall_5k: 0.7722 - lcm_f1_1k: 0.4727 - lcm_f1_2k: 0.5205 - lcm_f1_3k: 0.4921 - lcm_f1_5k: 0.4131 - lcm_accuracy_1k: 0.6147 - lcm_accuracy_2k: 0.7669 - lcm_accuracy_3k: 0.8339 - lcm_accuracy_5k: 0.8949 - lcm_hamming_loss_k: 0.0039
Epoch 00008: val_loss improved from 0.29651 to 0.29543, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2617 - lcm_precision_1k: 0.6147 - lcm_precision_2k: 0.4824 - lcm_precision_3k: 0.3907 - lcm_precision_5k: 0.2820 - lcm_recall_1k: 0.3840 - lcm_recall_2k: 0.5653 - lcm_recall_3k: 0.6646 - lcm_recall_5k: 0.7722 - lcm_f1_1k: 0.4727 - lcm_f1_2k: 0.5205 - lcm_f1_3k: 0.4921 - lcm_f1_5k: 0.4131 - lcm_accuracy_1k: 0.6147 - lcm_accuracy_2k: 0.7669 - lcm_accuracy_3k: 0.8339 - lcm_accuracy_5k: 0.8949 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2954 - val_lcm_precision_1k: 0.5364 - val_lcm_precision_2k: 0.4300 - val_lcm_precision_3k: 0.3519 - val_lcm_precision_5k: 0.2517 - val_lcm_recall_1k: 0.3354 - val_lcm_recall_2k: 0.5108 - val_lcm_recall_3k: 0.6073 - val_lcm_recall_5k: 0.7038 - val_lcm_f1_1k: 0.4127 - val_lcm_f1_2k: 0.4668 - val_lcm_f1_3k: 0.4455 - val_lcm_f1_5k: 0.3707 - val_lcm_accuracy_1k: 0.5364 - val_lcm_accuracy_2k: 0.6992 - val_lcm_accuracy_3k: 0.7732 - val_lcm_accuracy_5k: 0.8392 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/16
27/27 [==============================] - ETA: 0s - loss: 0.2569 - lcm_precision_1k: 0.6284 - lcm_precision_2k: 0.4929 - lcm_precision_3k: 0.3994 - lcm_precision_5k: 0.2867 - lcm_recall_1k: 0.3939 - lcm_recall_2k: 0.5787 - lcm_recall_3k: 0.6796 - lcm_recall_5k: 0.7858 - lcm_f1_1k: 0.4841 - lcm_f1_2k: 0.5323 - lcm_f1_3k: 0.5031 - lcm_f1_5k: 0.4200 - lcm_accuracy_1k: 0.6284 - lcm_accuracy_2k: 0.7776 - lcm_accuracy_3k: 0.8444 - lcm_accuracy_5k: 0.9050 - lcm_hamming_loss_k: 0.0039 ETA: 5s - loss: 0.2571 - lcm_precision_1k: 0.6378 - lcm_precision_2k: 0.5016 - lcm_precision_3k: 0.4063 - lcm_precision_5k: 0.2908 - lcm_recall_1k: 0.3943 - lcm_recall_2k: 0.5816 - lcm_recall_3k: 0.6833 - lcm_recall_5k: 0.7899 - lcm_f1_1k: 0.4873 - lcm_f1_2k: 0.5386 - lcm_f1_3k: 0.5096 - lcm_f1_5k: 0.4251 - lcm_accuracy_1k: 0.6378 - lcm_accuracy_2k: 0.7811 - lcm_accuracy_3k: 0.8480 - lcm_accuracy_5k: 0.9
Epoch 00009: val_loss improved from 0.29543 to 0.29199, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.2569 - lcm_precision_1k: 0.6284 - lcm_precision_2k: 0.4929 - lcm_precision_3k: 0.3994 - lcm_precision_5k: 0.2867 - lcm_recall_1k: 0.3939 - lcm_recall_2k: 0.5787 - lcm_recall_3k: 0.6796 - lcm_recall_5k: 0.7858 - lcm_f1_1k: 0.4841 - lcm_f1_2k: 0.5323 - lcm_f1_3k: 0.5031 - lcm_f1_5k: 0.4200 - lcm_accuracy_1k: 0.6284 - lcm_accuracy_2k: 0.7776 - lcm_accuracy_3k: 0.8444 - lcm_accuracy_5k: 0.9050 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2920 - val_lcm_precision_1k: 0.5474 - val_lcm_precision_2k: 0.4329 - val_lcm_precision_3k: 0.3531 - val_lcm_precision_5k: 0.2520 - val_lcm_recall_1k: 0.3451 - val_lcm_recall_2k: 0.5151 - val_lcm_recall_3k: 0.6120 - val_lcm_recall_5k: 0.7091 - val_lcm_f1_1k: 0.4233 - val_lcm_f1_2k: 0.4703 - val_lcm_f1_3k: 0.4477 - val_lcm_f1_5k: 0.3718 - val_lcm_accuracy_1k: 0.5474 - val_lcm_accuracy_2k: 0.7021 - val_lcm_accuracy_3k: 0.7767 - val_lcm_accuracy_5k: 0.8455 - val_lcm_hamming_loss_k: 0.0042
Epoch 10/16
27/27 [==============================] - ETA: 0s - loss: 0.2481 - lcm_precision_1k: 0.6434 - lcm_precision_2k: 0.5035 - lcm_precision_3k: 0.4087 - lcm_precision_5k: 0.2918 - lcm_recall_1k: 0.4023 - lcm_recall_2k: 0.5881 - lcm_recall_3k: 0.6938 - lcm_recall_5k: 0.7968 - lcm_f1_1k: 0.4950 - lcm_f1_2k: 0.5424 - lcm_f1_3k: 0.5143 - lcm_f1_5k: 0.4271 - lcm_accuracy_1k: 0.6434 - lcm_accuracy_2k: 0.7877 - lcm_accuracy_3k: 0.8567 - lcm_accuracy_5k: 0.9124 - lcm_hamming_loss_k: 0.0038
Epoch 00010: val_loss improved from 0.29199 to 0.28912, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.2481 - lcm_precision_1k: 0.6434 - lcm_precision_2k: 0.5035 - lcm_precision_3k: 0.4087 - lcm_precision_5k: 0.2918 - lcm_recall_1k: 0.4023 - lcm_recall_2k: 0.5881 - lcm_recall_3k: 0.6938 - lcm_recall_5k: 0.7968 - lcm_f1_1k: 0.4950 - lcm_f1_2k: 0.5424 - lcm_f1_3k: 0.5143 - lcm_f1_5k: 0.4271 - lcm_accuracy_1k: 0.6434 - lcm_accuracy_2k: 0.7877 - lcm_accuracy_3k: 0.8567 - lcm_accuracy_5k: 0.9124 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2891 - val_lcm_precision_1k: 0.5535 - val_lcm_precision_2k: 0.4367 - val_lcm_precision_3k: 0.3552 - val_lcm_precision_5k: 0.2569 - val_lcm_recall_1k: 0.3489 - val_lcm_recall_2k: 0.5196 - val_lcm_recall_3k: 0.6180 - val_lcm_recall_5k: 0.7197 - val_lcm_f1_1k: 0.4279 - val_lcm_f1_2k: 0.4744 - val_lcm_f1_3k: 0.4510 - val_lcm_f1_5k: 0.3786 - val_lcm_accuracy_1k: 0.5535 - val_lcm_accuracy_2k: 0.7112 - val_lcm_accuracy_3k: 0.7869 - val_lcm_accuracy_5k: 0.8522 - val_lcm_hamming_loss_k: 0.0042
Epoch 11/16
27/27 [==============================] - ETA: 0s - loss: 0.2418 - lcm_precision_1k: 0.6576 - lcm_precision_2k: 0.5132 - lcm_precision_3k: 0.4148 - lcm_precision_5k: 0.2961 - lcm_recall_1k: 0.4132 - lcm_recall_2k: 0.6032 - lcm_recall_3k: 0.7072 - lcm_recall_5k: 0.8101 - lcm_f1_1k: 0.5074 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5228 - lcm_f1_5k: 0.4336 - lcm_accuracy_1k: 0.6576 - lcm_accuracy_2k: 0.8062 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9215 - lcm_hamming_loss_k: 0.0037 ETA: 5s - loss: 0.2380 - lcm_precision_1k: 0.6660 - lcm_precision_2k: 0.5193 - lcm_precision_3k: 0.4214 - lcm_precision_5k: 0.3008 - lcm_recall_1k: 0.4203 - lcm_recall_2k: 0.6142 - lcm_recall_3k: 0.7226 - lcm_recall_5k: 0.8257 - lcm_f1_1k: 0.5153 - lcm_f1_2k: 0.5627 - lcm_f1_3k: 0.5323 - lcm_f1_5k: 0.4410 - lcm_accuracy_1k: 0.6660 - lcm_accuracy_2k: 0.8158 - lcm_accuracy_3k: 0.8843 - lcm_accuracy_5k:
Epoch 00011: val_loss improved from 0.28912 to 0.28685, saving model to logs/afzzvl-labs-0604-155705/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2418 - lcm_precision_1k: 0.6576 - lcm_precision_2k: 0.5132 - lcm_precision_3k: 0.4148 - lcm_precision_5k: 0.2961 - lcm_recall_1k: 0.4132 - lcm_recall_2k: 0.6032 - lcm_recall_3k: 0.7072 - lcm_recall_5k: 0.8101 - lcm_f1_1k: 0.5074 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5228 - lcm_f1_5k: 0.4336 - lcm_accuracy_1k: 0.6576 - lcm_accuracy_2k: 0.8062 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9215 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2868 - val_lcm_precision_1k: 0.5650 - val_lcm_precision_2k: 0.4408 - val_lcm_precision_3k: 0.3591 - val_lcm_precision_5k: 0.2571 - val_lcm_recall_1k: 0.3543 - val_lcm_recall_2k: 0.5250 - val_lcm_recall_3k: 0.6241 - val_lcm_recall_5k: 0.7198 - val_lcm_f1_1k: 0.4354 - val_lcm_f1_2k: 0.4791 - val_lcm_f1_3k: 0.4558 - val_lcm_f1_5k: 0.3788 - val_lcm_accuracy_1k: 0.5650 - val_lcm_accuracy_2k: 0.7123 - val_lcm_accuracy_3k: 0.7925 - val_lcm_accuracy_5k: 0.8526 - val_lcm_hamming_loss_k: 0.0041
Epoch 12/16
27/27 [==============================] - ETA: 0s - loss: 0.2359 - lcm_precision_1k: 0.6693 - lcm_precision_2k: 0.5245 - lcm_precision_3k: 0.4249 - lcm_precision_5k: 0.3005 - lcm_recall_1k: 0.4224 - lcm_recall_2k: 0.6158 - lcm_recall_3k: 0.7234 - lcm_recall_5k: 0.8226 - lcm_f1_1k: 0.5178 - lcm_f1_2k: 0.5664 - lcm_f1_3k: 0.5353 - lcm_f1_5k: 0.4401 - lcm_accuracy_1k: 0.6693 - lcm_accuracy_2k: 0.8144 - lcm_accuracy_3k: 0.8811 - lcm_accuracy_5k: 0.9287 - lcm_hamming_loss_k: 0.0037
Epoch 00012: val_loss did not improve from 0.28685
27/27 [==============================] - 10s 390ms/step - loss: 0.2359 - lcm_precision_1k: 0.6693 - lcm_precision_2k: 0.5245 - lcm_precision_3k: 0.4249 - lcm_precision_5k: 0.3005 - lcm_recall_1k: 0.4224 - lcm_recall_2k: 0.6158 - lcm_recall_3k: 0.7234 - lcm_recall_5k: 0.8226 - lcm_f1_1k: 0.5178 - lcm_f1_2k: 0.5664 - lcm_f1_3k: 0.5353 - lcm_f1_5k: 0.4401 - lcm_accuracy_1k: 0.6693 - lcm_accuracy_2k: 0.8144 - lcm_accuracy_3k: 0.8811 - lcm_accuracy_5k: 0.9287 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2888 - val_lcm_precision_1k: 0.5594 - val_lcm_precision_2k: 0.4443 - val_lcm_precision_3k: 0.3615 - val_lcm_precision_5k: 0.2590 - val_lcm_recall_1k: 0.3501 - val_lcm_recall_2k: 0.5286 - val_lcm_recall_3k: 0.6273 - val_lcm_recall_5k: 0.7264 - val_lcm_f1_1k: 0.4306 - val_lcm_f1_2k: 0.4826 - val_lcm_f1_3k: 0.4585 - val_lcm_f1_5k: 0.3817 - val_lcm_accuracy_1k: 0.5594 - val_lcm_accuracy_2k: 0.7175 - val_lcm_accuracy_3k: 0.7954 - val_lcm_accuracy_5k: 0.8585 - val_lcm_hamming_loss_k: 0.0041
Epoch 13/16
27/27 [==============================] - ETA: 0s - loss: 0.2296 - lcm_precision_1k: 0.6821 - lcm_precision_2k: 0.5334 - lcm_precision_3k: 0.4301 - lcm_precision_5k: 0.3052 - lcm_recall_1k: 0.4303 - lcm_recall_2k: 0.6249 - lcm_recall_3k: 0.7291 - lcm_recall_5k: 0.8323 - lcm_f1_1k: 0.5276 - lcm_f1_2k: 0.5754 - lcm_f1_3k: 0.5410 - lcm_f1_5k: 0.4466 - lcm_accuracy_1k: 0.6821 - lcm_accuracy_2k: 0.8254 - lcm_accuracy_3k: 0.8867 - lcm_accuracy_5k: 0.9358 - lcm_hamming_loss_k: 0.0036
Epoch 00013: val_loss did not improve from 0.28685
27/27 [==============================] - 11s 391ms/step - loss: 0.2296 - lcm_precision_1k: 0.6821 - lcm_precision_2k: 0.5334 - lcm_precision_3k: 0.4301 - lcm_precision_5k: 0.3052 - lcm_recall_1k: 0.4303 - lcm_recall_2k: 0.6249 - lcm_recall_3k: 0.7291 - lcm_recall_5k: 0.8323 - lcm_f1_1k: 0.5276 - lcm_f1_2k: 0.5754 - lcm_f1_3k: 0.5410 - lcm_f1_5k: 0.4466 - lcm_accuracy_1k: 0.6821 - lcm_accuracy_2k: 0.8254 - lcm_accuracy_3k: 0.8867 - lcm_accuracy_5k: 0.9358 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2882 - val_lcm_precision_1k: 0.5604 - val_lcm_precision_2k: 0.4450 - val_lcm_precision_3k: 0.3595 - val_lcm_precision_5k: 0.2591 - val_lcm_recall_1k: 0.3511 - val_lcm_recall_2k: 0.5291 - val_lcm_recall_3k: 0.6238 - val_lcm_recall_5k: 0.7243 - val_lcm_f1_1k: 0.4316 - val_lcm_f1_2k: 0.4833 - val_lcm_f1_3k: 0.4560 - val_lcm_f1_5k: 0.3816 - val_lcm_accuracy_1k: 0.5604 - val_lcm_accuracy_2k: 0.7161 - val_lcm_accuracy_3k: 0.7883 - val_lcm_accuracy_5k: 0.8555 - val_lcm_hamming_loss_k: 0.0041
Epoch 00013: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2565 - lcm_precision_1k: 0.6285 - lcm_precision_2k: 0.4891 - lcm_precision_3k: 0.3923 - lcm_precision_5k: 0.2812 - lcm_recall_1k: 0.3965 - lcm_recall_2k: 0.5789 - lcm_recall_3k: 0.6738 - lcm_recall_5k: 0.7758 - lcm_f1_1k: 0.4849 - lcm_f1_2k: 0.5291 - lcm_f1_3k: 0.4949 - lcm_f1_5k: 0.4120 - lcm_accuracy_1k: 0.6285 - lcm_accuracy_2k: 0.7700 - lcm_accuracy_3k: 0.8377 - lcm_accuracy_5k: 0.8937 - lcm_hamming_loss_k: 0.0038
Best model result:  [0.25650879740715027, 0.6285068392753601, 0.48912903666496277, 0.39226293563842773, 0.2811790704727173, 0.3965180516242981, 0.5789174437522888, 0.673799991607666, 0.7758073806762695, 0.48493242263793945, 0.5291039943695068, 0.4948848485946655, 0.41203343868255615, 0.6285068392753601, 0.769993245601654, 0.8376939296722412, 0.8936727643013, 0.003841844154521823]
13498
3375
5625
Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_5 (Lambda)              (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_5[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_6 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_2 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_6[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_2[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_3 (Dense)                (None, 1024)         2098176     ['tf.concat_1[0][0]']            
                                                                                                  
 dense_4 (Dense)                (None, 15)           15375       ['dense_3[0][0]']                
                                                                                                  
 tf.nn.softmax_1 (TFOpLambda)   (None, 15)           0           ['dense_4[0][0]']                
                                                                                                  
 tf.expand_dims_2 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_1[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_2[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_4 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_7 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_4[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 15, 150)      22650       ['lambda_7[0][0]']               
                                                                                                  
 tf.math.reduce_mean_3 (TFOpLam  (None, 150)         0           ['dense_5[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_3 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_3[0][0]']  
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_1 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_3[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_5 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_1[0][0]']     
                                                                                                  
 lambda_8 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_5[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_1[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_3"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_5 (Lambda)              (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_5[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_6 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_2 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_6[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_2[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_3 (Dense)                (None, 1024)         2098176     ['tf.concat_1[0][0]']            
                                                                                                  
 dense_4 (Dense)                (None, 15)           15375       ['dense_3[0][0]']                
                                                                                                  
 tf.nn.softmax_1 (TFOpLambda)   (None, 15)           0           ['dense_4[0][0]']                
                                                                                                  
 tf.expand_dims_2 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_1[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_2[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_4 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_7 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_4[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 15, 150)      22650       ['lambda_7[0][0]']               
                                                                                                  
 tf.math.reduce_mean_3 (TFOpLam  (None, 150)         0           ['dense_5[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_3 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_3[0][0]']  
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_1 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_3[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_5 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_1[0][0]']     
                                                                                                  
 lambda_8 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_5[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_1[0][0]']     
                                                                                                  
 tf.__operators__.getitem_5 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_5[0][0
                                                                 ]']                              
                                                                                                  
 dot_1 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_1[0][0]']                  
                                                                                                  
 concatenate_1 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/16
27/27 [==============================] - ETA: 0s - loss: 0.4964 - lcm_precision_1k: 0.1386 - lcm_precision_2k: 0.1358 - lcm_precision_3k: 0.1246 - lcm_precision_5k: 0.1056 - lcm_recall_1k: 0.0741 - lcm_recall_2k: 0.1481 - lcm_recall_3k: 0.2022 - lcm_recall_5k: 0.2823 - lcm_f1_1k: 0.0965 - lcm_f1_2k: 0.1416 - lcm_f1_3k: 0.1541 - lcm_f1_5k: 0.1537 - lcm_accuracy_1k: 0.1386 - lcm_accuracy_2k: 0.2518 - lcm_accuracy_3k: 0.3175 - lcm_accuracy_5k: 0.4101 - lcm_hamming_loss_k: 0.0062 ETA: 3s - loss: 0.5193 - lcm_precision_1k: 0.1057 - lcm_precision_2k: 0.1017 - lcm_precision_3k: 0.0934 - lcm_precision_5k: 0.0788 - lcm_recall_1k: 0.0570 - lcm_recall_2k: 0.1119 - lcm_recall_3k: 0.1534 - lcm_recall_5k: 0.2121 - lcm_f1_1k: 0.0739 - lcm_f1_2k: 0.1065 - lcm_f1_3k: 0.1161 - lcm_f1_5k: 0.1149 - lcm_accuracy_1k: 0.1057 - lcm_accuracy_2k: 0.1924 - lcm_accuracy_3k: 0.2462 - lcm_accuracy_5k: 0.3226 - lcm_
Epoch 00001: val_loss improved from inf to 0.43138, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 13s 417ms/step - loss: 0.4964 - lcm_precision_1k: 0.1386 - lcm_precision_2k: 0.1358 - lcm_precision_3k: 0.1246 - lcm_precision_5k: 0.1056 - lcm_recall_1k: 0.0741 - lcm_recall_2k: 0.1481 - lcm_recall_3k: 0.2022 - lcm_recall_5k: 0.2823 - lcm_f1_1k: 0.0965 - lcm_f1_2k: 0.1416 - lcm_f1_3k: 0.1541 - lcm_f1_5k: 0.1537 - lcm_accuracy_1k: 0.1386 - lcm_accuracy_2k: 0.2518 - lcm_accuracy_3k: 0.3175 - lcm_accuracy_5k: 0.4101 - lcm_hamming_loss_k: 0.0062 - val_loss: 0.4314 - val_lcm_precision_1k: 0.3109 - val_lcm_precision_2k: 0.2541 - val_lcm_precision_3k: 0.2161 - val_lcm_precision_5k: 0.1779 - val_lcm_recall_1k: 0.1848 - val_lcm_recall_2k: 0.2906 - val_lcm_recall_3k: 0.3619 - val_lcm_recall_5k: 0.4778 - val_lcm_f1_1k: 0.2316 - val_lcm_f1_2k: 0.2709 - val_lcm_f1_3k: 0.2705 - val_lcm_f1_5k: 0.2592 - val_lcm_accuracy_1k: 0.3109 - val_lcm_accuracy_2k: 0.4373 - val_lcm_accuracy_3k: 0.5154 - val_lcm_accuracy_5k: 0.6306 - val_lcm_hamming_loss_k: 0.0054
Epoch 2/16
27/27 [==============================] - ETA: 0s - loss: 0.3807 - lcm_precision_1k: 0.3769 - lcm_precision_2k: 0.3116 - lcm_precision_3k: 0.2650 - lcm_precision_5k: 0.2043 - lcm_recall_1k: 0.2259 - lcm_recall_2k: 0.3560 - lcm_recall_3k: 0.4450 - lcm_recall_5k: 0.5548 - lcm_f1_1k: 0.2824 - lcm_f1_2k: 0.3322 - lcm_f1_3k: 0.3321 - lcm_f1_5k: 0.2986 - lcm_accuracy_1k: 0.3769 - lcm_accuracy_2k: 0.5275 - lcm_accuracy_3k: 0.6170 - lcm_accuracy_5k: 0.7056 - lcm_hamming_loss_k: 0.0050
Epoch 00002: val_loss improved from 0.43138 to 0.35047, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.3807 - lcm_precision_1k: 0.3769 - lcm_precision_2k: 0.3116 - lcm_precision_3k: 0.2650 - lcm_precision_5k: 0.2043 - lcm_recall_1k: 0.2259 - lcm_recall_2k: 0.3560 - lcm_recall_3k: 0.4450 - lcm_recall_5k: 0.5548 - lcm_f1_1k: 0.2824 - lcm_f1_2k: 0.3322 - lcm_f1_3k: 0.3321 - lcm_f1_5k: 0.2986 - lcm_accuracy_1k: 0.3769 - lcm_accuracy_2k: 0.5275 - lcm_accuracy_3k: 0.6170 - lcm_accuracy_5k: 0.7056 - lcm_hamming_loss_k: 0.0050 - val_loss: 0.3505 - val_lcm_precision_1k: 0.4401 - val_lcm_precision_2k: 0.3567 - val_lcm_precision_3k: 0.2983 - val_lcm_precision_5k: 0.2259 - val_lcm_recall_1k: 0.2640 - val_lcm_recall_2k: 0.4089 - val_lcm_recall_3k: 0.5039 - val_lcm_recall_5k: 0.6148 - val_lcm_f1_1k: 0.3299 - val_lcm_f1_2k: 0.3809 - val_lcm_f1_3k: 0.3746 - val_lcm_f1_5k: 0.3303 - val_lcm_accuracy_1k: 0.4401 - val_lcm_accuracy_2k: 0.5920 - val_lcm_accuracy_3k: 0.6768 - val_lcm_accuracy_5k: 0.7677 - val_lcm_hamming_loss_k: 0.0048
Epoch 3/16
27/27 [==============================] - ETA: 0s - loss: 0.3289 - lcm_precision_1k: 0.4753 - lcm_precision_2k: 0.3818 - lcm_precision_3k: 0.3172 - lcm_precision_5k: 0.2379 - lcm_recall_1k: 0.2894 - lcm_recall_2k: 0.4419 - lcm_recall_3k: 0.5391 - lcm_recall_5k: 0.6540 - lcm_f1_1k: 0.3597 - lcm_f1_2k: 0.4096 - lcm_f1_3k: 0.3993 - lcm_f1_5k: 0.3489 - lcm_accuracy_1k: 0.4753 - lcm_accuracy_2k: 0.6288 - lcm_accuracy_3k: 0.7147 - lcm_accuracy_5k: 0.8017 - lcm_hamming_loss_k: 0.0046
Epoch 00003: val_loss improved from 0.35047 to 0.32918, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.3289 - lcm_precision_1k: 0.4753 - lcm_precision_2k: 0.3818 - lcm_precision_3k: 0.3172 - lcm_precision_5k: 0.2379 - lcm_recall_1k: 0.2894 - lcm_recall_2k: 0.4419 - lcm_recall_3k: 0.5391 - lcm_recall_5k: 0.6540 - lcm_f1_1k: 0.3597 - lcm_f1_2k: 0.4096 - lcm_f1_3k: 0.3993 - lcm_f1_5k: 0.3489 - lcm_accuracy_1k: 0.4753 - lcm_accuracy_2k: 0.6288 - lcm_accuracy_3k: 0.7147 - lcm_accuracy_5k: 0.8017 - lcm_hamming_loss_k: 0.0046 - val_loss: 0.3292 - val_lcm_precision_1k: 0.4934 - val_lcm_precision_2k: 0.3962 - val_lcm_precision_3k: 0.3281 - val_lcm_precision_5k: 0.2414 - val_lcm_recall_1k: 0.2988 - val_lcm_recall_2k: 0.4549 - val_lcm_recall_3k: 0.5537 - val_lcm_recall_5k: 0.6594 - val_lcm_f1_1k: 0.3721 - val_lcm_f1_2k: 0.4234 - val_lcm_f1_3k: 0.4119 - val_lcm_f1_5k: 0.3533 - val_lcm_accuracy_1k: 0.4934 - val_lcm_accuracy_2k: 0.6434 - val_lcm_accuracy_3k: 0.7293 - val_lcm_accuracy_5k: 0.8067 - val_lcm_hamming_loss_k: 0.0045
Epoch 4/16
27/27 [==============================] - ETA: 0s - loss: 0.3063 - lcm_precision_1k: 0.5206 - lcm_precision_2k: 0.4157 - lcm_precision_3k: 0.3436 - lcm_precision_5k: 0.2529 - lcm_recall_1k: 0.3210 - lcm_recall_2k: 0.4842 - lcm_recall_3k: 0.5853 - lcm_recall_5k: 0.6958 - lcm_f1_1k: 0.3971 - lcm_f1_2k: 0.4472 - lcm_f1_3k: 0.4329 - lcm_f1_5k: 0.3709 - lcm_accuracy_1k: 0.5206 - lcm_accuracy_2k: 0.6770 - lcm_accuracy_3k: 0.7601 - lcm_accuracy_5k: 0.8358 - lcm_hamming_loss_k: 0.0044 ETA: 5s - loss: 0.3118 - lcm_precision_1k: 0.5182 - lcm_precision_2k: 0.4142 - lcm_precision_3k: 0.3389 - lcm_precision_5k: 0.2495 - lcm_recall_1k: 0.3196 - lcm_recall_2k: 0.4801 - lcm_recall_3k: 0.5758 - lcm_recall_5k: 0.6860 - lcm_f1_1k: 0.3953 - lcm_f1_2k: 0.4446 - lcm_f1_3k: 0.4267 - lcm_f1_5k: 0.3659 - lcm_accuracy_1k: 0.5182 - lcm_accuracy_2k: 0.6752 - lcm_accuracy_3k: 0.7506 - lcm_accuracy_5k: 0
Epoch 00004: val_loss improved from 0.32918 to 0.31340, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.3063 - lcm_precision_1k: 0.5206 - lcm_precision_2k: 0.4157 - lcm_precision_3k: 0.3436 - lcm_precision_5k: 0.2529 - lcm_recall_1k: 0.3210 - lcm_recall_2k: 0.4842 - lcm_recall_3k: 0.5853 - lcm_recall_5k: 0.6958 - lcm_f1_1k: 0.3971 - lcm_f1_2k: 0.4472 - lcm_f1_3k: 0.4329 - lcm_f1_5k: 0.3709 - lcm_accuracy_1k: 0.5206 - lcm_accuracy_2k: 0.6770 - lcm_accuracy_3k: 0.7601 - lcm_accuracy_5k: 0.8358 - lcm_hamming_loss_k: 0.0044 - val_loss: 0.3134 - val_lcm_precision_1k: 0.5082 - val_lcm_precision_2k: 0.4131 - val_lcm_precision_3k: 0.3416 - val_lcm_precision_5k: 0.2512 - val_lcm_recall_1k: 0.3107 - val_lcm_recall_2k: 0.4786 - val_lcm_recall_3k: 0.5775 - val_lcm_recall_5k: 0.6873 - val_lcm_f1_1k: 0.3855 - val_lcm_f1_2k: 0.4432 - val_lcm_f1_3k: 0.4291 - val_lcm_f1_5k: 0.3678 - val_lcm_accuracy_1k: 0.5082 - val_lcm_accuracy_2k: 0.6713 - val_lcm_accuracy_3k: 0.7567 - val_lcm_accuracy_5k: 0.8306 - val_lcm_hamming_loss_k: 0.0045
Epoch 5/16
27/27 [==============================] - ETA: 0s - loss: 0.2904 - lcm_precision_1k: 0.5538 - lcm_precision_2k: 0.4384 - lcm_precision_3k: 0.3603 - lcm_precision_5k: 0.2626 - lcm_recall_1k: 0.3441 - lcm_recall_2k: 0.5147 - lcm_recall_3k: 0.6148 - lcm_recall_5k: 0.7224 - lcm_f1_1k: 0.4244 - lcm_f1_2k: 0.4734 - lcm_f1_3k: 0.4543 - lcm_f1_5k: 0.3852 - lcm_accuracy_1k: 0.5538 - lcm_accuracy_2k: 0.7095 - lcm_accuracy_3k: 0.7875 - lcm_accuracy_5k: 0.8570 - lcm_hamming_loss_k: 0.0042
Epoch 00005: val_loss improved from 0.31340 to 0.30414, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 12s 430ms/step - loss: 0.2904 - lcm_precision_1k: 0.5538 - lcm_precision_2k: 0.4384 - lcm_precision_3k: 0.3603 - lcm_precision_5k: 0.2626 - lcm_recall_1k: 0.3441 - lcm_recall_2k: 0.5147 - lcm_recall_3k: 0.6148 - lcm_recall_5k: 0.7224 - lcm_f1_1k: 0.4244 - lcm_f1_2k: 0.4734 - lcm_f1_3k: 0.4543 - lcm_f1_5k: 0.3852 - lcm_accuracy_1k: 0.5538 - lcm_accuracy_2k: 0.7095 - lcm_accuracy_3k: 0.7875 - lcm_accuracy_5k: 0.8570 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.3041 - val_lcm_precision_1k: 0.5347 - val_lcm_precision_2k: 0.4342 - val_lcm_precision_3k: 0.3534 - val_lcm_precision_5k: 0.2563 - val_lcm_recall_1k: 0.3269 - val_lcm_recall_2k: 0.5048 - val_lcm_recall_3k: 0.5989 - val_lcm_recall_5k: 0.7004 - val_lcm_f1_1k: 0.4056 - val_lcm_f1_2k: 0.4667 - val_lcm_f1_3k: 0.4444 - val_lcm_f1_5k: 0.3752 - val_lcm_accuracy_1k: 0.5347 - val_lcm_accuracy_2k: 0.6969 - val_lcm_accuracy_3k: 0.7725 - val_lcm_accuracy_5k: 0.8386 - val_lcm_hamming_loss_k: 0.0043
Epoch 6/16
27/27 [==============================] - ETA: 0s - loss: 0.2792 - lcm_precision_1k: 0.5748 - lcm_precision_2k: 0.4560 - lcm_precision_3k: 0.3719 - lcm_precision_5k: 0.2694 - lcm_recall_1k: 0.3583 - lcm_recall_2k: 0.5369 - lcm_recall_3k: 0.6360 - lcm_recall_5k: 0.7429 - lcm_f1_1k: 0.4414 - lcm_f1_2k: 0.4931 - lcm_f1_3k: 0.4693 - lcm_f1_5k: 0.3954 - lcm_accuracy_1k: 0.5748 - lcm_accuracy_2k: 0.7322 - lcm_accuracy_3k: 0.8062 - lcm_accuracy_5k: 0.8709 - lcm_hamming_loss_k: 0.0041
Epoch 00006: val_loss improved from 0.30414 to 0.30083, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 12s 429ms/step - loss: 0.2792 - lcm_precision_1k: 0.5748 - lcm_precision_2k: 0.4560 - lcm_precision_3k: 0.3719 - lcm_precision_5k: 0.2694 - lcm_recall_1k: 0.3583 - lcm_recall_2k: 0.5369 - lcm_recall_3k: 0.6360 - lcm_recall_5k: 0.7429 - lcm_f1_1k: 0.4414 - lcm_f1_2k: 0.4931 - lcm_f1_3k: 0.4693 - lcm_f1_5k: 0.3954 - lcm_accuracy_1k: 0.5748 - lcm_accuracy_2k: 0.7322 - lcm_accuracy_3k: 0.8062 - lcm_accuracy_5k: 0.8709 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.3008 - val_lcm_precision_1k: 0.5493 - val_lcm_precision_2k: 0.4381 - val_lcm_precision_3k: 0.3588 - val_lcm_precision_5k: 0.2594 - val_lcm_recall_1k: 0.3383 - val_lcm_recall_2k: 0.5087 - val_lcm_recall_3k: 0.6101 - val_lcm_recall_5k: 0.7124 - val_lcm_f1_1k: 0.4185 - val_lcm_f1_2k: 0.4706 - val_lcm_f1_3k: 0.4517 - val_lcm_f1_5k: 0.3802 - val_lcm_accuracy_1k: 0.5493 - val_lcm_accuracy_2k: 0.7060 - val_lcm_accuracy_3k: 0.7850 - val_lcm_accuracy_5k: 0.8495 - val_lcm_hamming_loss_k: 0.0043
Epoch 7/16
27/27 [==============================] - ETA: 0s - loss: 0.2700 - lcm_precision_1k: 0.5967 - lcm_precision_2k: 0.4690 - lcm_precision_3k: 0.3803 - lcm_precision_5k: 0.2752 - lcm_recall_1k: 0.3722 - lcm_recall_2k: 0.5512 - lcm_recall_3k: 0.6497 - lcm_recall_5k: 0.7577 - lcm_f1_1k: 0.4584 - lcm_f1_2k: 0.5067 - lcm_f1_3k: 0.4797 - lcm_f1_5k: 0.4037 - lcm_accuracy_1k: 0.5967 - lcm_accuracy_2k: 0.7493 - lcm_accuracy_3k: 0.8220 - lcm_accuracy_5k: 0.8830 - lcm_hamming_loss_k: 0.0040
Epoch 00007: val_loss improved from 0.30083 to 0.29559, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 425ms/step - loss: 0.2700 - lcm_precision_1k: 0.5967 - lcm_precision_2k: 0.4690 - lcm_precision_3k: 0.3803 - lcm_precision_5k: 0.2752 - lcm_recall_1k: 0.3722 - lcm_recall_2k: 0.5512 - lcm_recall_3k: 0.6497 - lcm_recall_5k: 0.7577 - lcm_f1_1k: 0.4584 - lcm_f1_2k: 0.5067 - lcm_f1_3k: 0.4797 - lcm_f1_5k: 0.4037 - lcm_accuracy_1k: 0.5967 - lcm_accuracy_2k: 0.7493 - lcm_accuracy_3k: 0.8220 - lcm_accuracy_5k: 0.8830 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2956 - val_lcm_precision_1k: 0.5599 - val_lcm_precision_2k: 0.4442 - val_lcm_precision_3k: 0.3624 - val_lcm_precision_5k: 0.2599 - val_lcm_recall_1k: 0.3443 - val_lcm_recall_2k: 0.5181 - val_lcm_recall_3k: 0.6173 - val_lcm_recall_5k: 0.7145 - val_lcm_f1_1k: 0.4263 - val_lcm_f1_2k: 0.4782 - val_lcm_f1_3k: 0.4565 - val_lcm_f1_5k: 0.3810 - val_lcm_accuracy_1k: 0.5599 - val_lcm_accuracy_2k: 0.7175 - val_lcm_accuracy_3k: 0.7951 - val_lcm_accuracy_5k: 0.8524 - val_lcm_hamming_loss_k: 0.0042
Epoch 8/16
27/27 [==============================] - ETA: 0s - loss: 0.2618 - lcm_precision_1k: 0.6118 - lcm_precision_2k: 0.4797 - lcm_precision_3k: 0.3900 - lcm_precision_5k: 0.2821 - lcm_recall_1k: 0.3845 - lcm_recall_2k: 0.5654 - lcm_recall_3k: 0.6669 - lcm_recall_5k: 0.7773 - lcm_f1_1k: 0.4720 - lcm_f1_2k: 0.5189 - lcm_f1_3k: 0.4921 - lcm_f1_5k: 0.4139 - lcm_accuracy_1k: 0.6118 - lcm_accuracy_2k: 0.7648 - lcm_accuracy_3k: 0.8345 - lcm_accuracy_5k: 0.8988 - lcm_hamming_loss_k: 0.0039
Epoch 00008: val_loss improved from 0.29559 to 0.29080, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2618 - lcm_precision_1k: 0.6118 - lcm_precision_2k: 0.4797 - lcm_precision_3k: 0.3900 - lcm_precision_5k: 0.2821 - lcm_recall_1k: 0.3845 - lcm_recall_2k: 0.5654 - lcm_recall_3k: 0.6669 - lcm_recall_5k: 0.7773 - lcm_f1_1k: 0.4720 - lcm_f1_2k: 0.5189 - lcm_f1_3k: 0.4921 - lcm_f1_5k: 0.4139 - lcm_accuracy_1k: 0.6118 - lcm_accuracy_2k: 0.7648 - lcm_accuracy_3k: 0.8345 - lcm_accuracy_5k: 0.8988 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2908 - val_lcm_precision_1k: 0.5692 - val_lcm_precision_2k: 0.4497 - val_lcm_precision_3k: 0.3641 - val_lcm_precision_5k: 0.2618 - val_lcm_recall_1k: 0.3508 - val_lcm_recall_2k: 0.5270 - val_lcm_recall_3k: 0.6177 - val_lcm_recall_5k: 0.7192 - val_lcm_f1_1k: 0.4340 - val_lcm_f1_2k: 0.4851 - val_lcm_f1_3k: 0.4580 - val_lcm_f1_5k: 0.3837 - val_lcm_accuracy_1k: 0.5692 - val_lcm_accuracy_2k: 0.7264 - val_lcm_accuracy_3k: 0.7948 - val_lcm_accuracy_5k: 0.8563 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/16
27/27 [==============================] - ETA: 0s - loss: 0.2544 - lcm_precision_1k: 0.6259 - lcm_precision_2k: 0.4923 - lcm_precision_3k: 0.3979 - lcm_precision_5k: 0.2863 - lcm_recall_1k: 0.3938 - lcm_recall_2k: 0.5808 - lcm_recall_3k: 0.6818 - lcm_recall_5k: 0.7888 - lcm_f1_1k: 0.4834 - lcm_f1_2k: 0.5329 - lcm_f1_3k: 0.5024 - lcm_f1_5k: 0.4200 - lcm_accuracy_1k: 0.6259 - lcm_accuracy_2k: 0.7800 - lcm_accuracy_3k: 0.8490 - lcm_accuracy_5k: 0.9062 - lcm_hamming_loss_k: 0.0039
Epoch 00009: val_loss improved from 0.29080 to 0.28934, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 12s 430ms/step - loss: 0.2544 - lcm_precision_1k: 0.6259 - lcm_precision_2k: 0.4923 - lcm_precision_3k: 0.3979 - lcm_precision_5k: 0.2863 - lcm_recall_1k: 0.3938 - lcm_recall_2k: 0.5808 - lcm_recall_3k: 0.6818 - lcm_recall_5k: 0.7888 - lcm_f1_1k: 0.4834 - lcm_f1_2k: 0.5329 - lcm_f1_3k: 0.5024 - lcm_f1_5k: 0.4200 - lcm_accuracy_1k: 0.6259 - lcm_accuracy_2k: 0.7800 - lcm_accuracy_3k: 0.8490 - lcm_accuracy_5k: 0.9062 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2893 - val_lcm_precision_1k: 0.5628 - val_lcm_precision_2k: 0.4517 - val_lcm_precision_3k: 0.3666 - val_lcm_precision_5k: 0.2639 - val_lcm_recall_1k: 0.3479 - val_lcm_recall_2k: 0.5275 - val_lcm_recall_3k: 0.6234 - val_lcm_recall_5k: 0.7242 - val_lcm_f1_1k: 0.4299 - val_lcm_f1_2k: 0.4864 - val_lcm_f1_3k: 0.4615 - val_lcm_f1_5k: 0.3867 - val_lcm_accuracy_1k: 0.5628 - val_lcm_accuracy_2k: 0.7232 - val_lcm_accuracy_3k: 0.7970 - val_lcm_accuracy_5k: 0.8592 - val_lcm_hamming_loss_k: 0.0042
Epoch 10/16
27/27 [==============================] - ETA: 0s - loss: 0.2479 - lcm_precision_1k: 0.6399 - lcm_precision_2k: 0.5015 - lcm_precision_3k: 0.4051 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.4035 - lcm_recall_2k: 0.5905 - lcm_recall_3k: 0.6923 - lcm_recall_5k: 0.8000 - lcm_f1_1k: 0.4949 - lcm_f1_2k: 0.5423 - lcm_f1_3k: 0.5111 - lcm_f1_5k: 0.4260 - lcm_accuracy_1k: 0.6399 - lcm_accuracy_2k: 0.7899 - lcm_accuracy_3k: 0.8550 - lcm_accuracy_5k: 0.9150 - lcm_hamming_loss_k: 0.0038
Epoch 00010: val_loss improved from 0.28934 to 0.28683, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.2479 - lcm_precision_1k: 0.6399 - lcm_precision_2k: 0.5015 - lcm_precision_3k: 0.4051 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.4035 - lcm_recall_2k: 0.5905 - lcm_recall_3k: 0.6923 - lcm_recall_5k: 0.8000 - lcm_f1_1k: 0.4949 - lcm_f1_2k: 0.5423 - lcm_f1_3k: 0.5111 - lcm_f1_5k: 0.4260 - lcm_accuracy_1k: 0.6399 - lcm_accuracy_2k: 0.7899 - lcm_accuracy_3k: 0.8550 - lcm_accuracy_5k: 0.9150 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2868 - val_lcm_precision_1k: 0.5707 - val_lcm_precision_2k: 0.4560 - val_lcm_precision_3k: 0.3700 - val_lcm_precision_5k: 0.2649 - val_lcm_recall_1k: 0.3514 - val_lcm_recall_2k: 0.5343 - val_lcm_recall_3k: 0.6308 - val_lcm_recall_5k: 0.7273 - val_lcm_f1_1k: 0.4349 - val_lcm_f1_2k: 0.4919 - val_lcm_f1_3k: 0.4662 - val_lcm_f1_5k: 0.3882 - val_lcm_accuracy_1k: 0.5707 - val_lcm_accuracy_2k: 0.7302 - val_lcm_accuracy_3k: 0.8035 - val_lcm_accuracy_5k: 0.8603 - val_lcm_hamming_loss_k: 0.0042
Epoch 11/16
27/27 [==============================] - ETA: 0s - loss: 0.2422 - lcm_precision_1k: 0.6536 - lcm_precision_2k: 0.5127 - lcm_precision_3k: 0.4148 - lcm_precision_5k: 0.2951 - lcm_recall_1k: 0.4133 - lcm_recall_2k: 0.6039 - lcm_recall_3k: 0.7083 - lcm_recall_5k: 0.8118 - lcm_f1_1k: 0.5063 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5231 - lcm_f1_5k: 0.4328 - lcm_accuracy_1k: 0.6536 - lcm_accuracy_2k: 0.8041 - lcm_accuracy_3k: 0.8677 - lcm_accuracy_5k: 0.9213 - lcm_hamming_loss_k: 0.0037
Epoch 00011: val_loss improved from 0.28683 to 0.28464, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.2422 - lcm_precision_1k: 0.6536 - lcm_precision_2k: 0.5127 - lcm_precision_3k: 0.4148 - lcm_precision_5k: 0.2951 - lcm_recall_1k: 0.4133 - lcm_recall_2k: 0.6039 - lcm_recall_3k: 0.7083 - lcm_recall_5k: 0.8118 - lcm_f1_1k: 0.5063 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5231 - lcm_f1_5k: 0.4328 - lcm_accuracy_1k: 0.6536 - lcm_accuracy_2k: 0.8041 - lcm_accuracy_3k: 0.8677 - lcm_accuracy_5k: 0.9213 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2846 - val_lcm_precision_1k: 0.5861 - val_lcm_precision_2k: 0.4628 - val_lcm_precision_3k: 0.3722 - val_lcm_precision_5k: 0.2658 - val_lcm_recall_1k: 0.3617 - val_lcm_recall_2k: 0.5401 - val_lcm_recall_3k: 0.6337 - val_lcm_recall_5k: 0.7302 - val_lcm_f1_1k: 0.4472 - val_lcm_f1_2k: 0.4983 - val_lcm_f1_3k: 0.4688 - val_lcm_f1_5k: 0.3895 - val_lcm_accuracy_1k: 0.5861 - val_lcm_accuracy_2k: 0.7399 - val_lcm_accuracy_3k: 0.8046 - val_lcm_accuracy_5k: 0.8642 - val_lcm_hamming_loss_k: 0.0041
Epoch 12/16
27/27 [==============================] - ETA: 0s - loss: 0.2374 - lcm_precision_1k: 0.6672 - lcm_precision_2k: 0.5220 - lcm_precision_3k: 0.4214 - lcm_precision_5k: 0.2994 - lcm_recall_1k: 0.4223 - lcm_recall_2k: 0.6154 - lcm_recall_3k: 0.7200 - lcm_recall_5k: 0.8232 - lcm_f1_1k: 0.5171 - lcm_f1_2k: 0.5648 - lcm_f1_3k: 0.5316 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6672 - lcm_accuracy_2k: 0.8128 - lcm_accuracy_3k: 0.8785 - lcm_accuracy_5k: 0.9298 - lcm_hamming_loss_k: 0.0037
Epoch 00012: val_loss improved from 0.28464 to 0.28366, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 12s 430ms/step - loss: 0.2374 - lcm_precision_1k: 0.6672 - lcm_precision_2k: 0.5220 - lcm_precision_3k: 0.4214 - lcm_precision_5k: 0.2994 - lcm_recall_1k: 0.4223 - lcm_recall_2k: 0.6154 - lcm_recall_3k: 0.7200 - lcm_recall_5k: 0.8232 - lcm_f1_1k: 0.5171 - lcm_f1_2k: 0.5648 - lcm_f1_3k: 0.5316 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6672 - lcm_accuracy_2k: 0.8128 - lcm_accuracy_3k: 0.8785 - lcm_accuracy_5k: 0.9298 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2837 - val_lcm_precision_1k: 0.5807 - val_lcm_precision_2k: 0.4589 - val_lcm_precision_3k: 0.3725 - val_lcm_precision_5k: 0.2674 - val_lcm_recall_1k: 0.3590 - val_lcm_recall_2k: 0.5368 - val_lcm_recall_3k: 0.6345 - val_lcm_recall_5k: 0.7321 - val_lcm_f1_1k: 0.4436 - val_lcm_f1_2k: 0.4946 - val_lcm_f1_3k: 0.4693 - val_lcm_f1_5k: 0.3916 - val_lcm_accuracy_1k: 0.5807 - val_lcm_accuracy_2k: 0.7337 - val_lcm_accuracy_3k: 0.8076 - val_lcm_accuracy_5k: 0.8637 - val_lcm_hamming_loss_k: 0.0041
Epoch 13/16
27/27 [==============================] - ETA: 0s - loss: 0.2312 - lcm_precision_1k: 0.6714 - lcm_precision_2k: 0.5283 - lcm_precision_3k: 0.4274 - lcm_precision_5k: 0.3030 - lcm_recall_1k: 0.4264 - lcm_recall_2k: 0.6238 - lcm_recall_3k: 0.7297 - lcm_recall_5k: 0.8328 - lcm_f1_1k: 0.5215 - lcm_f1_2k: 0.5720 - lcm_f1_3k: 0.5390 - lcm_f1_5k: 0.4443 - lcm_accuracy_1k: 0.6714 - lcm_accuracy_2k: 0.8209 - lcm_accuracy_3k: 0.8843 - lcm_accuracy_5k: 0.9336 - lcm_hamming_loss_k: 0.0037
Epoch 00013: val_loss did not improve from 0.28366
27/27 [==============================] - 10s 391ms/step - loss: 0.2312 - lcm_precision_1k: 0.6714 - lcm_precision_2k: 0.5283 - lcm_precision_3k: 0.4274 - lcm_precision_5k: 0.3030 - lcm_recall_1k: 0.4264 - lcm_recall_2k: 0.6238 - lcm_recall_3k: 0.7297 - lcm_recall_5k: 0.8328 - lcm_f1_1k: 0.5215 - lcm_f1_2k: 0.5720 - lcm_f1_3k: 0.5390 - lcm_f1_5k: 0.4443 - lcm_accuracy_1k: 0.6714 - lcm_accuracy_2k: 0.8209 - lcm_accuracy_3k: 0.8843 - lcm_accuracy_5k: 0.9336 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2874 - val_lcm_precision_1k: 0.5902 - val_lcm_precision_2k: 0.4660 - val_lcm_precision_3k: 0.3746 - val_lcm_precision_5k: 0.2658 - val_lcm_recall_1k: 0.3660 - val_lcm_recall_2k: 0.5453 - val_lcm_recall_3k: 0.6367 - val_lcm_recall_5k: 0.7324 - val_lcm_f1_1k: 0.4517 - val_lcm_f1_2k: 0.5024 - val_lcm_f1_3k: 0.4715 - val_lcm_f1_5k: 0.3900 - val_lcm_accuracy_1k: 0.5902 - val_lcm_accuracy_2k: 0.7430 - val_lcm_accuracy_3k: 0.8087 - val_lcm_accuracy_5k: 0.8664 - val_lcm_hamming_loss_k: 0.0041
Epoch 14/16
27/27 [==============================] - ETA: 0s - loss: 0.2249 - lcm_precision_1k: 0.6919 - lcm_precision_2k: 0.5402 - lcm_precision_3k: 0.4340 - lcm_precision_5k: 0.3078 - lcm_recall_1k: 0.4399 - lcm_recall_2k: 0.6366 - lcm_recall_3k: 0.7402 - lcm_recall_5k: 0.8439 - lcm_f1_1k: 0.5378 - lcm_f1_2k: 0.5844 - lcm_f1_3k: 0.5471 - lcm_f1_5k: 0.4510 - lcm_accuracy_1k: 0.6919 - lcm_accuracy_2k: 0.8328 - lcm_accuracy_3k: 0.8921 - lcm_accuracy_5k: 0.9411 - lcm_hamming_loss_k: 0.0036
Epoch 00014: val_loss improved from 0.28366 to 0.28328, saving model to logs/nuhkhm-labs-0604-155944/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2249 - lcm_precision_1k: 0.6919 - lcm_precision_2k: 0.5402 - lcm_precision_3k: 0.4340 - lcm_precision_5k: 0.3078 - lcm_recall_1k: 0.4399 - lcm_recall_2k: 0.6366 - lcm_recall_3k: 0.7402 - lcm_recall_5k: 0.8439 - lcm_f1_1k: 0.5378 - lcm_f1_2k: 0.5844 - lcm_f1_3k: 0.5471 - lcm_f1_5k: 0.4510 - lcm_accuracy_1k: 0.6919 - lcm_accuracy_2k: 0.8328 - lcm_accuracy_3k: 0.8921 - lcm_accuracy_5k: 0.9411 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2833 - val_lcm_precision_1k: 0.5877 - val_lcm_precision_2k: 0.4658 - val_lcm_precision_3k: 0.3742 - val_lcm_precision_5k: 0.2661 - val_lcm_recall_1k: 0.3624 - val_lcm_recall_2k: 0.5473 - val_lcm_recall_3k: 0.6401 - val_lcm_recall_5k: 0.7316 - val_lcm_f1_1k: 0.4482 - val_lcm_f1_2k: 0.5031 - val_lcm_f1_3k: 0.4721 - val_lcm_f1_5k: 0.3901 - val_lcm_accuracy_1k: 0.5877 - val_lcm_accuracy_2k: 0.7463 - val_lcm_accuracy_3k: 0.8141 - val_lcm_accuracy_5k: 0.8628 - val_lcm_hamming_loss_k: 0.0041
Epoch 15/16
27/27 [==============================] - ETA: 0s - loss: 0.2185 - lcm_precision_1k: 0.7054 - lcm_precision_2k: 0.5522 - lcm_precision_3k: 0.4430 - lcm_precision_5k: 0.3130 - lcm_recall_1k: 0.4476 - lcm_recall_2k: 0.6504 - lcm_recall_3k: 0.7534 - lcm_recall_5k: 0.8556 - lcm_f1_1k: 0.5476 - lcm_f1_2k: 0.5972 - lcm_f1_3k: 0.5579 - lcm_f1_5k: 0.4583 - lcm_accuracy_1k: 0.7054 - lcm_accuracy_2k: 0.8461 - lcm_accuracy_3k: 0.9016 - lcm_accuracy_5k: 0.9469 - lcm_hamming_loss_k: 0.0035
Epoch 00015: val_loss did not improve from 0.28328
27/27 [==============================] - 10s 389ms/step - loss: 0.2185 - lcm_precision_1k: 0.7054 - lcm_precision_2k: 0.5522 - lcm_precision_3k: 0.4430 - lcm_precision_5k: 0.3130 - lcm_recall_1k: 0.4476 - lcm_recall_2k: 0.6504 - lcm_recall_3k: 0.7534 - lcm_recall_5k: 0.8556 - lcm_f1_1k: 0.5476 - lcm_f1_2k: 0.5972 - lcm_f1_3k: 0.5579 - lcm_f1_5k: 0.4583 - lcm_accuracy_1k: 0.7054 - lcm_accuracy_2k: 0.8461 - lcm_accuracy_3k: 0.9016 - lcm_accuracy_5k: 0.9469 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2860 - val_lcm_precision_1k: 0.5814 - val_lcm_precision_2k: 0.4626 - val_lcm_precision_3k: 0.3751 - val_lcm_precision_5k: 0.2654 - val_lcm_recall_1k: 0.3593 - val_lcm_recall_2k: 0.5405 - val_lcm_recall_3k: 0.6394 - val_lcm_recall_5k: 0.7297 - val_lcm_f1_1k: 0.4440 - val_lcm_f1_2k: 0.4984 - val_lcm_f1_3k: 0.4726 - val_lcm_f1_5k: 0.3891 - val_lcm_accuracy_1k: 0.5814 - val_lcm_accuracy_2k: 0.7365 - val_lcm_accuracy_3k: 0.8120 - val_lcm_accuracy_5k: 0.8619 - val_lcm_hamming_loss_k: 0.0041
Epoch 16/16
27/27 [==============================] - ETA: 0s - loss: 0.2138 - lcm_precision_1k: 0.7139 - lcm_precision_2k: 0.5613 - lcm_precision_3k: 0.4507 - lcm_precision_5k: 0.3158 - lcm_recall_1k: 0.4554 - lcm_recall_2k: 0.6615 - lcm_recall_3k: 0.7662 - lcm_recall_5k: 0.8635 - lcm_f1_1k: 0.5561 - lcm_f1_2k: 0.6073 - lcm_f1_3k: 0.5675 - lcm_f1_5k: 0.4624 - lcm_accuracy_1k: 0.7139 - lcm_accuracy_2k: 0.8544 - lcm_accuracy_3k: 0.9094 - lcm_accuracy_5k: 0.9518 - lcm_hamming_loss_k: 0.0035
Epoch 00016: val_loss did not improve from 0.28328
27/27 [==============================] - 10s 389ms/step - loss: 0.2138 - lcm_precision_1k: 0.7139 - lcm_precision_2k: 0.5613 - lcm_precision_3k: 0.4507 - lcm_precision_5k: 0.3158 - lcm_recall_1k: 0.4554 - lcm_recall_2k: 0.6615 - lcm_recall_3k: 0.7662 - lcm_recall_5k: 0.8635 - lcm_f1_1k: 0.5561 - lcm_f1_2k: 0.6073 - lcm_f1_3k: 0.5675 - lcm_f1_5k: 0.4624 - lcm_accuracy_1k: 0.7139 - lcm_accuracy_2k: 0.8544 - lcm_accuracy_3k: 0.9094 - lcm_accuracy_5k: 0.9518 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2845 - val_lcm_precision_1k: 0.5817 - val_lcm_precision_2k: 0.4610 - val_lcm_precision_3k: 0.3723 - val_lcm_precision_5k: 0.2659 - val_lcm_recall_1k: 0.3605 - val_lcm_recall_2k: 0.5383 - val_lcm_recall_3k: 0.6316 - val_lcm_recall_5k: 0.7295 - val_lcm_f1_1k: 0.4450 - val_lcm_f1_2k: 0.4965 - val_lcm_f1_3k: 0.4683 - val_lcm_f1_5k: 0.3896 - val_lcm_accuracy_1k: 0.5817 - val_lcm_accuracy_2k: 0.7357 - val_lcm_accuracy_3k: 0.8024 - val_lcm_accuracy_5k: 0.8621 - val_lcm_hamming_loss_k: 0.0041
Epoch 00016: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2438 - lcm_precision_1k: 0.6563 - lcm_precision_2k: 0.5184 - lcm_precision_3k: 0.4162 - lcm_precision_5k: 0.2929 - lcm_recall_1k: 0.4146 - lcm_recall_2k: 0.6142 - lcm_recall_3k: 0.7137 - lcm_recall_5k: 0.8065 - lcm_f1_1k: 0.5068 - lcm_f1_2k: 0.5609 - lcm_f1_3k: 0.5247 - lcm_f1_5k: 0.4289 - lcm_accuracy_1k: 0.6563 - lcm_accuracy_2k: 0.8061 - lcm_accuracy_3k: 0.8679 - lcm_accuracy_5k: 0.9108 - lcm_hamming_loss_k: 0.0037
Best model result:  [0.24381349980831146, 0.6562572717666626, 0.5183631181716919, 0.4162061810493469, 0.29286134243011475, 0.41455402970314026, 0.6141789555549622, 0.7136789560317993, 0.8065295815467834, 0.5067789554595947, 0.5608782172203064, 0.5246820449829102, 0.42893174290657043, 0.6562572717666626, 0.806134819984436, 0.8679265975952148, 0.9107629656791687, 0.0037118762265890837]
13498
3375
5625
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_6 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_10 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_6[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_10[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_11 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_4 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_11[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_4[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_6 (Dense)                (None, 1024)         2098176     ['tf.concat_2[0][0]']            
                                                                                                  
 dense_7 (Dense)                (None, 15)           15375       ['dense_6[0][0]']                
                                                                                                  
 tf.nn.softmax_2 (TFOpLambda)   (None, 15)           0           ['dense_7[0][0]']                
                                                                                                  
 tf.expand_dims_4 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_2[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_4[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_7 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_12 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_7[0][0]']              
                                                                                                  
 dense_8 (Dense)                (None, 15, 150)      22650       ['lambda_12[0][0]']              
                                                                                                  
 tf.math.reduce_mean_5 (TFOpLam  (None, 150)         0           ['dense_8[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_5 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_5[0][0]']  
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_2 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_5[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 permute_8 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_2[0][0]']     
                                                                                                  
 lambda_13 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_8[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_13[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_14 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_2[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_14[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_5"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_6 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_10 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_6[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_10[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_11 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_4 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_11[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_4[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_6 (Dense)                (None, 1024)         2098176     ['tf.concat_2[0][0]']            
                                                                                                  
 dense_7 (Dense)                (None, 15)           15375       ['dense_6[0][0]']                
                                                                                                  
 tf.nn.softmax_2 (TFOpLambda)   (None, 15)           0           ['dense_7[0][0]']                
                                                                                                  
 tf.expand_dims_4 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_2[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_4[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_7 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_12 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_7[0][0]']              
                                                                                                  
 dense_8 (Dense)                (None, 15, 150)      22650       ['lambda_12[0][0]']              
                                                                                                  
 tf.math.reduce_mean_5 (TFOpLam  (None, 150)         0           ['dense_8[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_5 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_5[0][0]']  
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_2 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_5[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 permute_8 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_2[0][0]']     
                                                                                                  
 lambda_13 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_8[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_13[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_14 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_2[0][0]']     
                                                                                                  
 tf.__operators__.getitem_8 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_14[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_8[0][0
                                                                 ]']                              
                                                                                                  
 dot_2 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_2[0][0]']                  
                                                                                                  
 concatenate_2 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/16
27/27 [==============================] - ETA: 0s - loss: 0.4928 - lcm_precision_1k: 0.1786 - lcm_precision_2k: 0.1474 - lcm_precision_3k: 0.1306 - lcm_precision_5k: 0.1091 - lcm_recall_1k: 0.0993 - lcm_recall_2k: 0.1608 - lcm_recall_3k: 0.2110 - lcm_recall_5k: 0.2886 - lcm_f1_1k: 0.1275 - lcm_f1_2k: 0.1537 - lcm_f1_3k: 0.1613 - lcm_f1_5k: 0.1583 - lcm_accuracy_1k: 0.1786 - lcm_accuracy_2k: 0.2644 - lcm_accuracy_3k: 0.3267 - lcm_accuracy_5k: 0.4206 - lcm_hamming_loss_k: 0.0060
Epoch 00001: val_loss improved from inf to 0.43521, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 13s 424ms/step - loss: 0.4928 - lcm_precision_1k: 0.1786 - lcm_precision_2k: 0.1474 - lcm_precision_3k: 0.1306 - lcm_precision_5k: 0.1091 - lcm_recall_1k: 0.0993 - lcm_recall_2k: 0.1608 - lcm_recall_3k: 0.2110 - lcm_recall_5k: 0.2886 - lcm_f1_1k: 0.1275 - lcm_f1_2k: 0.1537 - lcm_f1_3k: 0.1613 - lcm_f1_5k: 0.1583 - lcm_accuracy_1k: 0.1786 - lcm_accuracy_2k: 0.2644 - lcm_accuracy_3k: 0.3267 - lcm_accuracy_5k: 0.4206 - lcm_hamming_loss_k: 0.0060 - val_loss: 0.4352 - val_lcm_precision_1k: 0.3137 - val_lcm_precision_2k: 0.2494 - val_lcm_precision_3k: 0.2125 - val_lcm_precision_5k: 0.1683 - val_lcm_recall_1k: 0.1819 - val_lcm_recall_2k: 0.2841 - val_lcm_recall_3k: 0.3557 - val_lcm_recall_5k: 0.4525 - val_lcm_f1_1k: 0.2300 - val_lcm_f1_2k: 0.2655 - val_lcm_f1_3k: 0.2659 - val_lcm_f1_5k: 0.2452 - val_lcm_accuracy_1k: 0.3137 - val_lcm_accuracy_2k: 0.4351 - val_lcm_accuracy_3k: 0.5148 - val_lcm_accuracy_5k: 0.6050 - val_lcm_hamming_loss_k: 0.0054
Epoch 2/16
27/27 [==============================] - ETA: 0s - loss: 0.3802 - lcm_precision_1k: 0.3764 - lcm_precision_2k: 0.3102 - lcm_precision_3k: 0.2622 - lcm_precision_5k: 0.2029 - lcm_recall_1k: 0.2250 - lcm_recall_2k: 0.3546 - lcm_recall_3k: 0.4385 - lcm_recall_5k: 0.5528 - lcm_f1_1k: 0.2815 - lcm_f1_2k: 0.3308 - lcm_f1_3k: 0.3281 - lcm_f1_5k: 0.2968 - lcm_accuracy_1k: 0.3764 - lcm_accuracy_2k: 0.5265 - lcm_accuracy_3k: 0.6097 - lcm_accuracy_5k: 0.7047 - lcm_hamming_loss_k: 0.0050
Epoch 00002: val_loss improved from 0.43521 to 0.36102, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.3802 - lcm_precision_1k: 0.3764 - lcm_precision_2k: 0.3102 - lcm_precision_3k: 0.2622 - lcm_precision_5k: 0.2029 - lcm_recall_1k: 0.2250 - lcm_recall_2k: 0.3546 - lcm_recall_3k: 0.4385 - lcm_recall_5k: 0.5528 - lcm_f1_1k: 0.2815 - lcm_f1_2k: 0.3308 - lcm_f1_3k: 0.3281 - lcm_f1_5k: 0.2968 - lcm_accuracy_1k: 0.3764 - lcm_accuracy_2k: 0.5265 - lcm_accuracy_3k: 0.6097 - lcm_accuracy_5k: 0.7047 - lcm_hamming_loss_k: 0.0050 - val_loss: 0.3610 - val_lcm_precision_1k: 0.4400 - val_lcm_precision_2k: 0.3507 - val_lcm_precision_3k: 0.2899 - val_lcm_precision_5k: 0.2221 - val_lcm_recall_1k: 0.2661 - val_lcm_recall_2k: 0.4000 - val_lcm_recall_3k: 0.4849 - val_lcm_recall_5k: 0.6023 - val_lcm_f1_1k: 0.3314 - val_lcm_f1_2k: 0.3735 - val_lcm_f1_3k: 0.3627 - val_lcm_f1_5k: 0.3244 - val_lcm_accuracy_1k: 0.4400 - val_lcm_accuracy_2k: 0.5796 - val_lcm_accuracy_3k: 0.6581 - val_lcm_accuracy_5k: 0.7512 - val_lcm_hamming_loss_k: 0.0048
Epoch 3/16
27/27 [==============================] - ETA: 0s - loss: 0.3288 - lcm_precision_1k: 0.4734 - lcm_precision_2k: 0.3845 - lcm_precision_3k: 0.3179 - lcm_precision_5k: 0.2385 - lcm_recall_1k: 0.2878 - lcm_recall_2k: 0.4443 - lcm_recall_3k: 0.5381 - lcm_recall_5k: 0.6534 - lcm_f1_1k: 0.3579 - lcm_f1_2k: 0.4122 - lcm_f1_3k: 0.3996 - lcm_f1_5k: 0.3494 - lcm_accuracy_1k: 0.4734 - lcm_accuracy_2k: 0.6335 - lcm_accuracy_3k: 0.7142 - lcm_accuracy_5k: 0.7989 - lcm_hamming_loss_k: 0.0046
Epoch 00003: val_loss improved from 0.36102 to 0.33230, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 422ms/step - loss: 0.3288 - lcm_precision_1k: 0.4734 - lcm_precision_2k: 0.3845 - lcm_precision_3k: 0.3179 - lcm_precision_5k: 0.2385 - lcm_recall_1k: 0.2878 - lcm_recall_2k: 0.4443 - lcm_recall_3k: 0.5381 - lcm_recall_5k: 0.6534 - lcm_f1_1k: 0.3579 - lcm_f1_2k: 0.4122 - lcm_f1_3k: 0.3996 - lcm_f1_5k: 0.3494 - lcm_accuracy_1k: 0.4734 - lcm_accuracy_2k: 0.6335 - lcm_accuracy_3k: 0.7142 - lcm_accuracy_5k: 0.7989 - lcm_hamming_loss_k: 0.0046 - val_loss: 0.3323 - val_lcm_precision_1k: 0.4901 - val_lcm_precision_2k: 0.3912 - val_lcm_precision_3k: 0.3206 - val_lcm_precision_5k: 0.2381 - val_lcm_recall_1k: 0.2988 - val_lcm_recall_2k: 0.4521 - val_lcm_recall_3k: 0.5433 - val_lcm_recall_5k: 0.6509 - val_lcm_f1_1k: 0.3710 - val_lcm_f1_2k: 0.4192 - val_lcm_f1_3k: 0.4030 - val_lcm_f1_5k: 0.3485 - val_lcm_accuracy_1k: 0.4901 - val_lcm_accuracy_2k: 0.6416 - val_lcm_accuracy_3k: 0.7191 - val_lcm_accuracy_5k: 0.7981 - val_lcm_hamming_loss_k: 0.0045
Epoch 4/16
27/27 [==============================] - ETA: 0s - loss: 0.3063 - lcm_precision_1k: 0.5198 - lcm_precision_2k: 0.4174 - lcm_precision_3k: 0.3456 - lcm_precision_5k: 0.2526 - lcm_recall_1k: 0.3213 - lcm_recall_2k: 0.4862 - lcm_recall_3k: 0.5883 - lcm_recall_5k: 0.6954 - lcm_f1_1k: 0.3970 - lcm_f1_2k: 0.4491 - lcm_f1_3k: 0.4353 - lcm_f1_5k: 0.3706 - lcm_accuracy_1k: 0.5198 - lcm_accuracy_2k: 0.6779 - lcm_accuracy_3k: 0.7618 - lcm_accuracy_5k: 0.8335 - lcm_hamming_loss_k: 0.0044 ETA: 0s - loss: 0.3074 - lcm_precision_1k: 0.5199 - lcm_precision_2k: 0.4164 - lcm_precision_3k: 0.3445 - lcm_precision_5k: 0.2519 - lcm_recall_1k: 0.3206 - lcm_recall_2k: 0.4851 - lcm_recall_3k: 0.5861 - lcm_recall_5k: 0.6924 - lcm_f1_1k: 0.3966 - lcm_f1_2k: 0.4481 - lcm_f1_3k: 0.4339 - lcm_f1_5k: 0.3694 - lcm_accuracy_1k: 0.5199 - lcm_accuracy_2k: 0.6767 - lcm_accuracy_3k: 0.7602 - lcm_accuracy_5k: 0.8313 - lcm_hamming_loss_k: 
Epoch 00004: val_loss improved from 0.33230 to 0.32701, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.3063 - lcm_precision_1k: 0.5198 - lcm_precision_2k: 0.4174 - lcm_precision_3k: 0.3456 - lcm_precision_5k: 0.2526 - lcm_recall_1k: 0.3213 - lcm_recall_2k: 0.4862 - lcm_recall_3k: 0.5883 - lcm_recall_5k: 0.6954 - lcm_f1_1k: 0.3970 - lcm_f1_2k: 0.4491 - lcm_f1_3k: 0.4353 - lcm_f1_5k: 0.3706 - lcm_accuracy_1k: 0.5198 - lcm_accuracy_2k: 0.6779 - lcm_accuracy_3k: 0.7618 - lcm_accuracy_5k: 0.8335 - lcm_hamming_loss_k: 0.0044 - val_loss: 0.3270 - val_lcm_precision_1k: 0.5174 - val_lcm_precision_2k: 0.4024 - val_lcm_precision_3k: 0.3355 - val_lcm_precision_5k: 0.2469 - val_lcm_recall_1k: 0.3195 - val_lcm_recall_2k: 0.4677 - val_lcm_recall_3k: 0.5666 - val_lcm_recall_5k: 0.6763 - val_lcm_f1_1k: 0.3948 - val_lcm_f1_2k: 0.4324 - val_lcm_f1_3k: 0.4213 - val_lcm_f1_5k: 0.3615 - val_lcm_accuracy_1k: 0.5174 - val_lcm_accuracy_2k: 0.6603 - val_lcm_accuracy_3k: 0.7393 - val_lcm_accuracy_5k: 0.8249 - val_lcm_hamming_loss_k: 0.0044
Epoch 5/16
27/27 [==============================] - ETA: 0s - loss: 0.2910 - lcm_precision_1k: 0.5491 - lcm_precision_2k: 0.4402 - lcm_precision_3k: 0.3609 - lcm_precision_5k: 0.2627 - lcm_recall_1k: 0.3393 - lcm_recall_2k: 0.5151 - lcm_recall_3k: 0.6154 - lcm_recall_5k: 0.7237 - lcm_f1_1k: 0.4194 - lcm_f1_2k: 0.4746 - lcm_f1_3k: 0.4549 - lcm_f1_5k: 0.3855 - lcm_accuracy_1k: 0.5491 - lcm_accuracy_2k: 0.7129 - lcm_accuracy_3k: 0.7898 - lcm_accuracy_5k: 0.8560 - lcm_hamming_loss_k: 0.0042
Epoch 00005: val_loss improved from 0.32701 to 0.30920, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 425ms/step - loss: 0.2910 - lcm_precision_1k: 0.5491 - lcm_precision_2k: 0.4402 - lcm_precision_3k: 0.3609 - lcm_precision_5k: 0.2627 - lcm_recall_1k: 0.3393 - lcm_recall_2k: 0.5151 - lcm_recall_3k: 0.6154 - lcm_recall_5k: 0.7237 - lcm_f1_1k: 0.4194 - lcm_f1_2k: 0.4746 - lcm_f1_3k: 0.4549 - lcm_f1_5k: 0.3855 - lcm_accuracy_1k: 0.5491 - lcm_accuracy_2k: 0.7129 - lcm_accuracy_3k: 0.7898 - lcm_accuracy_5k: 0.8560 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.3092 - val_lcm_precision_1k: 0.5287 - val_lcm_precision_2k: 0.4179 - val_lcm_precision_3k: 0.3438 - val_lcm_precision_5k: 0.2520 - val_lcm_recall_1k: 0.3258 - val_lcm_recall_2k: 0.4854 - val_lcm_recall_3k: 0.5822 - val_lcm_recall_5k: 0.6881 - val_lcm_f1_1k: 0.4030 - val_lcm_f1_2k: 0.4489 - val_lcm_f1_3k: 0.4321 - val_lcm_f1_5k: 0.3687 - val_lcm_accuracy_1k: 0.5287 - val_lcm_accuracy_2k: 0.6780 - val_lcm_accuracy_3k: 0.7521 - val_lcm_accuracy_5k: 0.8306 - val_lcm_hamming_loss_k: 0.0044
Epoch 6/16
27/27 [==============================] - ETA: 0s - loss: 0.2786 - lcm_precision_1k: 0.5771 - lcm_precision_2k: 0.4583 - lcm_precision_3k: 0.3728 - lcm_precision_5k: 0.2692 - lcm_recall_1k: 0.3607 - lcm_recall_2k: 0.5383 - lcm_recall_3k: 0.6363 - lcm_recall_5k: 0.7414 - lcm_f1_1k: 0.4439 - lcm_f1_2k: 0.4951 - lcm_f1_3k: 0.4701 - lcm_f1_5k: 0.3950 - lcm_accuracy_1k: 0.5771 - lcm_accuracy_2k: 0.7358 - lcm_accuracy_3k: 0.8062 - lcm_accuracy_5k: 0.8702 - lcm_hamming_loss_k: 0.0041
Epoch 00006: val_loss improved from 0.30920 to 0.30508, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 421ms/step - loss: 0.2786 - lcm_precision_1k: 0.5771 - lcm_precision_2k: 0.4583 - lcm_precision_3k: 0.3728 - lcm_precision_5k: 0.2692 - lcm_recall_1k: 0.3607 - lcm_recall_2k: 0.5383 - lcm_recall_3k: 0.6363 - lcm_recall_5k: 0.7414 - lcm_f1_1k: 0.4439 - lcm_f1_2k: 0.4951 - lcm_f1_3k: 0.4701 - lcm_f1_5k: 0.3950 - lcm_accuracy_1k: 0.5771 - lcm_accuracy_2k: 0.7358 - lcm_accuracy_3k: 0.8062 - lcm_accuracy_5k: 0.8702 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.3051 - val_lcm_precision_1k: 0.5431 - val_lcm_precision_2k: 0.4301 - val_lcm_precision_3k: 0.3500 - val_lcm_precision_5k: 0.2546 - val_lcm_recall_1k: 0.3328 - val_lcm_recall_2k: 0.4993 - val_lcm_recall_3k: 0.5934 - val_lcm_recall_5k: 0.6961 - val_lcm_f1_1k: 0.4125 - val_lcm_f1_2k: 0.4619 - val_lcm_f1_3k: 0.4400 - val_lcm_f1_5k: 0.3726 - val_lcm_accuracy_1k: 0.5431 - val_lcm_accuracy_2k: 0.6926 - val_lcm_accuracy_3k: 0.7665 - val_lcm_accuracy_5k: 0.8343 - val_lcm_hamming_loss_k: 0.0043
Epoch 7/16
27/27 [==============================] - ETA: 0s - loss: 0.2685 - lcm_precision_1k: 0.5982 - lcm_precision_2k: 0.4711 - lcm_precision_3k: 0.3828 - lcm_precision_5k: 0.2754 - lcm_recall_1k: 0.3753 - lcm_recall_2k: 0.5545 - lcm_recall_3k: 0.6540 - lcm_recall_5k: 0.7592 - lcm_f1_1k: 0.4611 - lcm_f1_2k: 0.5094 - lcm_f1_3k: 0.4829 - lcm_f1_5k: 0.4041 - lcm_accuracy_1k: 0.5982 - lcm_accuracy_2k: 0.7508 - lcm_accuracy_3k: 0.8214 - lcm_accuracy_5k: 0.8829 - lcm_hamming_loss_k: 0.0040
Epoch 00007: val_loss improved from 0.30508 to 0.29764, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 424ms/step - loss: 0.2685 - lcm_precision_1k: 0.5982 - lcm_precision_2k: 0.4711 - lcm_precision_3k: 0.3828 - lcm_precision_5k: 0.2754 - lcm_recall_1k: 0.3753 - lcm_recall_2k: 0.5545 - lcm_recall_3k: 0.6540 - lcm_recall_5k: 0.7592 - lcm_f1_1k: 0.4611 - lcm_f1_2k: 0.5094 - lcm_f1_3k: 0.4829 - lcm_f1_5k: 0.4041 - lcm_accuracy_1k: 0.5982 - lcm_accuracy_2k: 0.7508 - lcm_accuracy_3k: 0.8214 - lcm_accuracy_5k: 0.8829 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2976 - val_lcm_precision_1k: 0.5524 - val_lcm_precision_2k: 0.4327 - val_lcm_precision_3k: 0.3534 - val_lcm_precision_5k: 0.2582 - val_lcm_recall_1k: 0.3404 - val_lcm_recall_2k: 0.5035 - val_lcm_recall_3k: 0.5998 - val_lcm_recall_5k: 0.7052 - val_lcm_f1_1k: 0.4210 - val_lcm_f1_2k: 0.4652 - val_lcm_f1_3k: 0.4446 - val_lcm_f1_5k: 0.3778 - val_lcm_accuracy_1k: 0.5524 - val_lcm_accuracy_2k: 0.6997 - val_lcm_accuracy_3k: 0.7719 - val_lcm_accuracy_5k: 0.8432 - val_lcm_hamming_loss_k: 0.0043
Epoch 8/16
27/27 [==============================] - ETA: 0s - loss: 0.2616 - lcm_precision_1k: 0.6120 - lcm_precision_2k: 0.4829 - lcm_precision_3k: 0.3911 - lcm_precision_5k: 0.2811 - lcm_recall_1k: 0.3847 - lcm_recall_2k: 0.5681 - lcm_recall_3k: 0.6678 - lcm_recall_5k: 0.7737 - lcm_f1_1k: 0.4724 - lcm_f1_2k: 0.5220 - lcm_f1_3k: 0.4933 - lcm_f1_5k: 0.4124 - lcm_accuracy_1k: 0.6120 - lcm_accuracy_2k: 0.7674 - lcm_accuracy_3k: 0.8351 - lcm_accuracy_5k: 0.8930 - lcm_hamming_loss_k: 0.0039
Epoch 00008: val_loss improved from 0.29764 to 0.29489, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2616 - lcm_precision_1k: 0.6120 - lcm_precision_2k: 0.4829 - lcm_precision_3k: 0.3911 - lcm_precision_5k: 0.2811 - lcm_recall_1k: 0.3847 - lcm_recall_2k: 0.5681 - lcm_recall_3k: 0.6678 - lcm_recall_5k: 0.7737 - lcm_f1_1k: 0.4724 - lcm_f1_2k: 0.5220 - lcm_f1_3k: 0.4933 - lcm_f1_5k: 0.4124 - lcm_accuracy_1k: 0.6120 - lcm_accuracy_2k: 0.7674 - lcm_accuracy_3k: 0.8351 - lcm_accuracy_5k: 0.8930 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2949 - val_lcm_precision_1k: 0.5598 - val_lcm_precision_2k: 0.4389 - val_lcm_precision_3k: 0.3552 - val_lcm_precision_5k: 0.2584 - val_lcm_recall_1k: 0.3441 - val_lcm_recall_2k: 0.5101 - val_lcm_recall_3k: 0.6004 - val_lcm_recall_5k: 0.7063 - val_lcm_f1_1k: 0.4260 - val_lcm_f1_2k: 0.4716 - val_lcm_f1_3k: 0.4462 - val_lcm_f1_5k: 0.3783 - val_lcm_accuracy_1k: 0.5598 - val_lcm_accuracy_2k: 0.7048 - val_lcm_accuracy_3k: 0.7706 - val_lcm_accuracy_5k: 0.8449 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/16
27/27 [==============================] - ETA: 0s - loss: 0.2548 - lcm_precision_1k: 0.6263 - lcm_precision_2k: 0.4926 - lcm_precision_3k: 0.3986 - lcm_precision_5k: 0.2853 - lcm_recall_1k: 0.3938 - lcm_recall_2k: 0.5810 - lcm_recall_3k: 0.6810 - lcm_recall_5k: 0.7862 - lcm_f1_1k: 0.4835 - lcm_f1_2k: 0.5331 - lcm_f1_3k: 0.5028 - lcm_f1_5k: 0.4186 - lcm_accuracy_1k: 0.6263 - lcm_accuracy_2k: 0.7781 - lcm_accuracy_3k: 0.8448 - lcm_accuracy_5k: 0.9027 - lcm_hamming_loss_k: 0.0039
Epoch 00009: val_loss did not improve from 0.29489
27/27 [==============================] - 11s 392ms/step - loss: 0.2548 - lcm_precision_1k: 0.6263 - lcm_precision_2k: 0.4926 - lcm_precision_3k: 0.3986 - lcm_precision_5k: 0.2853 - lcm_recall_1k: 0.3938 - lcm_recall_2k: 0.5810 - lcm_recall_3k: 0.6810 - lcm_recall_5k: 0.7862 - lcm_f1_1k: 0.4835 - lcm_f1_2k: 0.5331 - lcm_f1_3k: 0.5028 - lcm_f1_5k: 0.4186 - lcm_accuracy_1k: 0.6263 - lcm_accuracy_2k: 0.7781 - lcm_accuracy_3k: 0.8448 - lcm_accuracy_5k: 0.9027 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.3000 - val_lcm_precision_1k: 0.5711 - val_lcm_precision_2k: 0.4409 - val_lcm_precision_3k: 0.3575 - val_lcm_precision_5k: 0.2602 - val_lcm_recall_1k: 0.3531 - val_lcm_recall_2k: 0.5114 - val_lcm_recall_3k: 0.6078 - val_lcm_recall_5k: 0.7109 - val_lcm_f1_1k: 0.4361 - val_lcm_f1_2k: 0.4733 - val_lcm_f1_3k: 0.4499 - val_lcm_f1_5k: 0.3808 - val_lcm_accuracy_1k: 0.5711 - val_lcm_accuracy_2k: 0.7094 - val_lcm_accuracy_3k: 0.7814 - val_lcm_accuracy_5k: 0.8509 - val_lcm_hamming_loss_k: 0.0042
Epoch 10/16
27/27 [==============================] - ETA: 0s - loss: 0.2484 - lcm_precision_1k: 0.6417 - lcm_precision_2k: 0.5017 - lcm_precision_3k: 0.4061 - lcm_precision_5k: 0.2889 - lcm_recall_1k: 0.4043 - lcm_recall_2k: 0.5917 - lcm_recall_3k: 0.6938 - lcm_recall_5k: 0.7950 - lcm_f1_1k: 0.4960 - lcm_f1_2k: 0.5429 - lcm_f1_3k: 0.5123 - lcm_f1_5k: 0.4238 - lcm_accuracy_1k: 0.6417 - lcm_accuracy_2k: 0.7889 - lcm_accuracy_3k: 0.8545 - lcm_accuracy_5k: 0.9099 - lcm_hamming_loss_k: 0.0038
Epoch 00010: val_loss improved from 0.29489 to 0.28931, saving model to logs/znlyry-labs-0604-160256/model/checkpoint_labs.h5
27/27 [==============================] - 11s 424ms/step - loss: 0.2484 - lcm_precision_1k: 0.6417 - lcm_precision_2k: 0.5017 - lcm_precision_3k: 0.4061 - lcm_precision_5k: 0.2889 - lcm_recall_1k: 0.4043 - lcm_recall_2k: 0.5917 - lcm_recall_3k: 0.6938 - lcm_recall_5k: 0.7950 - lcm_f1_1k: 0.4960 - lcm_f1_2k: 0.5429 - lcm_f1_3k: 0.5123 - lcm_f1_5k: 0.4238 - lcm_accuracy_1k: 0.6417 - lcm_accuracy_2k: 0.7889 - lcm_accuracy_3k: 0.8545 - lcm_accuracy_5k: 0.9099 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2893 - val_lcm_precision_1k: 0.5726 - val_lcm_precision_2k: 0.4450 - val_lcm_precision_3k: 0.3623 - val_lcm_precision_5k: 0.2610 - val_lcm_recall_1k: 0.3542 - val_lcm_recall_2k: 0.5183 - val_lcm_recall_3k: 0.6156 - val_lcm_recall_5k: 0.7117 - val_lcm_f1_1k: 0.4374 - val_lcm_f1_2k: 0.4787 - val_lcm_f1_3k: 0.4559 - val_lcm_f1_5k: 0.3818 - val_lcm_accuracy_1k: 0.5726 - val_lcm_accuracy_2k: 0.7153 - val_lcm_accuracy_3k: 0.7876 - val_lcm_accuracy_5k: 0.8481 - val_lcm_hamming_loss_k: 0.0042
Epoch 11/16
27/27 [==============================] - ETA: 0s - loss: 0.2411 - lcm_precision_1k: 0.6535 - lcm_precision_2k: 0.5126 - lcm_precision_3k: 0.4145 - lcm_precision_5k: 0.2946 - lcm_recall_1k: 0.4122 - lcm_recall_2k: 0.6040 - lcm_recall_3k: 0.7077 - lcm_recall_5k: 0.8110 - lcm_f1_1k: 0.5054 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5227 - lcm_f1_5k: 0.4321 - lcm_accuracy_1k: 0.6535 - lcm_accuracy_2k: 0.7994 - lcm_accuracy_3k: 0.8676 - lcm_accuracy_5k: 0.9208 - lcm_hamming_loss_k: 0.0037
Epoch 00011: val_loss did not improve from 0.28931
27/27 [==============================] - 10s 387ms/step - loss: 0.2411 - lcm_precision_1k: 0.6535 - lcm_precision_2k: 0.5126 - lcm_precision_3k: 0.4145 - lcm_precision_5k: 0.2946 - lcm_recall_1k: 0.4122 - lcm_recall_2k: 0.6040 - lcm_recall_3k: 0.7077 - lcm_recall_5k: 0.8110 - lcm_f1_1k: 0.5054 - lcm_f1_2k: 0.5545 - lcm_f1_3k: 0.5227 - lcm_f1_5k: 0.4321 - lcm_accuracy_1k: 0.6535 - lcm_accuracy_2k: 0.7994 - lcm_accuracy_3k: 0.8676 - lcm_accuracy_5k: 0.9208 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2913 - val_lcm_precision_1k: 0.5769 - val_lcm_precision_2k: 0.4487 - val_lcm_precision_3k: 0.3658 - val_lcm_precision_5k: 0.2635 - val_lcm_recall_1k: 0.3571 - val_lcm_recall_2k: 0.5195 - val_lcm_recall_3k: 0.6193 - val_lcm_recall_5k: 0.7165 - val_lcm_f1_1k: 0.4410 - val_lcm_f1_2k: 0.4813 - val_lcm_f1_3k: 0.4598 - val_lcm_f1_5k: 0.3851 - val_lcm_accuracy_1k: 0.5769 - val_lcm_accuracy_2k: 0.7159 - val_lcm_accuracy_3k: 0.7899 - val_lcm_accuracy_5k: 0.8536 - val_lcm_hamming_loss_k: 0.0041
Epoch 12/16
27/27 [==============================] - ETA: 0s - loss: 0.2352 - lcm_precision_1k: 0.6666 - lcm_precision_2k: 0.5228 - lcm_precision_3k: 0.4217 - lcm_precision_5k: 0.2991 - lcm_recall_1k: 0.4231 - lcm_recall_2k: 0.6175 - lcm_recall_3k: 0.7197 - lcm_recall_5k: 0.8228 - lcm_f1_1k: 0.5176 - lcm_f1_2k: 0.5662 - lcm_f1_3k: 0.5317 - lcm_f1_5k: 0.4387 - lcm_accuracy_1k: 0.6666 - lcm_accuracy_2k: 0.8156 - lcm_accuracy_3k: 0.8760 - lcm_accuracy_5k: 0.9292 - lcm_hamming_loss_k: 0.0037
Epoch 00012: val_loss did not improve from 0.28931
27/27 [==============================] - 11s 393ms/step - loss: 0.2352 - lcm_precision_1k: 0.6666 - lcm_precision_2k: 0.5228 - lcm_precision_3k: 0.4217 - lcm_precision_5k: 0.2991 - lcm_recall_1k: 0.4231 - lcm_recall_2k: 0.6175 - lcm_recall_3k: 0.7197 - lcm_recall_5k: 0.8228 - lcm_f1_1k: 0.5176 - lcm_f1_2k: 0.5662 - lcm_f1_3k: 0.5317 - lcm_f1_5k: 0.4387 - lcm_accuracy_1k: 0.6666 - lcm_accuracy_2k: 0.8156 - lcm_accuracy_3k: 0.8760 - lcm_accuracy_5k: 0.9292 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2916 - val_lcm_precision_1k: 0.5781 - val_lcm_precision_2k: 0.4547 - val_lcm_precision_3k: 0.3666 - val_lcm_precision_5k: 0.2640 - val_lcm_recall_1k: 0.3571 - val_lcm_recall_2k: 0.5277 - val_lcm_recall_3k: 0.6226 - val_lcm_recall_5k: 0.7208 - val_lcm_f1_1k: 0.4412 - val_lcm_f1_2k: 0.4883 - val_lcm_f1_3k: 0.4613 - val_lcm_f1_5k: 0.3863 - val_lcm_accuracy_1k: 0.5781 - val_lcm_accuracy_2k: 0.7230 - val_lcm_accuracy_3k: 0.7957 - val_lcm_accuracy_5k: 0.8546 - val_lcm_hamming_loss_k: 0.0041
Epoch 00012: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2547 - lcm_precision_1k: 0.6333 - lcm_precision_2k: 0.4938 - lcm_precision_3k: 0.3971 - lcm_precision_5k: 0.2836 - lcm_recall_1k: 0.3988 - lcm_recall_2k: 0.5827 - lcm_recall_3k: 0.6801 - lcm_recall_5k: 0.7836 - lcm_f1_1k: 0.4881 - lcm_f1_2k: 0.5333 - lcm_f1_3k: 0.5004 - lcm_f1_5k: 0.4158 - lcm_accuracy_1k: 0.6333 - lcm_accuracy_2k: 0.7763 - lcm_accuracy_3k: 0.8408 - lcm_accuracy_5k: 0.8969 - lcm_hamming_loss_k: 0.0038
Best model result:  [0.25472307205200195, 0.6332526803016663, 0.4937584698200226, 0.3971249759197235, 0.2836090326309204, 0.3988352119922638, 0.5827046036720276, 0.6800766587257385, 0.7836320400238037, 0.4880787134170532, 0.5332728028297424, 0.5004100203514099, 0.41578465700149536, 0.6332526803016663, 0.7763351798057556, 0.8408381938934326, 0.896868109703064, 0.0038196241948753595]
13499
3374
5625
Model: "model_6"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 permute_9 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_15 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_9[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_15[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_16 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_6 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_16[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_3 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_6[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_9 (Dense)                (None, 1024)         2098176     ['tf.concat_3[0][0]']            
                                                                                                  
 dense_10 (Dense)               (None, 15)           15375       ['dense_9[0][0]']                
                                                                                                  
 tf.nn.softmax_3 (TFOpLambda)   (None, 15)           0           ['dense_10[0][0]']               
                                                                                                  
 tf.expand_dims_6 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_3[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_6[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_10 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_17 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_10[0][0]']             
                                                                                                  
 dense_11 (Dense)               (None, 15, 150)      22650       ['lambda_17[0][0]']              
                                                                                                  
 tf.math.reduce_mean_7 (TFOpLam  (None, 150)         0           ['dense_11[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_7 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_7[0][0]']  
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_3 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_7[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_10[0][
                                                                 0]']                             
                                                                                                  
 permute_11 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_3[0][0]']     
                                                                                                  
 lambda_18 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_11[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_18[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_19 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_3[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_19[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_7"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 permute_9 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_15 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_9[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_15[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_16 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_6 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_16[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_3 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_6[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_9 (Dense)                (None, 1024)         2098176     ['tf.concat_3[0][0]']            
                                                                                                  
 dense_10 (Dense)               (None, 15)           15375       ['dense_9[0][0]']                
                                                                                                  
 tf.nn.softmax_3 (TFOpLambda)   (None, 15)           0           ['dense_10[0][0]']               
                                                                                                  
 tf.expand_dims_6 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_3[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_6[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_10 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_17 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_10[0][0]']             
                                                                                                  
 dense_11 (Dense)               (None, 15, 150)      22650       ['lambda_17[0][0]']              
                                                                                                  
 tf.math.reduce_mean_7 (TFOpLam  (None, 150)         0           ['dense_11[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_7 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_7[0][0]']  
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_3 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_7[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_10[0][
                                                                 0]']                             
                                                                                                  
 permute_11 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_3[0][0]']     
                                                                                                  
 lambda_18 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_11[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_18[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_19 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_3[0][0]']     
                                                                                                  
 tf.__operators__.getitem_11 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_19[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_11[0][
                                                                 0]']                             
                                                                                                  
 dot_3 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_3[0][0]']                  
                                                                                                  
 concatenate_3 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/16
27/27 [==============================] - ETA: 0s - loss: 0.4948 - lcm_precision_1k: 0.1947 - lcm_precision_2k: 0.1616 - lcm_precision_3k: 0.1418 - lcm_precision_5k: 0.1144 - lcm_recall_1k: 0.1064 - lcm_recall_2k: 0.1760 - lcm_recall_3k: 0.2284 - lcm_recall_5k: 0.3011 - lcm_f1_1k: 0.1375 - lcm_f1_2k: 0.1684 - lcm_f1_3k: 0.1749 - lcm_f1_5k: 0.1658 - lcm_accuracy_1k: 0.1947 - lcm_accuracy_2k: 0.2895 - lcm_accuracy_3k: 0.3541 - lcm_accuracy_5k: 0.4351 - lcm_hamming_loss_k: 0.0059
Epoch 00001: val_loss improved from inf to 0.42412, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 13s 414ms/step - loss: 0.4948 - lcm_precision_1k: 0.1947 - lcm_precision_2k: 0.1616 - lcm_precision_3k: 0.1418 - lcm_precision_5k: 0.1144 - lcm_recall_1k: 0.1064 - lcm_recall_2k: 0.1760 - lcm_recall_3k: 0.2284 - lcm_recall_5k: 0.3011 - lcm_f1_1k: 0.1375 - lcm_f1_2k: 0.1684 - lcm_f1_3k: 0.1749 - lcm_f1_5k: 0.1658 - lcm_accuracy_1k: 0.1947 - lcm_accuracy_2k: 0.2895 - lcm_accuracy_3k: 0.3541 - lcm_accuracy_5k: 0.4351 - lcm_hamming_loss_k: 0.0059 - val_loss: 0.4241 - val_lcm_precision_1k: 0.3066 - val_lcm_precision_2k: 0.2495 - val_lcm_precision_3k: 0.2221 - val_lcm_precision_5k: 0.1755 - val_lcm_recall_1k: 0.1760 - val_lcm_recall_2k: 0.2808 - val_lcm_recall_3k: 0.3688 - val_lcm_recall_5k: 0.4728 - val_lcm_f1_1k: 0.2233 - val_lcm_f1_2k: 0.2639 - val_lcm_f1_3k: 0.2770 - val_lcm_f1_5k: 0.2558 - val_lcm_accuracy_1k: 0.3066 - val_lcm_accuracy_2k: 0.4383 - val_lcm_accuracy_3k: 0.5303 - val_lcm_accuracy_5k: 0.6237 - val_lcm_hamming_loss_k: 0.0054
Epoch 2/16
27/27 [==============================] - ETA: 0s - loss: 0.3786 - lcm_precision_1k: 0.3839 - lcm_precision_2k: 0.3097 - lcm_precision_3k: 0.2650 - lcm_precision_5k: 0.2033 - lcm_recall_1k: 0.2282 - lcm_recall_2k: 0.3527 - lcm_recall_3k: 0.4438 - lcm_recall_5k: 0.5528 - lcm_f1_1k: 0.2862 - lcm_f1_2k: 0.3297 - lcm_f1_3k: 0.3318 - lcm_f1_5k: 0.2972 - lcm_accuracy_1k: 0.3839 - lcm_accuracy_2k: 0.5268 - lcm_accuracy_3k: 0.6157 - lcm_accuracy_5k: 0.7046 - lcm_hamming_loss_k: 0.0050
Epoch 00002: val_loss improved from 0.42412 to 0.35478, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.3786 - lcm_precision_1k: 0.3839 - lcm_precision_2k: 0.3097 - lcm_precision_3k: 0.2650 - lcm_precision_5k: 0.2033 - lcm_recall_1k: 0.2282 - lcm_recall_2k: 0.3527 - lcm_recall_3k: 0.4438 - lcm_recall_5k: 0.5528 - lcm_f1_1k: 0.2862 - lcm_f1_2k: 0.3297 - lcm_f1_3k: 0.3318 - lcm_f1_5k: 0.2972 - lcm_accuracy_1k: 0.3839 - lcm_accuracy_2k: 0.5268 - lcm_accuracy_3k: 0.6157 - lcm_accuracy_5k: 0.7046 - lcm_hamming_loss_k: 0.0050 - val_loss: 0.3548 - val_lcm_precision_1k: 0.4249 - val_lcm_precision_2k: 0.3454 - val_lcm_precision_3k: 0.2924 - val_lcm_precision_5k: 0.2199 - val_lcm_recall_1k: 0.2576 - val_lcm_recall_2k: 0.4001 - val_lcm_recall_3k: 0.4982 - val_lcm_recall_5k: 0.6038 - val_lcm_f1_1k: 0.3205 - val_lcm_f1_2k: 0.3705 - val_lcm_f1_3k: 0.3683 - val_lcm_f1_5k: 0.3223 - val_lcm_accuracy_1k: 0.4249 - val_lcm_accuracy_2k: 0.5786 - val_lcm_accuracy_3k: 0.6731 - val_lcm_accuracy_5k: 0.7573 - val_lcm_hamming_loss_k: 0.0048
Epoch 3/16
27/27 [==============================] - ETA: 0s - loss: 0.3283 - lcm_precision_1k: 0.4756 - lcm_precision_2k: 0.3832 - lcm_precision_3k: 0.3185 - lcm_precision_5k: 0.2375 - lcm_recall_1k: 0.2902 - lcm_recall_2k: 0.4435 - lcm_recall_3k: 0.5419 - lcm_recall_5k: 0.6531 - lcm_f1_1k: 0.3604 - lcm_f1_2k: 0.4111 - lcm_f1_3k: 0.4012 - lcm_f1_5k: 0.3483 - lcm_accuracy_1k: 0.4756 - lcm_accuracy_2k: 0.6317 - lcm_accuracy_3k: 0.7165 - lcm_accuracy_5k: 0.8000 - lcm_hamming_loss_k: 0.0046
Epoch 00003: val_loss improved from 0.35478 to 0.33079, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.3283 - lcm_precision_1k: 0.4756 - lcm_precision_2k: 0.3832 - lcm_precision_3k: 0.3185 - lcm_precision_5k: 0.2375 - lcm_recall_1k: 0.2902 - lcm_recall_2k: 0.4435 - lcm_recall_3k: 0.5419 - lcm_recall_5k: 0.6531 - lcm_f1_1k: 0.3604 - lcm_f1_2k: 0.4111 - lcm_f1_3k: 0.4012 - lcm_f1_5k: 0.3483 - lcm_accuracy_1k: 0.4756 - lcm_accuracy_2k: 0.6317 - lcm_accuracy_3k: 0.7165 - lcm_accuracy_5k: 0.8000 - lcm_hamming_loss_k: 0.0046 - val_loss: 0.3308 - val_lcm_precision_1k: 0.4741 - val_lcm_precision_2k: 0.3852 - val_lcm_precision_3k: 0.3194 - val_lcm_precision_5k: 0.2377 - val_lcm_recall_1k: 0.2909 - val_lcm_recall_2k: 0.4474 - val_lcm_recall_3k: 0.5407 - val_lcm_recall_5k: 0.6504 - val_lcm_f1_1k: 0.3603 - val_lcm_f1_2k: 0.4137 - val_lcm_f1_3k: 0.4013 - val_lcm_f1_5k: 0.3480 - val_lcm_accuracy_1k: 0.4741 - val_lcm_accuracy_2k: 0.6397 - val_lcm_accuracy_3k: 0.7251 - val_lcm_accuracy_5k: 0.7993 - val_lcm_hamming_loss_k: 0.0046
Epoch 4/16
27/27 [==============================] - ETA: 0s - loss: 0.3066 - lcm_precision_1k: 0.5196 - lcm_precision_2k: 0.4192 - lcm_precision_3k: 0.3443 - lcm_precision_5k: 0.2525 - lcm_recall_1k: 0.3200 - lcm_recall_2k: 0.4902 - lcm_recall_3k: 0.5877 - lcm_recall_5k: 0.6952 - lcm_f1_1k: 0.3960 - lcm_f1_2k: 0.4519 - lcm_f1_3k: 0.4341 - lcm_f1_5k: 0.3704 - lcm_accuracy_1k: 0.5196 - lcm_accuracy_2k: 0.6845 - lcm_accuracy_3k: 0.7632 - lcm_accuracy_5k: 0.8362 - lcm_hamming_loss_k: 0.0044
Epoch 00004: val_loss improved from 0.33079 to 0.32255, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.3066 - lcm_precision_1k: 0.5196 - lcm_precision_2k: 0.4192 - lcm_precision_3k: 0.3443 - lcm_precision_5k: 0.2525 - lcm_recall_1k: 0.3200 - lcm_recall_2k: 0.4902 - lcm_recall_3k: 0.5877 - lcm_recall_5k: 0.6952 - lcm_f1_1k: 0.3960 - lcm_f1_2k: 0.4519 - lcm_f1_3k: 0.4341 - lcm_f1_5k: 0.3704 - lcm_accuracy_1k: 0.5196 - lcm_accuracy_2k: 0.6845 - lcm_accuracy_3k: 0.7632 - lcm_accuracy_5k: 0.8362 - lcm_hamming_loss_k: 0.0044 - val_loss: 0.3225 - val_lcm_precision_1k: 0.4925 - val_lcm_precision_2k: 0.4005 - val_lcm_precision_3k: 0.3306 - val_lcm_precision_5k: 0.2449 - val_lcm_recall_1k: 0.3040 - val_lcm_recall_2k: 0.4655 - val_lcm_recall_3k: 0.5597 - val_lcm_recall_5k: 0.6701 - val_lcm_f1_1k: 0.3757 - val_lcm_f1_2k: 0.4303 - val_lcm_f1_3k: 0.4155 - val_lcm_f1_5k: 0.3586 - val_lcm_accuracy_1k: 0.4925 - val_lcm_accuracy_2k: 0.6558 - val_lcm_accuracy_3k: 0.7359 - val_lcm_accuracy_5k: 0.8099 - val_lcm_hamming_loss_k: 0.0045
Epoch 5/16
27/27 [==============================] - ETA: 0s - loss: 0.2914 - lcm_precision_1k: 0.5503 - lcm_precision_2k: 0.4423 - lcm_precision_3k: 0.3628 - lcm_precision_5k: 0.2621 - lcm_recall_1k: 0.3406 - lcm_recall_2k: 0.5184 - lcm_recall_3k: 0.6196 - lcm_recall_5k: 0.7222 - lcm_f1_1k: 0.4206 - lcm_f1_2k: 0.4773 - lcm_f1_3k: 0.4576 - lcm_f1_5k: 0.3846 - lcm_accuracy_1k: 0.5503 - lcm_accuracy_2k: 0.7132 - lcm_accuracy_3k: 0.7918 - lcm_accuracy_5k: 0.8566 - lcm_hamming_loss_k: 0.0042
Epoch 00005: val_loss improved from 0.32255 to 0.30749, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 424ms/step - loss: 0.2914 - lcm_precision_1k: 0.5503 - lcm_precision_2k: 0.4423 - lcm_precision_3k: 0.3628 - lcm_precision_5k: 0.2621 - lcm_recall_1k: 0.3406 - lcm_recall_2k: 0.5184 - lcm_recall_3k: 0.6196 - lcm_recall_5k: 0.7222 - lcm_f1_1k: 0.4206 - lcm_f1_2k: 0.4773 - lcm_f1_3k: 0.4576 - lcm_f1_5k: 0.3846 - lcm_accuracy_1k: 0.5503 - lcm_accuracy_2k: 0.7132 - lcm_accuracy_3k: 0.7918 - lcm_accuracy_5k: 0.8566 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.3075 - val_lcm_precision_1k: 0.5153 - val_lcm_precision_2k: 0.4142 - val_lcm_precision_3k: 0.3396 - val_lcm_precision_5k: 0.2495 - val_lcm_recall_1k: 0.3190 - val_lcm_recall_2k: 0.4829 - val_lcm_recall_3k: 0.5753 - val_lcm_recall_5k: 0.6831 - val_lcm_f1_1k: 0.3938 - val_lcm_f1_2k: 0.4456 - val_lcm_f1_3k: 0.4269 - val_lcm_f1_5k: 0.3653 - val_lcm_accuracy_1k: 0.5153 - val_lcm_accuracy_2k: 0.6763 - val_lcm_accuracy_3k: 0.7522 - val_lcm_accuracy_5k: 0.8210 - val_lcm_hamming_loss_k: 0.0044
Epoch 6/16
27/27 [==============================] - ETA: 0s - loss: 0.2795 - lcm_precision_1k: 0.5782 - lcm_precision_2k: 0.4572 - lcm_precision_3k: 0.3731 - lcm_precision_5k: 0.2693 - lcm_recall_1k: 0.3589 - lcm_recall_2k: 0.5353 - lcm_recall_3k: 0.6362 - lcm_recall_5k: 0.7405 - lcm_f1_1k: 0.4428 - lcm_f1_2k: 0.4931 - lcm_f1_3k: 0.4703 - lcm_f1_5k: 0.3949 - lcm_accuracy_1k: 0.5782 - lcm_accuracy_2k: 0.7316 - lcm_accuracy_3k: 0.8067 - lcm_accuracy_5k: 0.8715 - lcm_hamming_loss_k: 0.0041
Epoch 00006: val_loss improved from 0.30749 to 0.30351, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2795 - lcm_precision_1k: 0.5782 - lcm_precision_2k: 0.4572 - lcm_precision_3k: 0.3731 - lcm_precision_5k: 0.2693 - lcm_recall_1k: 0.3589 - lcm_recall_2k: 0.5353 - lcm_recall_3k: 0.6362 - lcm_recall_5k: 0.7405 - lcm_f1_1k: 0.4428 - lcm_f1_2k: 0.4931 - lcm_f1_3k: 0.4703 - lcm_f1_5k: 0.3949 - lcm_accuracy_1k: 0.5782 - lcm_accuracy_2k: 0.7316 - lcm_accuracy_3k: 0.8067 - lcm_accuracy_5k: 0.8715 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.3035 - val_lcm_precision_1k: 0.5367 - val_lcm_precision_2k: 0.4223 - val_lcm_precision_3k: 0.3441 - val_lcm_precision_5k: 0.2509 - val_lcm_recall_1k: 0.3329 - val_lcm_recall_2k: 0.4928 - val_lcm_recall_3k: 0.5847 - val_lcm_recall_5k: 0.6836 - val_lcm_f1_1k: 0.4106 - val_lcm_f1_2k: 0.4546 - val_lcm_f1_3k: 0.4330 - val_lcm_f1_5k: 0.3669 - val_lcm_accuracy_1k: 0.5367 - val_lcm_accuracy_2k: 0.6842 - val_lcm_accuracy_3k: 0.7592 - val_lcm_accuracy_5k: 0.8227 - val_lcm_hamming_loss_k: 0.0043
Epoch 7/16
27/27 [==============================] - ETA: 0s - loss: 0.2708 - lcm_precision_1k: 0.6000 - lcm_precision_2k: 0.4685 - lcm_precision_3k: 0.3819 - lcm_precision_5k: 0.2751 - lcm_recall_1k: 0.3755 - lcm_recall_2k: 0.5515 - lcm_recall_3k: 0.6531 - lcm_recall_5k: 0.7585 - lcm_f1_1k: 0.4618 - lcm_f1_2k: 0.5066 - lcm_f1_3k: 0.4819 - lcm_f1_5k: 0.4037 - lcm_accuracy_1k: 0.6000 - lcm_accuracy_2k: 0.7502 - lcm_accuracy_3k: 0.8234 - lcm_accuracy_5k: 0.8854 - lcm_hamming_loss_k: 0.0040
Epoch 00007: val_loss improved from 0.30351 to 0.29817, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2708 - lcm_precision_1k: 0.6000 - lcm_precision_2k: 0.4685 - lcm_precision_3k: 0.3819 - lcm_precision_5k: 0.2751 - lcm_recall_1k: 0.3755 - lcm_recall_2k: 0.5515 - lcm_recall_3k: 0.6531 - lcm_recall_5k: 0.7585 - lcm_f1_1k: 0.4618 - lcm_f1_2k: 0.5066 - lcm_f1_3k: 0.4819 - lcm_f1_5k: 0.4037 - lcm_accuracy_1k: 0.6000 - lcm_accuracy_2k: 0.7502 - lcm_accuracy_3k: 0.8234 - lcm_accuracy_5k: 0.8854 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2982 - val_lcm_precision_1k: 0.5390 - val_lcm_precision_2k: 0.4306 - val_lcm_precision_3k: 0.3514 - val_lcm_precision_5k: 0.2553 - val_lcm_recall_1k: 0.3378 - val_lcm_recall_2k: 0.5060 - val_lcm_recall_3k: 0.5999 - val_lcm_recall_5k: 0.7013 - val_lcm_f1_1k: 0.4150 - val_lcm_f1_2k: 0.4650 - val_lcm_f1_3k: 0.4430 - val_lcm_f1_5k: 0.3742 - val_lcm_accuracy_1k: 0.5390 - val_lcm_accuracy_2k: 0.7004 - val_lcm_accuracy_3k: 0.7732 - val_lcm_accuracy_5k: 0.8356 - val_lcm_hamming_loss_k: 0.0043
Epoch 8/16
27/27 [==============================] - ETA: 0s - loss: 0.2628 - lcm_precision_1k: 0.6138 - lcm_precision_2k: 0.4820 - lcm_precision_3k: 0.3900 - lcm_precision_5k: 0.2797 - lcm_recall_1k: 0.3853 - lcm_recall_2k: 0.5667 - lcm_recall_3k: 0.6658 - lcm_recall_5k: 0.7703 - lcm_f1_1k: 0.4733 - lcm_f1_2k: 0.5208 - lcm_f1_3k: 0.4918 - lcm_f1_5k: 0.4103 - lcm_accuracy_1k: 0.6138 - lcm_accuracy_2k: 0.7652 - lcm_accuracy_3k: 0.8344 - lcm_accuracy_5k: 0.8941 - lcm_hamming_loss_k: 0.0039
Epoch 00008: val_loss improved from 0.29817 to 0.29328, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 423ms/step - loss: 0.2628 - lcm_precision_1k: 0.6138 - lcm_precision_2k: 0.4820 - lcm_precision_3k: 0.3900 - lcm_precision_5k: 0.2797 - lcm_recall_1k: 0.3853 - lcm_recall_2k: 0.5667 - lcm_recall_3k: 0.6658 - lcm_recall_5k: 0.7703 - lcm_f1_1k: 0.4733 - lcm_f1_2k: 0.5208 - lcm_f1_3k: 0.4918 - lcm_f1_5k: 0.4103 - lcm_accuracy_1k: 0.6138 - lcm_accuracy_2k: 0.7652 - lcm_accuracy_3k: 0.8344 - lcm_accuracy_5k: 0.8941 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2933 - val_lcm_precision_1k: 0.5539 - val_lcm_precision_2k: 0.4339 - val_lcm_precision_3k: 0.3534 - val_lcm_precision_5k: 0.2582 - val_lcm_recall_1k: 0.3455 - val_lcm_recall_2k: 0.5051 - val_lcm_recall_3k: 0.6018 - val_lcm_recall_5k: 0.7064 - val_lcm_f1_1k: 0.4253 - val_lcm_f1_2k: 0.4665 - val_lcm_f1_3k: 0.4451 - val_lcm_f1_5k: 0.3781 - val_lcm_accuracy_1k: 0.5539 - val_lcm_accuracy_2k: 0.7002 - val_lcm_accuracy_3k: 0.7789 - val_lcm_accuracy_5k: 0.8433 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/16
27/27 [==============================] - ETA: 0s - loss: 0.2552 - lcm_precision_1k: 0.6287 - lcm_precision_2k: 0.4912 - lcm_precision_3k: 0.3981 - lcm_precision_5k: 0.2859 - lcm_recall_1k: 0.3949 - lcm_recall_2k: 0.5778 - lcm_recall_3k: 0.6797 - lcm_recall_5k: 0.7861 - lcm_f1_1k: 0.4851 - lcm_f1_2k: 0.5309 - lcm_f1_3k: 0.5020 - lcm_f1_5k: 0.4193 - lcm_accuracy_1k: 0.6287 - lcm_accuracy_2k: 0.7768 - lcm_accuracy_3k: 0.8456 - lcm_accuracy_5k: 0.9055 - lcm_hamming_loss_k: 0.0039 ETA: 6s - loss: 0.2539 - lcm_precision_1k: 0.6296 - lcm_precision_2k: 0.4883 - lcm_precision_3k: 0.3987 - lcm_precision_5k: 0.2888 - lcm_recall_1k: 0.3930 - lcm_recall_2k: 0.5745 - lcm_recall_3k: 0.6821 - lcm_recall_5k: 0.7936 - lcm_f1_1k: 0.4839 - lcm_f1_2k: 0.5278 - lcm_f1_3k: 0.5031 - lcm_f1_5k: 0.4234 - lcm_accuracy_1k: 0.6296 - lcm_accuracy_2k: 0.7731 - lcm_accuracy_3k: 0.8480 - lcm_accura
Epoch 00009: val_loss improved from 0.29328 to 0.29076, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2552 - lcm_precision_1k: 0.6287 - lcm_precision_2k: 0.4912 - lcm_precision_3k: 0.3981 - lcm_precision_5k: 0.2859 - lcm_recall_1k: 0.3949 - lcm_recall_2k: 0.5778 - lcm_recall_3k: 0.6797 - lcm_recall_5k: 0.7861 - lcm_f1_1k: 0.4851 - lcm_f1_2k: 0.5309 - lcm_f1_3k: 0.5020 - lcm_f1_5k: 0.4193 - lcm_accuracy_1k: 0.6287 - lcm_accuracy_2k: 0.7768 - lcm_accuracy_3k: 0.8456 - lcm_accuracy_5k: 0.9055 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2908 - val_lcm_precision_1k: 0.5511 - val_lcm_precision_2k: 0.4393 - val_lcm_precision_3k: 0.3538 - val_lcm_precision_5k: 0.2588 - val_lcm_recall_1k: 0.3435 - val_lcm_recall_2k: 0.5139 - val_lcm_recall_3k: 0.6027 - val_lcm_recall_5k: 0.7080 - val_lcm_f1_1k: 0.4229 - val_lcm_f1_2k: 0.4734 - val_lcm_f1_3k: 0.4456 - val_lcm_f1_5k: 0.3789 - val_lcm_accuracy_1k: 0.5511 - val_lcm_accuracy_2k: 0.7072 - val_lcm_accuracy_3k: 0.7752 - val_lcm_accuracy_5k: 0.8415 - val_lcm_hamming_loss_k: 0.0042
Epoch 10/16
27/27 [==============================] - ETA: 0s - loss: 0.2489 - lcm_precision_1k: 0.6412 - lcm_precision_2k: 0.5035 - lcm_precision_3k: 0.4068 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.4030 - lcm_recall_2k: 0.5926 - lcm_recall_3k: 0.6947 - lcm_recall_5k: 0.7984 - lcm_f1_1k: 0.4948 - lcm_f1_2k: 0.5444 - lcm_f1_3k: 0.5130 - lcm_f1_5k: 0.4259 - lcm_accuracy_1k: 0.6412 - lcm_accuracy_2k: 0.7892 - lcm_accuracy_3k: 0.8579 - lcm_accuracy_5k: 0.9130 - lcm_hamming_loss_k: 0.0038
Epoch 00010: val_loss did not improve from 0.29076
27/27 [==============================] - 11s 391ms/step - loss: 0.2489 - lcm_precision_1k: 0.6412 - lcm_precision_2k: 0.5035 - lcm_precision_3k: 0.4068 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.4030 - lcm_recall_2k: 0.5926 - lcm_recall_3k: 0.6947 - lcm_recall_5k: 0.7984 - lcm_f1_1k: 0.4948 - lcm_f1_2k: 0.5444 - lcm_f1_3k: 0.5130 - lcm_f1_5k: 0.4259 - lcm_accuracy_1k: 0.6412 - lcm_accuracy_2k: 0.7892 - lcm_accuracy_3k: 0.8579 - lcm_accuracy_5k: 0.9130 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2908 - val_lcm_precision_1k: 0.5670 - val_lcm_precision_2k: 0.4409 - val_lcm_precision_3k: 0.3560 - val_lcm_precision_5k: 0.2581 - val_lcm_recall_1k: 0.3544 - val_lcm_recall_2k: 0.5152 - val_lcm_recall_3k: 0.6070 - val_lcm_recall_5k: 0.7078 - val_lcm_f1_1k: 0.4358 - val_lcm_f1_2k: 0.4749 - val_lcm_f1_3k: 0.4486 - val_lcm_f1_5k: 0.3781 - val_lcm_accuracy_1k: 0.5670 - val_lcm_accuracy_2k: 0.7122 - val_lcm_accuracy_3k: 0.7788 - val_lcm_accuracy_5k: 0.8412 - val_lcm_hamming_loss_k: 0.0041
Epoch 11/16
27/27 [==============================] - ETA: 0s - loss: 0.2427 - lcm_precision_1k: 0.6553 - lcm_precision_2k: 0.5127 - lcm_precision_3k: 0.4127 - lcm_precision_5k: 0.2940 - lcm_recall_1k: 0.4119 - lcm_recall_2k: 0.6032 - lcm_recall_3k: 0.7054 - lcm_recall_5k: 0.8088 - lcm_f1_1k: 0.5057 - lcm_f1_2k: 0.5542 - lcm_f1_3k: 0.5206 - lcm_f1_5k: 0.4312 - lcm_accuracy_1k: 0.6553 - lcm_accuracy_2k: 0.8028 - lcm_accuracy_3k: 0.8665 - lcm_accuracy_5k: 0.9221 - lcm_hamming_loss_k: 0.0037
Epoch 00011: val_loss improved from 0.29076 to 0.28518, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.2427 - lcm_precision_1k: 0.6553 - lcm_precision_2k: 0.5127 - lcm_precision_3k: 0.4127 - lcm_precision_5k: 0.2940 - lcm_recall_1k: 0.4119 - lcm_recall_2k: 0.6032 - lcm_recall_3k: 0.7054 - lcm_recall_5k: 0.8088 - lcm_f1_1k: 0.5057 - lcm_f1_2k: 0.5542 - lcm_f1_3k: 0.5206 - lcm_f1_5k: 0.4312 - lcm_accuracy_1k: 0.6553 - lcm_accuracy_2k: 0.8028 - lcm_accuracy_3k: 0.8665 - lcm_accuracy_5k: 0.9221 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2852 - val_lcm_precision_1k: 0.5707 - val_lcm_precision_2k: 0.4472 - val_lcm_precision_3k: 0.3600 - val_lcm_precision_5k: 0.2616 - val_lcm_recall_1k: 0.3553 - val_lcm_recall_2k: 0.5233 - val_lcm_recall_3k: 0.6125 - val_lcm_recall_5k: 0.7156 - val_lcm_f1_1k: 0.4377 - val_lcm_f1_2k: 0.4820 - val_lcm_f1_3k: 0.4532 - val_lcm_f1_5k: 0.3830 - val_lcm_accuracy_1k: 0.5707 - val_lcm_accuracy_2k: 0.7187 - val_lcm_accuracy_3k: 0.7831 - val_lcm_accuracy_5k: 0.8465 - val_lcm_hamming_loss_k: 0.0041
Epoch 12/16
27/27 [==============================] - ETA: 0s - loss: 0.2362 - lcm_precision_1k: 0.6648 - lcm_precision_2k: 0.5249 - lcm_precision_3k: 0.4220 - lcm_precision_5k: 0.2997 - lcm_recall_1k: 0.4178 - lcm_recall_2k: 0.6152 - lcm_recall_3k: 0.7185 - lcm_recall_5k: 0.8217 - lcm_f1_1k: 0.5131 - lcm_f1_2k: 0.5664 - lcm_f1_3k: 0.5317 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6648 - lcm_accuracy_2k: 0.8128 - lcm_accuracy_3k: 0.8770 - lcm_accuracy_5k: 0.9281 - lcm_hamming_loss_k: 0.0037 ETA: 1s - loss: 0.2358 - lcm_precision_1k: 0.6693 - lcm_precision_2k: 0.5267 - lcm_precision_3k: 0.4218 - lcm_precision_5k: 0.2995 - lcm_recall_1k: 0.4218 - lcm_recall_2k: 0.6195 - lcm_recall_3k: 0.7202 - lcm_recall_5k: 0.8230 - lcm_f1_1k: 0.5174 - lcm_f1_2k: 0.5693 - lcm_f1_3k: 0.5320 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6693 - lcm_accuracy_2k: 0.8162 - lcm_accuracy_3k: 0.8787 - lcm_accuracy_5k: 0.9297 - lcm_hamming_loss_k
Epoch 00012: val_loss did not improve from 0.28518
27/27 [==============================] - 10s 390ms/step - loss: 0.2362 - lcm_precision_1k: 0.6648 - lcm_precision_2k: 0.5249 - lcm_precision_3k: 0.4220 - lcm_precision_5k: 0.2997 - lcm_recall_1k: 0.4178 - lcm_recall_2k: 0.6152 - lcm_recall_3k: 0.7185 - lcm_recall_5k: 0.8217 - lcm_f1_1k: 0.5131 - lcm_f1_2k: 0.5664 - lcm_f1_3k: 0.5317 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6648 - lcm_accuracy_2k: 0.8128 - lcm_accuracy_3k: 0.8770 - lcm_accuracy_5k: 0.9281 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2864 - val_lcm_precision_1k: 0.5665 - val_lcm_precision_2k: 0.4430 - val_lcm_precision_3k: 0.3606 - val_lcm_precision_5k: 0.2608 - val_lcm_recall_1k: 0.3528 - val_lcm_recall_2k: 0.5198 - val_lcm_recall_3k: 0.6139 - val_lcm_recall_5k: 0.7160 - val_lcm_f1_1k: 0.4346 - val_lcm_f1_2k: 0.4781 - val_lcm_f1_3k: 0.4541 - val_lcm_f1_5k: 0.3822 - val_lcm_accuracy_1k: 0.5665 - val_lcm_accuracy_2k: 0.7158 - val_lcm_accuracy_3k: 0.7864 - val_lcm_accuracy_5k: 0.8479 - val_lcm_hamming_loss_k: 0.0042
Epoch 13/16
27/27 [==============================] - ETA: 0s - loss: 0.2311 - lcm_precision_1k: 0.6804 - lcm_precision_2k: 0.5295 - lcm_precision_3k: 0.4275 - lcm_precision_5k: 0.3034 - lcm_recall_1k: 0.4291 - lcm_recall_2k: 0.6229 - lcm_recall_3k: 0.7276 - lcm_recall_5k: 0.8312 - lcm_f1_1k: 0.5262 - lcm_f1_2k: 0.5724 - lcm_f1_3k: 0.5385 - lcm_f1_5k: 0.4445 - lcm_accuracy_1k: 0.6804 - lcm_accuracy_2k: 0.8213 - lcm_accuracy_3k: 0.8839 - lcm_accuracy_5k: 0.9350 - lcm_hamming_loss_k: 0.0036
Epoch 00013: val_loss improved from 0.28518 to 0.28211, saving model to logs/hwuvey-labs-0604-160521/model/checkpoint_labs.h5
27/27 [==============================] - 12s 430ms/step - loss: 0.2311 - lcm_precision_1k: 0.6804 - lcm_precision_2k: 0.5295 - lcm_precision_3k: 0.4275 - lcm_precision_5k: 0.3034 - lcm_recall_1k: 0.4291 - lcm_recall_2k: 0.6229 - lcm_recall_3k: 0.7276 - lcm_recall_5k: 0.8312 - lcm_f1_1k: 0.5262 - lcm_f1_2k: 0.5724 - lcm_f1_3k: 0.5385 - lcm_f1_5k: 0.4445 - lcm_accuracy_1k: 0.6804 - lcm_accuracy_2k: 0.8213 - lcm_accuracy_3k: 0.8839 - lcm_accuracy_5k: 0.9350 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2821 - val_lcm_precision_1k: 0.5767 - val_lcm_precision_2k: 0.4499 - val_lcm_precision_3k: 0.3641 - val_lcm_precision_5k: 0.2634 - val_lcm_recall_1k: 0.3614 - val_lcm_recall_2k: 0.5273 - val_lcm_recall_3k: 0.6200 - val_lcm_recall_5k: 0.7240 - val_lcm_f1_1k: 0.4440 - val_lcm_f1_2k: 0.4853 - val_lcm_f1_3k: 0.4585 - val_lcm_f1_5k: 0.3861 - val_lcm_accuracy_1k: 0.5767 - val_lcm_accuracy_2k: 0.7235 - val_lcm_accuracy_3k: 0.7886 - val_lcm_accuracy_5k: 0.8525 - val_lcm_hamming_loss_k: 0.0041
Epoch 14/16
27/27 [==============================] - ETA: 0s - loss: 0.2249 - lcm_precision_1k: 0.6920 - lcm_precision_2k: 0.5404 - lcm_precision_3k: 0.4354 - lcm_precision_5k: 0.3078 - lcm_recall_1k: 0.4358 - lcm_recall_2k: 0.6336 - lcm_recall_3k: 0.7406 - lcm_recall_5k: 0.8429 - lcm_f1_1k: 0.5348 - lcm_f1_2k: 0.5832 - lcm_f1_3k: 0.5484 - lcm_f1_5k: 0.4509 - lcm_accuracy_1k: 0.6920 - lcm_accuracy_2k: 0.8294 - lcm_accuracy_3k: 0.8936 - lcm_accuracy_5k: 0.9420 - lcm_hamming_loss_k: 0.0036
Epoch 00014: val_loss did not improve from 0.28211
27/27 [==============================] - 11s 393ms/step - loss: 0.2249 - lcm_precision_1k: 0.6920 - lcm_precision_2k: 0.5404 - lcm_precision_3k: 0.4354 - lcm_precision_5k: 0.3078 - lcm_recall_1k: 0.4358 - lcm_recall_2k: 0.6336 - lcm_recall_3k: 0.7406 - lcm_recall_5k: 0.8429 - lcm_f1_1k: 0.5348 - lcm_f1_2k: 0.5832 - lcm_f1_3k: 0.5484 - lcm_f1_5k: 0.4509 - lcm_accuracy_1k: 0.6920 - lcm_accuracy_2k: 0.8294 - lcm_accuracy_3k: 0.8936 - lcm_accuracy_5k: 0.9420 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2866 - val_lcm_precision_1k: 0.5695 - val_lcm_precision_2k: 0.4424 - val_lcm_precision_3k: 0.3598 - val_lcm_precision_5k: 0.2606 - val_lcm_recall_1k: 0.3566 - val_lcm_recall_2k: 0.5202 - val_lcm_recall_3k: 0.6135 - val_lcm_recall_5k: 0.7144 - val_lcm_f1_1k: 0.4383 - val_lcm_f1_2k: 0.4779 - val_lcm_f1_3k: 0.4534 - val_lcm_f1_5k: 0.3818 - val_lcm_accuracy_1k: 0.5695 - val_lcm_accuracy_2k: 0.7146 - val_lcm_accuracy_3k: 0.7842 - val_lcm_accuracy_5k: 0.8467 - val_lcm_hamming_loss_k: 0.0041
Epoch 15/16
27/27 [==============================] - ETA: 0s - loss: 0.2199 - lcm_precision_1k: 0.7001 - lcm_precision_2k: 0.5523 - lcm_precision_3k: 0.4436 - lcm_precision_5k: 0.3111 - lcm_recall_1k: 0.4441 - lcm_recall_2k: 0.6495 - lcm_recall_3k: 0.7544 - lcm_recall_5k: 0.8505 - lcm_f1_1k: 0.5434 - lcm_f1_2k: 0.5970 - lcm_f1_3k: 0.5586 - lcm_f1_5k: 0.4555 - lcm_accuracy_1k: 0.7001 - lcm_accuracy_2k: 0.8434 - lcm_accuracy_3k: 0.9036 - lcm_accuracy_5k: 0.9455 - lcm_hamming_loss_k: 0.0035 ETA: 7s - loss: 0.2158 - lcm_precision_1k: 0.7043 - lcm_precision_2k: 0.5566 - lcm_precision_3k: 0.4462 - lcm_precision_5k: 0.3123 - lcm_recall_1k: 0.4493 - lcm_recall_2k: 0.6599 - lcm_recall_3k: 0.7610 - lcm_recall_5k: 0.8535 - lcm_f1_1k: 0.5486 - lcm_f1_2k: 0.6039 - lcm_f1_3k: 0.5625 - lcm_f1_5k: 0.4572 - lcm_accuracy_1k: 0.7043 - lcm_accuracy_2k: 0.8496 - lcm_accuracy_3k: 0.9059 - lcm_accu
Epoch 00015: val_loss did not improve from 0.28211
27/27 [==============================] - 11s 393ms/step - loss: 0.2199 - lcm_precision_1k: 0.7001 - lcm_precision_2k: 0.5523 - lcm_precision_3k: 0.4436 - lcm_precision_5k: 0.3111 - lcm_recall_1k: 0.4441 - lcm_recall_2k: 0.6495 - lcm_recall_3k: 0.7544 - lcm_recall_5k: 0.8505 - lcm_f1_1k: 0.5434 - lcm_f1_2k: 0.5970 - lcm_f1_3k: 0.5586 - lcm_f1_5k: 0.4555 - lcm_accuracy_1k: 0.7001 - lcm_accuracy_2k: 0.8434 - lcm_accuracy_3k: 0.9036 - lcm_accuracy_5k: 0.9455 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2872 - val_lcm_precision_1k: 0.5750 - val_lcm_precision_2k: 0.4454 - val_lcm_precision_3k: 0.3615 - val_lcm_precision_5k: 0.2618 - val_lcm_recall_1k: 0.3606 - val_lcm_recall_2k: 0.5216 - val_lcm_recall_3k: 0.6162 - val_lcm_recall_5k: 0.7176 - val_lcm_f1_1k: 0.4429 - val_lcm_f1_2k: 0.4803 - val_lcm_f1_3k: 0.4554 - val_lcm_f1_5k: 0.3835 - val_lcm_accuracy_1k: 0.5750 - val_lcm_accuracy_2k: 0.7119 - val_lcm_accuracy_3k: 0.7808 - val_lcm_accuracy_5k: 0.8477 - val_lcm_hamming_loss_k: 0.0041
Epoch 00015: early stopping
176/176 [==============================] - 8s 41ms/step - loss: 0.2394 - lcm_precision_1k: 0.6622 - lcm_precision_2k: 0.5195 - lcm_precision_3k: 0.4165 - lcm_precision_5k: 0.2947 - lcm_recall_1k: 0.4182 - lcm_recall_2k: 0.6135 - lcm_recall_3k: 0.7135 - lcm_recall_5k: 0.8126 - lcm_f1_1k: 0.5112 - lcm_f1_2k: 0.5614 - lcm_f1_3k: 0.5249 - lcm_f1_5k: 0.4318 - lcm_accuracy_1k: 0.6622 - lcm_accuracy_2k: 0.8082 - lcm_accuracy_3k: 0.8689 - lcm_accuracy_5k: 0.9172 - lcm_hamming_loss_k: 0.0037 2s - loss: 0.2390 - lcm_precision_1k: 0.6637 - lcm_precision_2k: 0.5170 - lcm_precision_3k: 0.4148 - lcm_precision_5k: 0.2930 - lcm_recall_1k: 0.4226 - lcm_recall_2k: 0.6144 - lcm_recall_3k: 0.7164 - lcm_recall_5k: 0.8142 - lcm_f1_1k: 0.5150 - lcm_f1_2k: 0.5601 - lcm_f1_3k: 0.5243 - lcm_f1_5k: 0.4302 - lcm_accuracy_1k: 0.6637 - lcm_accuracy_2k: 0.8077 - lcm_accu
Best model result:  [0.23942811787128448, 0.6621965765953064, 0.51949542760849, 0.41645002365112305, 0.2946830093860626, 0.4181506931781769, 0.6134932637214661, 0.7134884595870972, 0.8125864267349243, 0.5111845135688782, 0.5613836646080017, 0.5248693823814392, 0.431792289018631, 0.6621965765953064, 0.8081670999526978, 0.8689380884170532, 0.9171537756919861, 0.003684066003188491]
13499
3374
5625
Model: "model_8"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_12 (S  (None, 15, 300)     0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_12[0][
                                                                 0]']                             
                                                                                                  
 permute_12 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_20 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_12[0][0]']             
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_20[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_21 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_8 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_21[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_4 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_8[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_12 (Dense)               (None, 1024)         2098176     ['tf.concat_4[0][0]']            
                                                                                                  
 dense_13 (Dense)               (None, 15)           15375       ['dense_12[0][0]']               
                                                                                                  
 tf.nn.softmax_4 (TFOpLambda)   (None, 15)           0           ['dense_13[0][0]']               
                                                                                                  
 tf.expand_dims_8 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_4[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_8[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_13 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_22 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_13[0][0]']             
                                                                                                  
 dense_14 (Dense)               (None, 15, 150)      22650       ['lambda_22[0][0]']              
                                                                                                  
 tf.math.reduce_mean_9 (TFOpLam  (None, 150)         0           ['dense_14[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_9 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_9[0][0]']  
                                                                                                  
 tf.__operators__.getitem_13 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_4 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_9[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_13[0][
                                                                 0]']                             
                                                                                                  
 permute_14 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_4[0][0]']     
                                                                                                  
 lambda_23 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_14[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_23[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_24 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_4[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_24[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_9"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_12 (S  (None, 15, 300)     0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_12[0][
                                                                 0]']                             
                                                                                                  
 permute_12 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_20 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_12[0][0]']             
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_20[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_21 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_8 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_21[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_4 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_8[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_12 (Dense)               (None, 1024)         2098176     ['tf.concat_4[0][0]']            
                                                                                                  
 dense_13 (Dense)               (None, 15)           15375       ['dense_12[0][0]']               
                                                                                                  
 tf.nn.softmax_4 (TFOpLambda)   (None, 15)           0           ['dense_13[0][0]']               
                                                                                                  
 tf.expand_dims_8 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_4[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_8[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_13 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_22 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_13[0][0]']             
                                                                                                  
 dense_14 (Dense)               (None, 15, 150)      22650       ['lambda_22[0][0]']              
                                                                                                  
 tf.math.reduce_mean_9 (TFOpLam  (None, 150)         0           ['dense_14[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_9 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_9[0][0]']  
                                                                                                  
 tf.__operators__.getitem_13 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_4 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_9[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_13[0][
                                                                 0]']                             
                                                                                                  
 permute_14 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_4[0][0]']     
                                                                                                  
 lambda_23 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_14[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_23[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_24 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_4[0][0]']     
                                                                                                  
 tf.__operators__.getitem_14 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_24[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_14[0][
                                                                 0]']                             
                                                                                                  
 dot_4 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_4[0][0]']                  
                                                                                                  
 concatenate_4 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/16
27/27 [==============================] - ETA: 0s - loss: 0.4973 - lcm_precision_1k: 0.1953 - lcm_precision_2k: 0.1506 - lcm_precision_3k: 0.1303 - lcm_precision_5k: 0.1082 - lcm_recall_1k: 0.1077 - lcm_recall_2k: 0.1640 - lcm_recall_3k: 0.2089 - lcm_recall_5k: 0.2836 - lcm_f1_1k: 0.1386 - lcm_f1_2k: 0.1568 - lcm_f1_3k: 0.1603 - lcm_f1_5k: 0.1566 - lcm_accuracy_1k: 0.1953 - lcm_accuracy_2k: 0.2726 - lcm_accuracy_3k: 0.3300 - lcm_accuracy_5k: 0.4179 - lcm_hamming_loss_k: 0.0059
Epoch 00001: val_loss improved from inf to 0.43079, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 13s 427ms/step - loss: 0.4973 - lcm_precision_1k: 0.1953 - lcm_precision_2k: 0.1506 - lcm_precision_3k: 0.1303 - lcm_precision_5k: 0.1082 - lcm_recall_1k: 0.1077 - lcm_recall_2k: 0.1640 - lcm_recall_3k: 0.2089 - lcm_recall_5k: 0.2836 - lcm_f1_1k: 0.1386 - lcm_f1_2k: 0.1568 - lcm_f1_3k: 0.1603 - lcm_f1_5k: 0.1566 - lcm_accuracy_1k: 0.1953 - lcm_accuracy_2k: 0.2726 - lcm_accuracy_3k: 0.3300 - lcm_accuracy_5k: 0.4179 - lcm_hamming_loss_k: 0.0059 - val_loss: 0.4308 - val_lcm_precision_1k: 0.3080 - val_lcm_precision_2k: 0.2438 - val_lcm_precision_3k: 0.2062 - val_lcm_precision_5k: 0.1627 - val_lcm_recall_1k: 0.1829 - val_lcm_recall_2k: 0.2727 - val_lcm_recall_3k: 0.3404 - val_lcm_recall_5k: 0.4392 - val_lcm_f1_1k: 0.2294 - val_lcm_f1_2k: 0.2572 - val_lcm_f1_3k: 0.2567 - val_lcm_f1_5k: 0.2373 - val_lcm_accuracy_1k: 0.3080 - val_lcm_accuracy_2k: 0.4208 - val_lcm_accuracy_3k: 0.5006 - val_lcm_accuracy_5k: 0.5921 - val_lcm_hamming_loss_k: 0.0053
Epoch 2/16
27/27 [==============================] - ETA: 0s - loss: 0.3836 - lcm_precision_1k: 0.3666 - lcm_precision_2k: 0.3056 - lcm_precision_3k: 0.2585 - lcm_precision_5k: 0.2010 - lcm_recall_1k: 0.2168 - lcm_recall_2k: 0.3472 - lcm_recall_3k: 0.4293 - lcm_recall_5k: 0.5435 - lcm_f1_1k: 0.2723 - lcm_f1_2k: 0.3250 - lcm_f1_3k: 0.3226 - lcm_f1_5k: 0.2934 - lcm_accuracy_1k: 0.3666 - lcm_accuracy_2k: 0.5156 - lcm_accuracy_3k: 0.5988 - lcm_accuracy_5k: 0.6974 - lcm_hamming_loss_k: 0.0051
Epoch 00002: val_loss improved from 0.43079 to 0.35784, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.3836 - lcm_precision_1k: 0.3666 - lcm_precision_2k: 0.3056 - lcm_precision_3k: 0.2585 - lcm_precision_5k: 0.2010 - lcm_recall_1k: 0.2168 - lcm_recall_2k: 0.3472 - lcm_recall_3k: 0.4293 - lcm_recall_5k: 0.5435 - lcm_f1_1k: 0.2723 - lcm_f1_2k: 0.3250 - lcm_f1_3k: 0.3226 - lcm_f1_5k: 0.2934 - lcm_accuracy_1k: 0.3666 - lcm_accuracy_2k: 0.5156 - lcm_accuracy_3k: 0.5988 - lcm_accuracy_5k: 0.6974 - lcm_hamming_loss_k: 0.0051 - val_loss: 0.3578 - val_lcm_precision_1k: 0.4269 - val_lcm_precision_2k: 0.3475 - val_lcm_precision_3k: 0.2878 - val_lcm_precision_5k: 0.2183 - val_lcm_recall_1k: 0.2607 - val_lcm_recall_2k: 0.4033 - val_lcm_recall_3k: 0.4906 - val_lcm_recall_5k: 0.6019 - val_lcm_f1_1k: 0.3235 - val_lcm_f1_2k: 0.3732 - val_lcm_f1_3k: 0.3626 - val_lcm_f1_5k: 0.3203 - val_lcm_accuracy_1k: 0.4269 - val_lcm_accuracy_2k: 0.5804 - val_lcm_accuracy_3k: 0.6647 - val_lcm_accuracy_5k: 0.7472 - val_lcm_hamming_loss_k: 0.0048
Epoch 3/16
27/27 [==============================] - ETA: 0s - loss: 0.3313 - lcm_precision_1k: 0.4746 - lcm_precision_2k: 0.3854 - lcm_precision_3k: 0.3172 - lcm_precision_5k: 0.2377 - lcm_recall_1k: 0.2884 - lcm_recall_2k: 0.4453 - lcm_recall_3k: 0.5370 - lcm_recall_5k: 0.6520 - lcm_f1_1k: 0.3587 - lcm_f1_2k: 0.4131 - lcm_f1_3k: 0.3987 - lcm_f1_5k: 0.3484 - lcm_accuracy_1k: 0.4746 - lcm_accuracy_2k: 0.6338 - lcm_accuracy_3k: 0.7115 - lcm_accuracy_5k: 0.8006 - lcm_hamming_loss_k: 0.0046
Epoch 00003: val_loss improved from 0.35784 to 0.32829, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.3313 - lcm_precision_1k: 0.4746 - lcm_precision_2k: 0.3854 - lcm_precision_3k: 0.3172 - lcm_precision_5k: 0.2377 - lcm_recall_1k: 0.2884 - lcm_recall_2k: 0.4453 - lcm_recall_3k: 0.5370 - lcm_recall_5k: 0.6520 - lcm_f1_1k: 0.3587 - lcm_f1_2k: 0.4131 - lcm_f1_3k: 0.3987 - lcm_f1_5k: 0.3484 - lcm_accuracy_1k: 0.4746 - lcm_accuracy_2k: 0.6338 - lcm_accuracy_3k: 0.7115 - lcm_accuracy_5k: 0.8006 - lcm_hamming_loss_k: 0.0046 - val_loss: 0.3283 - val_lcm_precision_1k: 0.4795 - val_lcm_precision_2k: 0.3859 - val_lcm_precision_3k: 0.3174 - val_lcm_precision_5k: 0.2336 - val_lcm_recall_1k: 0.2971 - val_lcm_recall_2k: 0.4537 - val_lcm_recall_3k: 0.5455 - val_lcm_recall_5k: 0.6488 - val_lcm_f1_1k: 0.3668 - val_lcm_f1_2k: 0.4169 - val_lcm_f1_3k: 0.4011 - val_lcm_f1_5k: 0.3434 - val_lcm_accuracy_1k: 0.4795 - val_lcm_accuracy_2k: 0.6370 - val_lcm_accuracy_3k: 0.7164 - val_lcm_accuracy_5k: 0.7931 - val_lcm_hamming_loss_k: 0.0045
Epoch 4/16
27/27 [==============================] - ETA: 0s - loss: 0.3085 - lcm_precision_1k: 0.5150 - lcm_precision_2k: 0.4183 - lcm_precision_3k: 0.3446 - lcm_precision_5k: 0.2533 - lcm_recall_1k: 0.3174 - lcm_recall_2k: 0.4863 - lcm_recall_3k: 0.5852 - lcm_recall_5k: 0.6952 - lcm_f1_1k: 0.3927 - lcm_f1_2k: 0.4497 - lcm_f1_3k: 0.4337 - lcm_f1_5k: 0.3713 - lcm_accuracy_1k: 0.5150 - lcm_accuracy_2k: 0.6801 - lcm_accuracy_3k: 0.7602 - lcm_accuracy_5k: 0.8365 - lcm_hamming_loss_k: 0.0044
Epoch 00004: val_loss improved from 0.32829 to 0.31432, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.3085 - lcm_precision_1k: 0.5150 - lcm_precision_2k: 0.4183 - lcm_precision_3k: 0.3446 - lcm_precision_5k: 0.2533 - lcm_recall_1k: 0.3174 - lcm_recall_2k: 0.4863 - lcm_recall_3k: 0.5852 - lcm_recall_5k: 0.6952 - lcm_f1_1k: 0.3927 - lcm_f1_2k: 0.4497 - lcm_f1_3k: 0.4337 - lcm_f1_5k: 0.3713 - lcm_accuracy_1k: 0.5150 - lcm_accuracy_2k: 0.6801 - lcm_accuracy_3k: 0.7602 - lcm_accuracy_5k: 0.8365 - lcm_hamming_loss_k: 0.0044 - val_loss: 0.3143 - val_lcm_precision_1k: 0.5017 - val_lcm_precision_2k: 0.4022 - val_lcm_precision_3k: 0.3301 - val_lcm_precision_5k: 0.2411 - val_lcm_recall_1k: 0.3105 - val_lcm_recall_2k: 0.4731 - val_lcm_recall_3k: 0.5682 - val_lcm_recall_5k: 0.6709 - val_lcm_f1_1k: 0.3835 - val_lcm_f1_2k: 0.4346 - val_lcm_f1_3k: 0.4175 - val_lcm_f1_5k: 0.3546 - val_lcm_accuracy_1k: 0.5017 - val_lcm_accuracy_2k: 0.6563 - val_lcm_accuracy_3k: 0.7366 - val_lcm_accuracy_5k: 0.8100 - val_lcm_hamming_loss_k: 0.0044
Epoch 5/16
27/27 [==============================] - ETA: 0s - loss: 0.2928 - lcm_precision_1k: 0.5472 - lcm_precision_2k: 0.4405 - lcm_precision_3k: 0.3586 - lcm_precision_5k: 0.2622 - lcm_recall_1k: 0.3380 - lcm_recall_2k: 0.5157 - lcm_recall_3k: 0.6107 - lcm_recall_5k: 0.7200 - lcm_f1_1k: 0.4178 - lcm_f1_2k: 0.4751 - lcm_f1_3k: 0.4518 - lcm_f1_5k: 0.3844 - lcm_accuracy_1k: 0.5472 - lcm_accuracy_2k: 0.7126 - lcm_accuracy_3k: 0.7861 - lcm_accuracy_5k: 0.8570 - lcm_hamming_loss_k: 0.0043 ETA: 6s - loss: 0.2932 - lcm_precision_1k: 0.5400 - lcm_precision_2k: 0.4456 - lcm_precision_3k: 0.3620 - lcm_precision_5k: 0.2640 - lcm_recall_1k: 0.3289 - lcm_recall_2k: 0.5209 - lcm_recall_3k: 0.6153 - lcm_recall_5k: 0.7211 - lcm_f1_1k: 0.4087 - lcm_f1_2k: 0.4803 - lcm_f1_3k: 0.4558 - lcm_f1_5k: 0.3865 - lcm_accuracy_1k: 0.5400 - lcm_accuracy_2k: 0.7207 - lcm_accuracy_3k: 0.7914 - lcm_accuracy_5k: 0.8584 - lcm_hamming_loss_k - ETA: 5s - loss: 0.2926 - lcm_precision_1k: 0.5406 - lcm_precision_2k: 0.4387 - lcm_precision_3k: 0.3560 - lcm_precision_5k: 0.2611 - lcm_recall_1k: 0.3336 - lcm_recall_2k: 0.5162 - lcm_recall_3k: 0.6092 - lcm_recall_5k: 0.7205 - lcm_f1_1k: 0.4125 - lcm_f1_2k: 0.4742 - lcm_f1_3k: 0.4493 - lcm_f1_5k: 0.3833 - lcm_accuracy_1k: 0.5406 - lcm_accuracy_2k: 0.7094 - lcm_accuracy_3k: 0.7819 - lcm_accuracy_5k: 0.855 - ETA: 0s - loss: 0.2925 - lcm_precision_1k: 0.5488 - lcm_precision_2k: 0.4403 - lcm_precision_3k: 0.3587 - lcm_precision_5k: 0.2623 - lcm_recall_1k: 0.3396 - lcm_recall_2k: 0.5154 - lcm_recall_3k: 0.6107 - lcm_recall_5k: 0.7207 - lcm_f1_1k: 0.4195 - lcm_f1_2k: 0.4748 - lcm_f1_3k: 0.4519 - lcm_f1_5k: 0.3846 - lcm_accuracy_1k: 0.5488 - lcm_accuracy_2k: 0.7124 - lcm_accuracy_3k: 0.7859 - lcm_accuracy_5k: 0.8574 - lcm_hamming_loss_k: 0.
Epoch 00005: val_loss improved from 0.31432 to 0.30663, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.2928 - lcm_precision_1k: 0.5472 - lcm_precision_2k: 0.4405 - lcm_precision_3k: 0.3586 - lcm_precision_5k: 0.2622 - lcm_recall_1k: 0.3380 - lcm_recall_2k: 0.5157 - lcm_recall_3k: 0.6107 - lcm_recall_5k: 0.7200 - lcm_f1_1k: 0.4178 - lcm_f1_2k: 0.4751 - lcm_f1_3k: 0.4518 - lcm_f1_5k: 0.3844 - lcm_accuracy_1k: 0.5472 - lcm_accuracy_2k: 0.7126 - lcm_accuracy_3k: 0.7861 - lcm_accuracy_5k: 0.8570 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3066 - val_lcm_precision_1k: 0.5207 - val_lcm_precision_2k: 0.4151 - val_lcm_precision_3k: 0.3386 - val_lcm_precision_5k: 0.2454 - val_lcm_recall_1k: 0.3218 - val_lcm_recall_2k: 0.4876 - val_lcm_recall_3k: 0.5836 - val_lcm_recall_5k: 0.6869 - val_lcm_f1_1k: 0.3977 - val_lcm_f1_2k: 0.4483 - val_lcm_f1_3k: 0.4284 - val_lcm_f1_5k: 0.3615 - val_lcm_accuracy_1k: 0.5207 - val_lcm_accuracy_2k: 0.6754 - val_lcm_accuracy_3k: 0.7532 - val_lcm_accuracy_5k: 0.8245 - val_lcm_hamming_loss_k: 0.0043
Epoch 6/16
27/27 [==============================] - ETA: 0s - loss: 0.2805 - lcm_precision_1k: 0.5708 - lcm_precision_2k: 0.4537 - lcm_precision_3k: 0.3720 - lcm_precision_5k: 0.2703 - lcm_recall_1k: 0.3540 - lcm_recall_2k: 0.5310 - lcm_recall_3k: 0.6317 - lcm_recall_5k: 0.7411 - lcm_f1_1k: 0.4370 - lcm_f1_2k: 0.4893 - lcm_f1_3k: 0.4682 - lcm_f1_5k: 0.3962 - lcm_accuracy_1k: 0.5708 - lcm_accuracy_2k: 0.7293 - lcm_accuracy_3k: 0.8064 - lcm_accuracy_5k: 0.8733 - lcm_hamming_loss_k: 0.0042
Epoch 00006: val_loss improved from 0.30663 to 0.29790, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 425ms/step - loss: 0.2805 - lcm_precision_1k: 0.5708 - lcm_precision_2k: 0.4537 - lcm_precision_3k: 0.3720 - lcm_precision_5k: 0.2703 - lcm_recall_1k: 0.3540 - lcm_recall_2k: 0.5310 - lcm_recall_3k: 0.6317 - lcm_recall_5k: 0.7411 - lcm_f1_1k: 0.4370 - lcm_f1_2k: 0.4893 - lcm_f1_3k: 0.4682 - lcm_f1_5k: 0.3962 - lcm_accuracy_1k: 0.5708 - lcm_accuracy_2k: 0.7293 - lcm_accuracy_3k: 0.8064 - lcm_accuracy_5k: 0.8733 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.2979 - val_lcm_precision_1k: 0.5319 - val_lcm_precision_2k: 0.4267 - val_lcm_precision_3k: 0.3458 - val_lcm_precision_5k: 0.2518 - val_lcm_recall_1k: 0.3306 - val_lcm_recall_2k: 0.5008 - val_lcm_recall_3k: 0.5967 - val_lcm_recall_5k: 0.7007 - val_lcm_f1_1k: 0.4076 - val_lcm_f1_2k: 0.4607 - val_lcm_f1_3k: 0.4376 - val_lcm_f1_5k: 0.3704 - val_lcm_accuracy_1k: 0.5319 - val_lcm_accuracy_2k: 0.6878 - val_lcm_accuracy_3k: 0.7627 - val_lcm_accuracy_5k: 0.8304 - val_lcm_hamming_loss_k: 0.0043
Epoch 7/16
27/27 [==============================] - ETA: 0s - loss: 0.2721 - lcm_precision_1k: 0.5928 - lcm_precision_2k: 0.4689 - lcm_precision_3k: 0.3815 - lcm_precision_5k: 0.2752 - lcm_recall_1k: 0.3690 - lcm_recall_2k: 0.5504 - lcm_recall_3k: 0.6512 - lcm_recall_5k: 0.7568 - lcm_f1_1k: 0.4548 - lcm_f1_2k: 0.5063 - lcm_f1_3k: 0.4811 - lcm_f1_5k: 0.4036 - lcm_accuracy_1k: 0.5928 - lcm_accuracy_2k: 0.7506 - lcm_accuracy_3k: 0.8238 - lcm_accuracy_5k: 0.8861 - lcm_hamming_loss_k: 0.0040
Epoch 00007: val_loss improved from 0.29790 to 0.29315, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 425ms/step - loss: 0.2721 - lcm_precision_1k: 0.5928 - lcm_precision_2k: 0.4689 - lcm_precision_3k: 0.3815 - lcm_precision_5k: 0.2752 - lcm_recall_1k: 0.3690 - lcm_recall_2k: 0.5504 - lcm_recall_3k: 0.6512 - lcm_recall_5k: 0.7568 - lcm_f1_1k: 0.4548 - lcm_f1_2k: 0.5063 - lcm_f1_3k: 0.4811 - lcm_f1_5k: 0.4036 - lcm_accuracy_1k: 0.5928 - lcm_accuracy_2k: 0.7506 - lcm_accuracy_3k: 0.8238 - lcm_accuracy_5k: 0.8861 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2932 - val_lcm_precision_1k: 0.5388 - val_lcm_precision_2k: 0.4312 - val_lcm_precision_3k: 0.3473 - val_lcm_precision_5k: 0.2520 - val_lcm_recall_1k: 0.3334 - val_lcm_recall_2k: 0.5086 - val_lcm_recall_3k: 0.6008 - val_lcm_recall_5k: 0.7015 - val_lcm_f1_1k: 0.4117 - val_lcm_f1_2k: 0.4666 - val_lcm_f1_3k: 0.4400 - val_lcm_f1_5k: 0.3707 - val_lcm_accuracy_1k: 0.5388 - val_lcm_accuracy_2k: 0.6961 - val_lcm_accuracy_3k: 0.7701 - val_lcm_accuracy_5k: 0.8321 - val_lcm_hamming_loss_k: 0.0042
Epoch 8/16
27/27 [==============================] - ETA: 0s - loss: 0.2643 - lcm_precision_1k: 0.6091 - lcm_precision_2k: 0.4788 - lcm_precision_3k: 0.3889 - lcm_precision_5k: 0.2814 - lcm_recall_1k: 0.3803 - lcm_recall_2k: 0.5611 - lcm_recall_3k: 0.6633 - lcm_recall_5k: 0.7718 - lcm_f1_1k: 0.4681 - lcm_f1_2k: 0.5166 - lcm_f1_3k: 0.4903 - lcm_f1_5k: 0.4123 - lcm_accuracy_1k: 0.6091 - lcm_accuracy_2k: 0.7605 - lcm_accuracy_3k: 0.8347 - lcm_accuracy_5k: 0.8965 - lcm_hamming_loss_k: 0.0040
Epoch 00008: val_loss improved from 0.29315 to 0.29087, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.2643 - lcm_precision_1k: 0.6091 - lcm_precision_2k: 0.4788 - lcm_precision_3k: 0.3889 - lcm_precision_5k: 0.2814 - lcm_recall_1k: 0.3803 - lcm_recall_2k: 0.5611 - lcm_recall_3k: 0.6633 - lcm_recall_5k: 0.7718 - lcm_f1_1k: 0.4681 - lcm_f1_2k: 0.5166 - lcm_f1_3k: 0.4903 - lcm_f1_5k: 0.4123 - lcm_accuracy_1k: 0.6091 - lcm_accuracy_2k: 0.7605 - lcm_accuracy_3k: 0.8347 - lcm_accuracy_5k: 0.8965 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2909 - val_lcm_precision_1k: 0.5538 - val_lcm_precision_2k: 0.4380 - val_lcm_precision_3k: 0.3515 - val_lcm_precision_5k: 0.2549 - val_lcm_recall_1k: 0.3462 - val_lcm_recall_2k: 0.5172 - val_lcm_recall_3k: 0.6053 - val_lcm_recall_5k: 0.7075 - val_lcm_f1_1k: 0.4259 - val_lcm_f1_2k: 0.4742 - val_lcm_f1_3k: 0.4446 - val_lcm_f1_5k: 0.3747 - val_lcm_accuracy_1k: 0.5538 - val_lcm_accuracy_2k: 0.7034 - val_lcm_accuracy_3k: 0.7691 - val_lcm_accuracy_5k: 0.8340 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/16
27/27 [==============================] - ETA: 0s - loss: 0.2561 - lcm_precision_1k: 0.6234 - lcm_precision_2k: 0.4926 - lcm_precision_3k: 0.3986 - lcm_precision_5k: 0.2872 - lcm_recall_1k: 0.3903 - lcm_recall_2k: 0.5796 - lcm_recall_3k: 0.6801 - lcm_recall_5k: 0.7884 - lcm_f1_1k: 0.4800 - lcm_f1_2k: 0.5325 - lcm_f1_3k: 0.5026 - lcm_f1_5k: 0.4210 - lcm_accuracy_1k: 0.6234 - lcm_accuracy_2k: 0.7795 - lcm_accuracy_3k: 0.8499 - lcm_accuracy_5k: 0.9088 - lcm_hamming_loss_k: 0.0039
Epoch 00009: val_loss improved from 0.29087 to 0.28780, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 12s 432ms/step - loss: 0.2561 - lcm_precision_1k: 0.6234 - lcm_precision_2k: 0.4926 - lcm_precision_3k: 0.3986 - lcm_precision_5k: 0.2872 - lcm_recall_1k: 0.3903 - lcm_recall_2k: 0.5796 - lcm_recall_3k: 0.6801 - lcm_recall_5k: 0.7884 - lcm_f1_1k: 0.4800 - lcm_f1_2k: 0.5325 - lcm_f1_3k: 0.5026 - lcm_f1_5k: 0.4210 - lcm_accuracy_1k: 0.6234 - lcm_accuracy_2k: 0.7795 - lcm_accuracy_3k: 0.8499 - lcm_accuracy_5k: 0.9088 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2878 - val_lcm_precision_1k: 0.5611 - val_lcm_precision_2k: 0.4441 - val_lcm_precision_3k: 0.3577 - val_lcm_precision_5k: 0.2571 - val_lcm_recall_1k: 0.3496 - val_lcm_recall_2k: 0.5255 - val_lcm_recall_3k: 0.6186 - val_lcm_recall_5k: 0.7175 - val_lcm_f1_1k: 0.4306 - val_lcm_f1_2k: 0.4812 - val_lcm_f1_3k: 0.4531 - val_lcm_f1_5k: 0.3784 - val_lcm_accuracy_1k: 0.5611 - val_lcm_accuracy_2k: 0.7169 - val_lcm_accuracy_3k: 0.7853 - val_lcm_accuracy_5k: 0.8441 - val_lcm_hamming_loss_k: 0.0041
Epoch 10/16
27/27 [==============================] - ETA: 0s - loss: 0.2500 - lcm_precision_1k: 0.6356 - lcm_precision_2k: 0.5007 - lcm_precision_3k: 0.4052 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.3982 - lcm_recall_2k: 0.5870 - lcm_recall_3k: 0.6894 - lcm_recall_5k: 0.7960 - lcm_f1_1k: 0.4896 - lcm_f1_2k: 0.5404 - lcm_f1_3k: 0.5104 - lcm_f1_5k: 0.4255 - lcm_accuracy_1k: 0.6356 - lcm_accuracy_2k: 0.7861 - lcm_accuracy_3k: 0.8555 - lcm_accuracy_5k: 0.9137 - lcm_hamming_loss_k: 0.0038 ETA: 2s - loss: 0.2496 - lcm_precision_1k: 0.6387 - lcm_precision_2k: 0.5043 - lcm_precision_3k: 0.4080 - lcm_precision_5k: 0.2921 - lcm_recall_1k: 0.3997 - lcm_recall_2k: 0.5911 - lcm_recall_3k: 0.6928 - lcm_recall_5k: 0.7972 - lcm_f1_1k: 0.4916 - lcm_f1_2k: 0.5442 - lcm_f1_3k: 0.5135 - lcm_f1_5k: 0.4274 - lcm_accuracy_1k: 0.6387 - lcm_accuracy_2k: 0.7919 - lcm_accuracy_3k: 0.8590 - lcm_accuracy_5k: 0.9151 - lcm_hammin
Epoch 00010: val_loss improved from 0.28780 to 0.28508, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2500 - lcm_precision_1k: 0.6356 - lcm_precision_2k: 0.5007 - lcm_precision_3k: 0.4052 - lcm_precision_5k: 0.2904 - lcm_recall_1k: 0.3982 - lcm_recall_2k: 0.5870 - lcm_recall_3k: 0.6894 - lcm_recall_5k: 0.7960 - lcm_f1_1k: 0.4896 - lcm_f1_2k: 0.5404 - lcm_f1_3k: 0.5104 - lcm_f1_5k: 0.4255 - lcm_accuracy_1k: 0.6356 - lcm_accuracy_2k: 0.7861 - lcm_accuracy_3k: 0.8555 - lcm_accuracy_5k: 0.9137 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2851 - val_lcm_precision_1k: 0.5763 - val_lcm_precision_2k: 0.4475 - val_lcm_precision_3k: 0.3593 - val_lcm_precision_5k: 0.2564 - val_lcm_recall_1k: 0.3603 - val_lcm_recall_2k: 0.5294 - val_lcm_recall_3k: 0.6207 - val_lcm_recall_5k: 0.7147 - val_lcm_f1_1k: 0.4433 - val_lcm_f1_2k: 0.4849 - val_lcm_f1_3k: 0.4550 - val_lcm_f1_5k: 0.3773 - val_lcm_accuracy_1k: 0.5763 - val_lcm_accuracy_2k: 0.7222 - val_lcm_accuracy_3k: 0.7828 - val_lcm_accuracy_5k: 0.8420 - val_lcm_hamming_loss_k: 0.0041
Epoch 11/16
27/27 [==============================] - ETA: 0s - loss: 0.2452 - lcm_precision_1k: 0.6507 - lcm_precision_2k: 0.5105 - lcm_precision_3k: 0.4131 - lcm_precision_5k: 0.2948 - lcm_recall_1k: 0.4088 - lcm_recall_2k: 0.5999 - lcm_recall_3k: 0.7036 - lcm_recall_5k: 0.8082 - lcm_f1_1k: 0.5020 - lcm_f1_2k: 0.5515 - lcm_f1_3k: 0.5205 - lcm_f1_5k: 0.4320 - lcm_accuracy_1k: 0.6507 - lcm_accuracy_2k: 0.8008 - lcm_accuracy_3k: 0.8681 - lcm_accuracy_5k: 0.9213 - lcm_hamming_loss_k: 0.0038
Epoch 00011: val_loss improved from 0.28508 to 0.28310, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 12s 430ms/step - loss: 0.2452 - lcm_precision_1k: 0.6507 - lcm_precision_2k: 0.5105 - lcm_precision_3k: 0.4131 - lcm_precision_5k: 0.2948 - lcm_recall_1k: 0.4088 - lcm_recall_2k: 0.5999 - lcm_recall_3k: 0.7036 - lcm_recall_5k: 0.8082 - lcm_f1_1k: 0.5020 - lcm_f1_2k: 0.5515 - lcm_f1_3k: 0.5205 - lcm_f1_5k: 0.4320 - lcm_accuracy_1k: 0.6507 - lcm_accuracy_2k: 0.8008 - lcm_accuracy_3k: 0.8681 - lcm_accuracy_5k: 0.9213 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2831 - val_lcm_precision_1k: 0.5759 - val_lcm_precision_2k: 0.4489 - val_lcm_precision_3k: 0.3625 - val_lcm_precision_5k: 0.2585 - val_lcm_recall_1k: 0.3581 - val_lcm_recall_2k: 0.5297 - val_lcm_recall_3k: 0.6242 - val_lcm_recall_5k: 0.7186 - val_lcm_f1_1k: 0.4415 - val_lcm_f1_2k: 0.4858 - val_lcm_f1_3k: 0.4584 - val_lcm_f1_5k: 0.3801 - val_lcm_accuracy_1k: 0.5759 - val_lcm_accuracy_2k: 0.7184 - val_lcm_accuracy_3k: 0.7834 - val_lcm_accuracy_5k: 0.8428 - val_lcm_hamming_loss_k: 0.0041
Epoch 12/16
27/27 [==============================] - ETA: 0s - loss: 0.2374 - lcm_precision_1k: 0.6667 - lcm_precision_2k: 0.5209 - lcm_precision_3k: 0.4203 - lcm_precision_5k: 0.2994 - lcm_recall_1k: 0.4210 - lcm_recall_2k: 0.6131 - lcm_recall_3k: 0.7166 - lcm_recall_5k: 0.8210 - lcm_f1_1k: 0.5160 - lcm_f1_2k: 0.5632 - lcm_f1_3k: 0.5298 - lcm_f1_5k: 0.4387 - lcm_accuracy_1k: 0.6667 - lcm_accuracy_2k: 0.8136 - lcm_accuracy_3k: 0.8783 - lcm_accuracy_5k: 0.9308 - lcm_hamming_loss_k: 0.0037 ETA: 5s - loss: 0.2349 - lcm_precision_1k: 0.6658 - lcm_precision_2k: 0.5191 - lcm_precision_3k: 0.4197 - lcm_precision_5k: 0.2989 - lcm_recall_1k: 0.4243 - lcm_recall_2k: 0.6169 - lcm_recall_3k: 0.7211 - lcm_recall_5k: 0.8270 - lcm_f1_1k: 0.5183 - lcm_f1_2k: 0.5637 - lcm_f1_3k: 0.5306 - lcm_f1_5k: 0.4391 - lcm_accuracy_1k: 0.6658 - lcm_accuracy_2k: 0.8103 - lcm_accuracy_3k: 0.8793 - lcm_accuracy_5k: 0
Epoch 00012: val_loss improved from 0.28310 to 0.28150, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 12s 432ms/step - loss: 0.2374 - lcm_precision_1k: 0.6667 - lcm_precision_2k: 0.5209 - lcm_precision_3k: 0.4203 - lcm_precision_5k: 0.2994 - lcm_recall_1k: 0.4210 - lcm_recall_2k: 0.6131 - lcm_recall_3k: 0.7166 - lcm_recall_5k: 0.8210 - lcm_f1_1k: 0.5160 - lcm_f1_2k: 0.5632 - lcm_f1_3k: 0.5298 - lcm_f1_5k: 0.4387 - lcm_accuracy_1k: 0.6667 - lcm_accuracy_2k: 0.8136 - lcm_accuracy_3k: 0.8783 - lcm_accuracy_5k: 0.9308 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2815 - val_lcm_precision_1k: 0.5767 - val_lcm_precision_2k: 0.4513 - val_lcm_precision_3k: 0.3633 - val_lcm_precision_5k: 0.2590 - val_lcm_recall_1k: 0.3622 - val_lcm_recall_2k: 0.5341 - val_lcm_recall_3k: 0.6262 - val_lcm_recall_5k: 0.7224 - val_lcm_f1_1k: 0.4449 - val_lcm_f1_2k: 0.4891 - val_lcm_f1_3k: 0.4597 - val_lcm_f1_5k: 0.3812 - val_lcm_accuracy_1k: 0.5767 - val_lcm_accuracy_2k: 0.7251 - val_lcm_accuracy_3k: 0.7856 - val_lcm_accuracy_5k: 0.8431 - val_lcm_hamming_loss_k: 0.0041
Epoch 13/16
27/27 [==============================] - ETA: 0s - loss: 0.2315 - lcm_precision_1k: 0.6747 - lcm_precision_2k: 0.5296 - lcm_precision_3k: 0.4283 - lcm_precision_5k: 0.3038 - lcm_recall_1k: 0.4246 - lcm_recall_2k: 0.6217 - lcm_recall_3k: 0.7288 - lcm_recall_5k: 0.8305 - lcm_f1_1k: 0.5211 - lcm_f1_2k: 0.5719 - lcm_f1_3k: 0.5394 - lcm_f1_5k: 0.4448 - lcm_accuracy_1k: 0.6747 - lcm_accuracy_2k: 0.8216 - lcm_accuracy_3k: 0.8869 - lcm_accuracy_5k: 0.9357 - lcm_hamming_loss_k: 0.0037
Epoch 00013: val_loss improved from 0.28150 to 0.27977, saving model to logs/nifcgt-labs-0604-160819/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2315 - lcm_precision_1k: 0.6747 - lcm_precision_2k: 0.5296 - lcm_precision_3k: 0.4283 - lcm_precision_5k: 0.3038 - lcm_recall_1k: 0.4246 - lcm_recall_2k: 0.6217 - lcm_recall_3k: 0.7288 - lcm_recall_5k: 0.8305 - lcm_f1_1k: 0.5211 - lcm_f1_2k: 0.5719 - lcm_f1_3k: 0.5394 - lcm_f1_5k: 0.4448 - lcm_accuracy_1k: 0.6747 - lcm_accuracy_2k: 0.8216 - lcm_accuracy_3k: 0.8869 - lcm_accuracy_5k: 0.9357 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2798 - val_lcm_precision_1k: 0.5855 - val_lcm_precision_2k: 0.4519 - val_lcm_precision_3k: 0.3658 - val_lcm_precision_5k: 0.2613 - val_lcm_recall_1k: 0.3652 - val_lcm_recall_2k: 0.5353 - val_lcm_recall_3k: 0.6323 - val_lcm_recall_5k: 0.7281 - val_lcm_f1_1k: 0.4497 - val_lcm_f1_2k: 0.4899 - val_lcm_f1_3k: 0.4634 - val_lcm_f1_5k: 0.3844 - val_lcm_accuracy_1k: 0.5855 - val_lcm_accuracy_2k: 0.7272 - val_lcm_accuracy_3k: 0.7962 - val_lcm_accuracy_5k: 0.8543 - val_lcm_hamming_loss_k: 0.0040
Epoch 14/16
27/27 [==============================] - ETA: 0s - loss: 0.2269 - lcm_precision_1k: 0.6875 - lcm_precision_2k: 0.5403 - lcm_precision_3k: 0.4336 - lcm_precision_5k: 0.3068 - lcm_recall_1k: 0.4357 - lcm_recall_2k: 0.6361 - lcm_recall_3k: 0.7384 - lcm_recall_5k: 0.8397 - lcm_f1_1k: 0.5332 - lcm_f1_2k: 0.5842 - lcm_f1_3k: 0.5463 - lcm_f1_5k: 0.4494 - lcm_accuracy_1k: 0.6875 - lcm_accuracy_2k: 0.8329 - lcm_accuracy_3k: 0.8947 - lcm_accuracy_5k: 0.9410 - lcm_hamming_loss_k: 0.0036
Epoch 00014: val_loss did not improve from 0.27977
27/27 [==============================] - 11s 394ms/step - loss: 0.2269 - lcm_precision_1k: 0.6875 - lcm_precision_2k: 0.5403 - lcm_precision_3k: 0.4336 - lcm_precision_5k: 0.3068 - lcm_recall_1k: 0.4357 - lcm_recall_2k: 0.6361 - lcm_recall_3k: 0.7384 - lcm_recall_5k: 0.8397 - lcm_f1_1k: 0.5332 - lcm_f1_2k: 0.5842 - lcm_f1_3k: 0.5463 - lcm_f1_5k: 0.4494 - lcm_accuracy_1k: 0.6875 - lcm_accuracy_2k: 0.8329 - lcm_accuracy_3k: 0.8947 - lcm_accuracy_5k: 0.9410 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2837 - val_lcm_precision_1k: 0.5813 - val_lcm_precision_2k: 0.4527 - val_lcm_precision_3k: 0.3679 - val_lcm_precision_5k: 0.2629 - val_lcm_recall_1k: 0.3641 - val_lcm_recall_2k: 0.5371 - val_lcm_recall_3k: 0.6353 - val_lcm_recall_5k: 0.7300 - val_lcm_f1_1k: 0.4476 - val_lcm_f1_2k: 0.4911 - val_lcm_f1_3k: 0.4658 - val_lcm_f1_5k: 0.3865 - val_lcm_accuracy_1k: 0.5813 - val_lcm_accuracy_2k: 0.7262 - val_lcm_accuracy_3k: 0.7961 - val_lcm_accuracy_5k: 0.8541 - val_lcm_hamming_loss_k: 0.0040
Epoch 15/16
27/27 [==============================] - ETA: 0s - loss: 0.2206 - lcm_precision_1k: 0.7022 - lcm_precision_2k: 0.5488 - lcm_precision_3k: 0.4426 - lcm_precision_5k: 0.3123 - lcm_recall_1k: 0.4449 - lcm_recall_2k: 0.6444 - lcm_recall_3k: 0.7520 - lcm_recall_5k: 0.8519 - lcm_f1_1k: 0.5446 - lcm_f1_2k: 0.5927 - lcm_f1_3k: 0.5572 - lcm_f1_5k: 0.4570 - lcm_accuracy_1k: 0.7022 - lcm_accuracy_2k: 0.8409 - lcm_accuracy_3k: 0.9042 - lcm_accuracy_5k: 0.9480 - lcm_hamming_loss_k: 0.0035 ETA: 3s - loss: 0.2183 - lcm_precision_1k: 0.7073 - lcm_precision_2k: 0.5559 - lcm_precision_3k: 0.4480 - lcm_precision_5k: 0.3141 - lcm_recall_1k: 0.4474 - lcm_recall_2k: 0.6507 - lcm_recall_3k: 0.7587 - lcm_recall_5k: 0.8545 - lcm_f1_1k: 0.5480 - lcm_f1_2k: 0.5995 - lcm_f1_3k: 0.5633 - lcm_f1_5k: 0.4593 - lcm_accuracy_1k: 0.7073 - lcm_accuracy_2k: 0.8464 - lcm_accuracy_3k: 0.9080 - lcm_accuracy_5k: 0.9475 - lcm_
Epoch 00015: val_loss did not improve from 0.27977
27/27 [==============================] - 11s 392ms/step - loss: 0.2206 - lcm_precision_1k: 0.7022 - lcm_precision_2k: 0.5488 - lcm_precision_3k: 0.4426 - lcm_precision_5k: 0.3123 - lcm_recall_1k: 0.4449 - lcm_recall_2k: 0.6444 - lcm_recall_3k: 0.7520 - lcm_recall_5k: 0.8519 - lcm_f1_1k: 0.5446 - lcm_f1_2k: 0.5927 - lcm_f1_3k: 0.5572 - lcm_f1_5k: 0.4570 - lcm_accuracy_1k: 0.7022 - lcm_accuracy_2k: 0.8409 - lcm_accuracy_3k: 0.9042 - lcm_accuracy_5k: 0.9480 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2823 - val_lcm_precision_1k: 0.5770 - val_lcm_precision_2k: 0.4546 - val_lcm_precision_3k: 0.3655 - val_lcm_precision_5k: 0.2609 - val_lcm_recall_1k: 0.3642 - val_lcm_recall_2k: 0.5392 - val_lcm_recall_3k: 0.6297 - val_lcm_recall_5k: 0.7278 - val_lcm_f1_1k: 0.4464 - val_lcm_f1_2k: 0.4931 - val_lcm_f1_3k: 0.4624 - val_lcm_f1_5k: 0.3840 - val_lcm_accuracy_1k: 0.5770 - val_lcm_accuracy_2k: 0.7258 - val_lcm_accuracy_3k: 0.7892 - val_lcm_accuracy_5k: 0.8507 - val_lcm_hamming_loss_k: 0.0040
Epoch 00015: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2403 - lcm_precision_1k: 0.6686 - lcm_precision_2k: 0.5162 - lcm_precision_3k: 0.4126 - lcm_precision_5k: 0.2942 - lcm_recall_1k: 0.4240 - lcm_recall_2k: 0.6105 - lcm_recall_3k: 0.7102 - lcm_recall_5k: 0.8103 - lcm_f1_1k: 0.5176 - lcm_f1_2k: 0.5582 - lcm_f1_3k: 0.5209 - lcm_f1_5k: 0.4309 - lcm_accuracy_1k: 0.6686 - lcm_accuracy_2k: 0.8046 - lcm_accuracy_3k: 0.8692 - lcm_accuracy_5k: 0.9156 - lcm_hamming_loss_k: 0.0037 1s - loss: 0.2411 - lcm_precision_1k: 0.6662 - lcm_precision_2k: 0.5154 - lcm_precision_3k: 0.4111 - lcm_precision_5k: 0.2931 - lcm_recall_1k: 0.4238 - lcm_recall_2k: 0.6122 - lcm_recall_3k: 0.7108 - lcm_recall_5k: 0.8122 - lcm_f1_1k: 0.5167 - lcm_f1_2k: 0.5584 - lcm_f1_3k: 0.5199 - lcm_f1_5k: 0.4301 - lcm_accuracy_1k: 0.6662 - lcm_accuracy_2k: 0.8033 - lcm_accuracy_3k: 0.8684 - lcm_accuracy_5k: 0
Best model result:  [0.24030721187591553, 0.6685838103294373, 0.516174852848053, 0.41263970732688904, 0.29421937465667725, 0.4240012466907501, 0.6104950308799744, 0.7101868391036987, 0.8102573156356812, 0.5175912380218506, 0.5581801533699036, 0.5209137201309204, 0.43094050884246826, 0.6685838103294373, 0.8045699596405029, 0.8692169785499573, 0.9156074523925781, 0.0036541270092129707]
fold_result:  [[0.25650879740715027, 0.6285068392753601, 0.48912903666496277, 0.39226293563842773, 0.2811790704727173, 0.3965180516242981, 0.5789174437522888, 0.673799991607666, 0.7758073806762695, 0.48493242263793945, 0.5291039943695068, 0.4948848485946655, 0.41203343868255615, 0.6285068392753601, 0.769993245601654, 0.8376939296722412, 0.8936727643013, 0.003841844154521823], [0.24381349980831146, 0.6562572717666626, 0.5183631181716919, 0.4162061810493469, 0.29286134243011475, 0.41455402970314026, 0.6141789555549622, 0.7136789560317993, 0.8065295815467834, 0.5067789554595947, 0.5608782172203064, 0.5246820449829102, 0.42893174290657043, 0.6562572717666626, 0.806134819984436, 0.8679265975952148, 0.9107629656791687, 0.0037118762265890837], [0.25472307205200195, 0.6332526803016663, 0.4937584698200226, 0.3971249759197235, 0.2836090326309204, 0.3988352119922638, 0.5827046036720276, 0.6800766587257385, 0.7836320400238037, 0.4880787134170532, 0.5332728028297424, 0.5004100203514099, 0.41578465700149536, 0.6332526803016663, 0.7763351798057556, 0.8408381938934326, 0.896868109703064, 0.0038196241948753595], [0.23942811787128448, 0.6621965765953064, 0.51949542760849, 0.41645002365112305, 0.2946830093860626, 0.4181506931781769, 0.6134932637214661, 0.7134884595870972, 0.8125864267349243, 0.5111845135688782, 0.5613836646080017, 0.5248693823814392, 0.431792289018631, 0.6621965765953064, 0.8081670999526978, 0.8689380884170532, 0.9171537756919861, 0.003684066003188491], [0.24030721187591553, 0.6685838103294373, 0.516174852848053, 0.41263970732688904, 0.29421937465667725, 0.4240012466907501, 0.6104950308799744, 0.7101868391036987, 0.8102573156356812, 0.5175912380218506, 0.5581801533699036, 0.5209137201309204, 0.43094050884246826, 0.6685838103294373, 0.8045699596405029, 0.8692169785499573, 0.9156074523925781, 0.0036541270092129707]]
average_result:  [0.24695613980293274, 0.6497594356536865, 0.507384181022644, 0.40693676471710205, 0.28931036591529846, 0.41041184663772584, 0.5999578595161438, 0.6982461810112, 0.7977625489234924, 0.5017131686210632, 0.5485637664794922, 0.513152003288269, 0.4238965272903442, 0.6497594356536865, 0.7930400609970093, 0.8569227576255798, 0.9068130135536194, 0.0037423075176775456]
2024-06-04 16:11:20,871 : INFO : =======End=======
