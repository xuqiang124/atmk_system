/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2024-06-04 11:45:26,289 : INFO : Loading config...
2024-06-04 11:45:26,290 : INFO : {'cache_file_h5py': '../file_data/a7/math_data.h5', 'cache_file_pickle': '../file_data/a7/vocab_label.pkl', 'embeddings': '../file_data/a7/embeddings.pkl', 'maxlen': 120, 'emb_size': 300, 'epochs': 100, 'batch_size': 256, 'alpha': 4, 'hidden_size': 512, 'num_classes_list': [49], 'l_patience': 2, 'b_patience': 3}
2024-06-04 11:45:26,291 : INFO : Loading data...
2024-06-04 11:45:26,294 : INFO : Loading embeddings...
2024-06-04 11:45:26,297 : INFO : model name lab
TOTAL: 4950 TRAIN: [[3325 1645 3528 ...    0    0    0]
 [3325 1645 2529 ...    0    0    0]
 [1732 1399  586 ...    0    0    0]
 ...
 [ 607 2529  281 ...    0    0    0]
 [3325 1645  607 ...    0    0    0]
 [2899 1645  924 ...    0    0    0]] 3712 TEST: [[3325 1645 1109 ...    0    0    0]
 [2899 1645  607 ...    0    0    0]
 [3325 1645  554 ...    0    0    0]
 ...
 [1109 1645 3325 ...    0    0    0]
 [3325 1645 2529 ...    0    0    0]
 [3325 1645  607 ...    0    0    0]] 1238
2024-06-04 11:45:26,300 : INFO : =====Start final=====
2969
743
1238
2024-06-04 11:45:26.338901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.360692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.360801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.361017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-04 11:45:26.363302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.363431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.363588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.698643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.698765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.698834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 11:45:26.698904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22102 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 49, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Epoch 1/100
2024-06-04 11:45:29.399751: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2024-06-04 11:45:29.434188: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8906
11/12 [==========================>...] - ETA: 0s - loss: 0.3443 - precision_1k: 0.2443 - precision_2k: 0.1969 - precision_3k: 0.1642 - precision_5k: 0.1249 - recall_1k: 0.1220 - recall_2k: 0.1967 - recall_3k: 0.2449 - recall_5k: 0.3090 - F1_1k: 0.1622 - F1_2k: 0.1965 - F1_3k: 0.1964 - F1_5k: 0.1778 - accuracy_1k: 0.2443 - accuracy_2k: 0.3562 - accuracy_3k: 0.4130 - accuracy_5k: 0.4862 - hamming_loss_k: 0.0529
Epoch 00001: val_loss improved from inf to 0.16023, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 3s 104ms/step - loss: 0.3353 - precision_1k: 0.2555 - precision_2k: 0.2069 - precision_3k: 0.1725 - precision_5k: 0.1295 - recall_1k: 0.1268 - recall_2k: 0.2060 - recall_3k: 0.2587 - recall_5k: 0.3225 - F1_1k: 0.1690 - F1_2k: 0.2062 - F1_3k: 0.2068 - F1_5k: 0.1847 - accuracy_1k: 0.2555 - accuracy_2k: 0.3657 - accuracy_3k: 0.4276 - accuracy_5k: 0.4996 - hamming_loss_k: 0.0526 - val_loss: 0.1602 - val_precision_1k: 0.4583 - val_precision_2k: 0.3276 - val_precision_3k: 0.2555 - val_precision_5k: 0.1936 - val_recall_1k: 0.2352 - val_recall_2k: 0.3288 - val_recall_3k: 0.3830 - val_recall_5k: 0.4714 - val_F1_1k: 0.3103 - val_F1_2k: 0.3275 - val_F1_3k: 0.3059 - val_F1_5k: 0.2741 - val_accuracy_1k: 0.4583 - val_accuracy_2k: 0.5622 - val_accuracy_3k: 0.6226 - val_accuracy_5k: 0.6921 - val_hamming_loss_k: 0.0449
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1415 - precision_1k: 0.4098 - precision_2k: 0.3462 - precision_3k: 0.2959 - precision_5k: 0.2255 - recall_1k: 0.2198 - recall_2k: 0.3603 - recall_3k: 0.4557 - recall_5k: 0.5677 - F1_1k: 0.2861 - F1_2k: 0.3531 - F1_3k: 0.3588 - F1_5k: 0.3227 - accuracy_1k: 0.4098 - accuracy_2k: 0.6112 - accuracy_3k: 0.6996 - accuracy_5k: 0.7866 - hamming_loss_k: 0.0462
Epoch 00002: val_loss improved from 0.16023 to 0.12670, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.1407 - precision_1k: 0.4187 - precision_2k: 0.3514 - precision_3k: 0.3003 - precision_5k: 0.2284 - recall_1k: 0.2257 - recall_2k: 0.3673 - recall_3k: 0.4633 - recall_5k: 0.5750 - F1_1k: 0.2932 - F1_2k: 0.3591 - F1_3k: 0.3644 - F1_5k: 0.3269 - accuracy_1k: 0.4187 - accuracy_2k: 0.6185 - accuracy_3k: 0.7072 - accuracy_5k: 0.7918 - hamming_loss_k: 0.0459 - val_loss: 0.1267 - val_precision_1k: 0.5442 - val_precision_2k: 0.4464 - val_precision_3k: 0.3447 - val_precision_5k: 0.2573 - val_recall_1k: 0.2848 - val_recall_2k: 0.4571 - val_recall_3k: 0.5216 - val_recall_5k: 0.6367 - val_F1_1k: 0.3725 - val_F1_2k: 0.4511 - val_F1_3k: 0.4146 - val_F1_5k: 0.3661 - val_accuracy_1k: 0.5442 - val_accuracy_2k: 0.7050 - val_accuracy_3k: 0.7543 - val_accuracy_5k: 0.8234 - val_hamming_loss_k: 0.0414
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1237 - precision_1k: 0.5408 - precision_2k: 0.4297 - precision_3k: 0.3495 - precision_5k: 0.2590 - recall_1k: 0.2917 - recall_2k: 0.4472 - recall_3k: 0.5434 - recall_5k: 0.6489 - F1_1k: 0.3788 - F1_2k: 0.4382 - F1_3k: 0.4253 - F1_5k: 0.3701 - accuracy_1k: 0.5408 - accuracy_2k: 0.6669 - accuracy_3k: 0.7532 - accuracy_5k: 0.8289 - hamming_loss_k: 0.0409
Epoch 00003: val_loss improved from 0.12670 to 0.11320, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.1230 - precision_1k: 0.5437 - precision_2k: 0.4342 - precision_3k: 0.3540 - precision_5k: 0.2622 - recall_1k: 0.2937 - recall_2k: 0.4540 - recall_3k: 0.5513 - recall_5k: 0.6585 - F1_1k: 0.3812 - F1_2k: 0.4437 - F1_3k: 0.4311 - F1_5k: 0.3750 - accuracy_1k: 0.5437 - accuracy_2k: 0.6745 - accuracy_3k: 0.7591 - accuracy_5k: 0.8350 - hamming_loss_k: 0.0407 - val_loss: 0.1132 - val_precision_1k: 0.5640 - val_precision_2k: 0.4678 - val_precision_3k: 0.3932 - val_precision_5k: 0.2849 - val_recall_1k: 0.2998 - val_recall_2k: 0.4955 - val_recall_3k: 0.5997 - val_recall_5k: 0.6986 - val_F1_1k: 0.3904 - val_F1_2k: 0.4809 - val_F1_3k: 0.4746 - val_F1_5k: 0.4044 - val_accuracy_1k: 0.5640 - val_accuracy_2k: 0.7572 - val_accuracy_3k: 0.8121 - val_accuracy_5k: 0.8650 - val_hamming_loss_k: 0.0406
Epoch 4/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1093 - precision_1k: 0.5991 - precision_2k: 0.4931 - precision_3k: 0.4048 - precision_5k: 0.2942 - recall_1k: 0.3221 - recall_2k: 0.5165 - recall_3k: 0.6190 - recall_5k: 0.7291 - F1_1k: 0.4186 - F1_2k: 0.5044 - F1_3k: 0.4894 - F1_5k: 0.4191 - accuracy_1k: 0.5991 - accuracy_2k: 0.7635 - accuracy_3k: 0.8239 - accuracy_5k: 0.8818 - hamming_loss_k: 0.0387
Epoch 00004: val_loss improved from 0.11320 to 0.10478, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.1087 - precision_1k: 0.6031 - precision_2k: 0.4928 - precision_3k: 0.4039 - precision_5k: 0.2929 - recall_1k: 0.3309 - recall_2k: 0.5234 - recall_3k: 0.6257 - recall_5k: 0.7335 - F1_1k: 0.4267 - F1_2k: 0.5073 - F1_3k: 0.4906 - F1_5k: 0.4184 - accuracy_1k: 0.6031 - accuracy_2k: 0.7652 - accuracy_3k: 0.8271 - accuracy_5k: 0.8845 - hamming_loss_k: 0.0382 - val_loss: 0.1048 - val_precision_1k: 0.6798 - val_precision_2k: 0.5237 - val_precision_3k: 0.4285 - val_precision_5k: 0.3111 - val_recall_1k: 0.3751 - val_recall_2k: 0.5487 - val_recall_3k: 0.6581 - val_recall_5k: 0.7685 - val_F1_1k: 0.4828 - val_F1_2k: 0.5350 - val_F1_3k: 0.5185 - val_F1_5k: 0.4424 - val_accuracy_1k: 0.6798 - val_accuracy_2k: 0.7936 - val_accuracy_3k: 0.8713 - val_accuracy_5k: 0.9214 - val_hamming_loss_k: 0.0359
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0998 - precision_1k: 0.6761 - precision_2k: 0.5425 - precision_3k: 0.4334 - precision_5k: 0.3112 - recall_1k: 0.3730 - recall_2k: 0.5713 - recall_3k: 0.6708 - recall_5k: 0.7818 - F1_1k: 0.4804 - F1_2k: 0.5563 - F1_3k: 0.5264 - F1_5k: 0.4451 - accuracy_1k: 0.6761 - accuracy_2k: 0.8054 - accuracy_3k: 0.8633 - accuracy_5k: 0.9190 - hamming_loss_k: 0.0353
Epoch 00005: val_loss improved from 0.10478 to 0.09555, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.0993 - precision_1k: 0.6753 - precision_2k: 0.5446 - precision_3k: 0.4352 - precision_5k: 0.3127 - recall_1k: 0.3716 - recall_2k: 0.5703 - recall_3k: 0.6705 - recall_5k: 0.7819 - F1_1k: 0.4791 - F1_2k: 0.5570 - F1_3k: 0.5276 - F1_5k: 0.4467 - accuracy_1k: 0.6753 - accuracy_2k: 0.8058 - accuracy_3k: 0.8643 - accuracy_5k: 0.9193 - hamming_loss_k: 0.0354 - val_loss: 0.0955 - val_precision_1k: 0.7182 - val_precision_2k: 0.5712 - val_precision_3k: 0.4551 - val_precision_5k: 0.3199 - val_recall_1k: 0.3889 - val_recall_2k: 0.5863 - val_recall_3k: 0.6843 - val_recall_5k: 0.7913 - val_F1_1k: 0.5034 - val_F1_2k: 0.5776 - val_F1_3k: 0.5459 - val_F1_5k: 0.4550 - val_accuracy_1k: 0.7182 - val_accuracy_2k: 0.8395 - val_accuracy_3k: 0.8817 - val_accuracy_5k: 0.9373 - val_hamming_loss_k: 0.0343
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0909 - precision_1k: 0.7212 - precision_2k: 0.5797 - precision_3k: 0.4613 - precision_5k: 0.3269 - recall_1k: 0.4019 - recall_2k: 0.6085 - recall_3k: 0.7083 - recall_5k: 0.8134 - F1_1k: 0.5160 - F1_2k: 0.5936 - F1_3k: 0.5586 - F1_5k: 0.4663 - accuracy_1k: 0.7212 - accuracy_2k: 0.8370 - accuracy_3k: 0.8913 - accuracy_5k: 0.9357 - hamming_loss_k: 0.0335
Epoch 00006: val_loss improved from 0.09555 to 0.09023, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.0912 - precision_1k: 0.7189 - precision_2k: 0.5763 - precision_3k: 0.4591 - precision_5k: 0.3247 - recall_1k: 0.4020 - recall_2k: 0.6061 - recall_3k: 0.7057 - recall_5k: 0.8087 - F1_1k: 0.5154 - F1_2k: 0.5907 - F1_3k: 0.5562 - F1_5k: 0.4633 - accuracy_1k: 0.7189 - accuracy_2k: 0.8337 - accuracy_3k: 0.8900 - accuracy_5k: 0.9340 - hamming_loss_k: 0.0336 - val_loss: 0.0902 - val_precision_1k: 0.7496 - val_precision_2k: 0.5880 - val_precision_3k: 0.4712 - val_precision_5k: 0.3299 - val_recall_1k: 0.4121 - val_recall_2k: 0.5993 - val_recall_3k: 0.7091 - val_recall_5k: 0.8134 - val_F1_1k: 0.5309 - val_F1_2k: 0.5923 - val_F1_3k: 0.5654 - val_F1_5k: 0.4688 - val_accuracy_1k: 0.7496 - val_accuracy_2k: 0.8477 - val_accuracy_3k: 0.8971 - val_accuracy_5k: 0.9416 - val_hamming_loss_k: 0.0330
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0850 - precision_1k: 0.7511 - precision_2k: 0.5987 - precision_3k: 0.4781 - precision_5k: 0.3347 - recall_1k: 0.4262 - recall_2k: 0.6297 - recall_3k: 0.7331 - recall_5k: 0.8351 - F1_1k: 0.5437 - F1_2k: 0.6137 - F1_3k: 0.5787 - F1_5k: 0.4778 - accuracy_1k: 0.7511 - accuracy_2k: 0.8565 - accuracy_3k: 0.9055 - accuracy_5k: 0.9482 - hamming_loss_k: 0.0321 ETA: 0s - loss: 0.0859 - precision_1k: 0.7377 - precision_2k: 0.5943 - precision_3k: 0.4756 - precision_5k: 0.3340 - recall_1k: 0.4150 - recall_2k: 0.6218 - recall_3k: 0.7264 - recall_5k: 0.8314 - F1_1k: 0.5311 - F1_2k: 0.6077 - F1_3k: 0.5748 - F1_5k: 0.4765 - accuracy_1k: 0.7377 - accuracy_2k: 0.8477 - accuracy_3k: 0.8995 - accuracy_5k: 0.9448 - hamming_loss_k
Epoch 00007: val_loss improved from 0.09023 to 0.08458, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.0851 - precision_1k: 0.7522 - precision_2k: 0.6000 - precision_3k: 0.4806 - precision_5k: 0.3370 - recall_1k: 0.4238 - recall_2k: 0.6274 - recall_3k: 0.7323 - recall_5k: 0.8352 - F1_1k: 0.5421 - F1_2k: 0.6133 - F1_3k: 0.5802 - F1_5k: 0.4801 - accuracy_1k: 0.7522 - accuracy_2k: 0.8570 - accuracy_3k: 0.9069 - accuracy_5k: 0.9487 - hamming_loss_k: 0.0324 - val_loss: 0.0846 - val_precision_1k: 0.7851 - val_precision_2k: 0.6184 - val_precision_3k: 0.4946 - val_precision_5k: 0.3429 - val_recall_1k: 0.4339 - val_recall_2k: 0.6391 - val_recall_3k: 0.7479 - val_recall_5k: 0.8423 - val_F1_1k: 0.5582 - val_F1_2k: 0.6278 - val_F1_3k: 0.5948 - val_F1_5k: 0.4867 - val_accuracy_1k: 0.7851 - val_accuracy_2k: 0.8751 - val_accuracy_3k: 0.9175 - val_accuracy_5k: 0.9497 - val_hamming_loss_k: 0.0316
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0799 - precision_1k: 0.7809 - precision_2k: 0.6231 - precision_3k: 0.4979 - precision_5k: 0.3479 - recall_1k: 0.4390 - recall_2k: 0.6484 - recall_3k: 0.7582 - recall_5k: 0.8574 - F1_1k: 0.5619 - F1_2k: 0.6354 - F1_3k: 0.6009 - F1_5k: 0.4949 - accuracy_1k: 0.7809 - accuracy_2k: 0.8718 - accuracy_3k: 0.9229 - accuracy_5k: 0.9549 - hamming_loss_k: 0.0313
Epoch 00008: val_loss improved from 0.08458 to 0.08305, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.0797 - precision_1k: 0.7790 - precision_2k: 0.6199 - precision_3k: 0.4934 - precision_5k: 0.3446 - recall_1k: 0.4406 - recall_2k: 0.6500 - recall_3k: 0.7564 - recall_5k: 0.8561 - F1_1k: 0.5627 - F1_2k: 0.6344 - F1_3k: 0.5971 - F1_5k: 0.4913 - accuracy_1k: 0.7790 - accuracy_2k: 0.8705 - accuracy_3k: 0.9206 - accuracy_5k: 0.9537 - hamming_loss_k: 0.0310 - val_loss: 0.0830 - val_precision_1k: 0.7704 - val_precision_2k: 0.6103 - val_precision_3k: 0.4934 - val_precision_5k: 0.3461 - val_recall_1k: 0.4278 - val_recall_2k: 0.6295 - val_recall_3k: 0.7389 - val_recall_5k: 0.8444 - val_F1_1k: 0.5495 - val_F1_2k: 0.6191 - val_F1_3k: 0.5912 - val_F1_5k: 0.4904 - val_accuracy_1k: 0.7704 - val_accuracy_2k: 0.8656 - val_accuracy_3k: 0.9108 - val_accuracy_5k: 0.9499 - val_hamming_loss_k: 0.0322
Epoch 9/100
12/12 [==============================] - ETA: 0s - loss: 0.0765 - precision_1k: 0.7868 - precision_2k: 0.6278 - precision_3k: 0.5037 - precision_5k: 0.3509 - recall_1k: 0.4453 - recall_2k: 0.6605 - recall_3k: 0.7693 - recall_5k: 0.8687 - F1_1k: 0.5686 - F1_2k: 0.6436 - F1_3k: 0.6086 - F1_5k: 0.4998 - accuracy_1k: 0.7868 - accuracy_2k: 0.8873 - accuracy_3k: 0.9296 - accuracy_5k: 0.9639 - hamming_loss_k: 0.0309
Epoch 00009: val_loss improved from 0.08305 to 0.08039, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 90ms/step - loss: 0.0765 - precision_1k: 0.7868 - precision_2k: 0.6278 - precision_3k: 0.5037 - precision_5k: 0.3509 - recall_1k: 0.4453 - recall_2k: 0.6605 - recall_3k: 0.7693 - recall_5k: 0.8687 - F1_1k: 0.5686 - F1_2k: 0.6436 - F1_3k: 0.6086 - F1_5k: 0.4998 - accuracy_1k: 0.7868 - accuracy_2k: 0.8873 - accuracy_3k: 0.9296 - accuracy_5k: 0.9639 - hamming_loss_k: 0.0309 - val_loss: 0.0804 - val_precision_1k: 0.7759 - val_precision_2k: 0.6132 - val_precision_3k: 0.4945 - val_precision_5k: 0.3480 - val_recall_1k: 0.4367 - val_recall_2k: 0.6340 - val_recall_3k: 0.7499 - val_recall_5k: 0.8567 - val_F1_1k: 0.5583 - val_F1_2k: 0.6227 - val_F1_3k: 0.5954 - val_F1_5k: 0.4944 - val_accuracy_1k: 0.7759 - val_accuracy_2k: 0.8727 - val_accuracy_3k: 0.9238 - val_accuracy_5k: 0.9653 - val_hamming_loss_k: 0.0320
Epoch 10/100
12/12 [==============================] - ETA: 0s - loss: 0.0733 - precision_1k: 0.8124 - precision_2k: 0.6434 - precision_3k: 0.5145 - precision_5k: 0.3544 - recall_1k: 0.4616 - recall_2k: 0.6765 - recall_3k: 0.7860 - recall_5k: 0.8755 - F1_1k: 0.5885 - F1_2k: 0.6593 - F1_3k: 0.6218 - F1_5k: 0.5044 - accuracy_1k: 0.8124 - accuracy_2k: 0.8964 - accuracy_3k: 0.9392 - accuracy_5k: 0.9642 - hamming_loss_k: 0.0298
Epoch 00010: val_loss improved from 0.08039 to 0.07924, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.0733 - precision_1k: 0.8124 - precision_2k: 0.6434 - precision_3k: 0.5145 - precision_5k: 0.3544 - recall_1k: 0.4616 - recall_2k: 0.6765 - recall_3k: 0.7860 - recall_5k: 0.8755 - F1_1k: 0.5885 - F1_2k: 0.6593 - F1_3k: 0.6218 - F1_5k: 0.5044 - accuracy_1k: 0.8124 - accuracy_2k: 0.8964 - accuracy_3k: 0.9392 - accuracy_5k: 0.9642 - hamming_loss_k: 0.0298 - val_loss: 0.0792 - val_precision_1k: 0.7975 - val_precision_2k: 0.6074 - val_precision_3k: 0.5044 - val_precision_5k: 0.3494 - val_recall_1k: 0.4494 - val_recall_2k: 0.6364 - val_recall_3k: 0.7613 - val_recall_5k: 0.8543 - val_F1_1k: 0.5745 - val_F1_2k: 0.6208 - val_F1_3k: 0.6059 - val_F1_5k: 0.4953 - val_accuracy_1k: 0.7975 - val_accuracy_2k: 0.8849 - val_accuracy_3k: 0.9199 - val_accuracy_5k: 0.9594 - val_hamming_loss_k: 0.0311
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0699 - precision_1k: 0.8235 - precision_2k: 0.6502 - precision_3k: 0.5197 - precision_5k: 0.3582 - recall_1k: 0.4698 - recall_2k: 0.6846 - recall_3k: 0.7964 - recall_5k: 0.8874 - F1_1k: 0.5982 - F1_2k: 0.6670 - F1_3k: 0.6289 - F1_5k: 0.5104 - accuracy_1k: 0.8235 - accuracy_2k: 0.9020 - accuracy_3k: 0.9418 - accuracy_5k: 0.9709 - hamming_loss_k: 0.0293
Epoch 00011: val_loss improved from 0.07924 to 0.07559, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 89ms/step - loss: 0.0697 - precision_1k: 0.8230 - precision_2k: 0.6527 - precision_3k: 0.5222 - precision_5k: 0.3593 - recall_1k: 0.4688 - recall_2k: 0.6865 - recall_3k: 0.7987 - recall_5k: 0.8886 - F1_1k: 0.5973 - F1_2k: 0.6691 - F1_3k: 0.6315 - F1_5k: 0.5117 - accuracy_1k: 0.8230 - accuracy_2k: 0.9058 - accuracy_3k: 0.9444 - accuracy_5k: 0.9717 - hamming_loss_k: 0.0294 - val_loss: 0.0756 - val_precision_1k: 0.8016 - val_precision_2k: 0.6405 - val_precision_3k: 0.5186 - val_precision_5k: 0.3572 - val_recall_1k: 0.4423 - val_recall_2k: 0.6599 - val_recall_3k: 0.7827 - val_recall_5k: 0.8699 - val_F1_1k: 0.5696 - val_F1_2k: 0.6490 - val_F1_3k: 0.6231 - val_F1_5k: 0.5059 - val_accuracy_1k: 0.8016 - val_accuracy_2k: 0.8856 - val_accuracy_3k: 0.9365 - val_accuracy_5k: 0.9621 - val_hamming_loss_k: 0.0309
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0665 - precision_1k: 0.8367 - precision_2k: 0.6688 - precision_3k: 0.5311 - precision_5k: 0.3629 - recall_1k: 0.4792 - recall_2k: 0.7038 - recall_3k: 0.8106 - recall_5k: 0.8993 - F1_1k: 0.6093 - F1_2k: 0.6858 - F1_3k: 0.6417 - F1_5k: 0.5171 - accuracy_1k: 0.8367 - accuracy_2k: 0.9180 - accuracy_3k: 0.9478 - accuracy_5k: 0.9752 - hamming_loss_k: 0.0287
Epoch 00012: val_loss improved from 0.07559 to 0.07410, saving model to logs/gepxdx-lab-0604-114526/model/checkpoint_lab.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.0672 - precision_1k: 0.8377 - precision_2k: 0.6673 - precision_3k: 0.5295 - precision_5k: 0.3630 - recall_1k: 0.4798 - recall_2k: 0.7018 - recall_3k: 0.8069 - recall_5k: 0.8974 - F1_1k: 0.6100 - F1_2k: 0.6840 - F1_3k: 0.6393 - F1_5k: 0.5168 - accuracy_1k: 0.8377 - accuracy_2k: 0.9199 - accuracy_3k: 0.9478 - accuracy_5k: 0.9745 - hamming_loss_k: 0.0288 - val_loss: 0.0741 - val_precision_1k: 0.8256 - val_precision_2k: 0.6504 - val_precision_3k: 0.5104 - val_precision_5k: 0.3573 - val_recall_1k: 0.4620 - val_recall_2k: 0.6750 - val_recall_3k: 0.7721 - val_recall_5k: 0.8727 - val_F1_1k: 0.5918 - val_F1_2k: 0.6617 - val_F1_3k: 0.6140 - val_F1_5k: 0.5064 - val_accuracy_1k: 0.8256 - val_accuracy_2k: 0.9076 - val_accuracy_3k: 0.9341 - val_accuracy_5k: 0.9646 - val_hamming_loss_k: 0.0299
Epoch 13/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0643 - precision_1k: 0.8512 - precision_2k: 0.6765 - precision_3k: 0.5389 - precision_5k: 0.3679 - recall_1k: 0.4901 - recall_2k: 0.7149 - recall_3k: 0.8245 - recall_5k: 0.9092 - F1_1k: 0.6218 - F1_2k: 0.6950 - F1_3k: 0.6517 - F1_5k: 0.5237 - accuracy_1k: 0.8512 - accuracy_2k: 0.9279 - accuracy_3k: 0.9602 - accuracy_5k: 0.9776 - hamming_loss_k: 0.0282
Epoch 00013: val_loss did not improve from 0.07410
12/12 [==============================] - 1s 65ms/step - loss: 0.0643 - precision_1k: 0.8494 - precision_2k: 0.6749 - precision_3k: 0.5391 - precision_5k: 0.3678 - recall_1k: 0.4904 - recall_2k: 0.7136 - recall_3k: 0.8242 - recall_5k: 0.9084 - F1_1k: 0.6216 - F1_2k: 0.6935 - F1_3k: 0.6517 - F1_5k: 0.5235 - accuracy_1k: 0.8494 - accuracy_2k: 0.9285 - accuracy_3k: 0.9592 - accuracy_5k: 0.9779 - hamming_loss_k: 0.0283 - val_loss: 0.0770 - val_precision_1k: 0.8146 - val_precision_2k: 0.6358 - val_precision_3k: 0.5106 - val_precision_5k: 0.3537 - val_recall_1k: 0.4539 - val_recall_2k: 0.6576 - val_recall_3k: 0.7697 - val_recall_5k: 0.8635 - val_F1_1k: 0.5821 - val_F1_2k: 0.6457 - val_F1_3k: 0.6132 - val_F1_5k: 0.5013 - val_accuracy_1k: 0.8146 - val_accuracy_2k: 0.8928 - val_accuracy_3k: 0.9311 - val_accuracy_5k: 0.9633 - val_hamming_loss_k: 0.0304
Epoch 14/100
12/12 [==============================] - ETA: 0s - loss: 0.0633 - precision_1k: 0.8597 - precision_2k: 0.6794 - precision_3k: 0.5403 - precision_5k: 0.3698 - recall_1k: 0.4935 - recall_2k: 0.7160 - recall_3k: 0.8267 - recall_5k: 0.9111 - F1_1k: 0.6270 - F1_2k: 0.6971 - F1_3k: 0.6534 - F1_5k: 0.5261 - accuracy_1k: 0.8597 - accuracy_2k: 0.9304 - accuracy_3k: 0.9639 - accuracy_5k: 0.9778 - hamming_loss_k: 0.0279
Epoch 00014: val_loss did not improve from 0.07410
12/12 [==============================] - 1s 66ms/step - loss: 0.0633 - precision_1k: 0.8597 - precision_2k: 0.6794 - precision_3k: 0.5403 - precision_5k: 0.3698 - recall_1k: 0.4935 - recall_2k: 0.7160 - recall_3k: 0.8267 - recall_5k: 0.9111 - F1_1k: 0.6270 - F1_2k: 0.6971 - F1_3k: 0.6534 - F1_5k: 0.5261 - accuracy_1k: 0.8597 - accuracy_2k: 0.9304 - accuracy_3k: 0.9639 - accuracy_5k: 0.9778 - hamming_loss_k: 0.0279 - val_loss: 0.0753 - val_precision_1k: 0.8099 - val_precision_2k: 0.6496 - val_precision_3k: 0.5202 - val_precision_5k: 0.3574 - val_recall_1k: 0.4450 - val_recall_2k: 0.6690 - val_recall_3k: 0.7832 - val_recall_5k: 0.8732 - val_F1_1k: 0.5733 - val_F1_2k: 0.6583 - val_F1_3k: 0.6244 - val_F1_5k: 0.5066 - val_accuracy_1k: 0.8099 - val_accuracy_2k: 0.8910 - val_accuracy_3k: 0.9365 - val_accuracy_5k: 0.9659 - val_hamming_loss_k: 0.0306
Epoch 00014: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.0712 - precision_1k: 0.8459 - precision_2k: 0.6704 - precision_3k: 0.5274 - precision_5k: 0.3655 - recall_1k: 0.4749 - recall_2k: 0.6976 - recall_3k: 0.7926 - recall_5k: 0.8915 - F1_1k: 0.6073 - F1_2k: 0.6827 - F1_3k: 0.6323 - F1_5k: 0.5177 - accuracy_1k: 0.8459 - accuracy_2k: 0.9255 - accuracy_3k: 0.9532 - accuracy_5k: 0.9824 - hamming_loss_k: 0.0294
Best model result:  [0.07121290266513824, 0.8458691835403442, 0.6703769564628601, 0.5273845791816711, 0.36553332209587097, 0.47485893964767456, 0.6976229548454285, 0.792594850063324, 0.8914691209793091, 0.6072724461555481, 0.6826519966125488, 0.6323443651199341, 0.5177146196365356, 0.8458691835403442, 0.9255487322807312, 0.9531742930412292, 0.9823870658874512, 0.029350105673074722]
2969
743
1238
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_2[0][0
                                                                 ]']                              
                                                                                                  
 permute_1 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_1[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_2[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_3 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_3[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3777 - precision_1k: 0.2329 - precision_2k: 0.1802 - precision_3k: 0.1433 - precision_5k: 0.1178 - recall_1k: 0.1188 - recall_2k: 0.1846 - recall_3k: 0.2169 - recall_5k: 0.2954 - F1_1k: 0.1566 - F1_2k: 0.1818 - F1_3k: 0.1723 - F1_5k: 0.1684 - accuracy_1k: 0.2329 - accuracy_2k: 0.3345 - accuracy_3k: 0.3874 - accuracy_5k: 0.4922 - hamming_loss_k: 0.0537
Epoch 00001: val_loss improved from inf to 0.16785, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 3s 128ms/step - loss: 0.3665 - precision_1k: 0.2402 - precision_2k: 0.1867 - precision_3k: 0.1534 - precision_5k: 0.1252 - recall_1k: 0.1257 - recall_2k: 0.1937 - recall_3k: 0.2345 - recall_5k: 0.3163 - F1_1k: 0.1642 - F1_2k: 0.1896 - F1_3k: 0.1852 - F1_5k: 0.1793 - accuracy_1k: 0.2402 - accuracy_2k: 0.3464 - accuracy_3k: 0.4085 - accuracy_5k: 0.5116 - hamming_loss_k: 0.0532 - val_loss: 0.1679 - val_precision_1k: 0.3985 - val_precision_2k: 0.3435 - val_precision_3k: 0.2733 - val_precision_5k: 0.2240 - val_recall_1k: 0.2216 - val_recall_2k: 0.3565 - val_recall_3k: 0.4146 - val_recall_5k: 0.5543 - val_F1_1k: 0.2846 - val_F1_2k: 0.3498 - val_F1_3k: 0.3293 - val_F1_5k: 0.3190 - val_accuracy_1k: 0.3985 - val_accuracy_2k: 0.5400 - val_accuracy_3k: 0.6177 - val_accuracy_5k: 0.7507 - val_hamming_loss_k: 0.0470
Epoch 2/100
12/12 [==============================] - ETA: 0s - loss: 0.1447 - precision_1k: 0.4835 - precision_2k: 0.3751 - precision_3k: 0.3083 - precision_5k: 0.2294 - recall_1k: 0.2607 - recall_2k: 0.3870 - recall_3k: 0.4705 - recall_5k: 0.5745 - F1_1k: 0.3387 - F1_2k: 0.3809 - F1_3k: 0.3725 - F1_5k: 0.3278 - accuracy_1k: 0.4835 - accuracy_2k: 0.6161 - accuracy_3k: 0.6960 - accuracy_5k: 0.7815 - hamming_loss_k: 0.0433
Epoch 00002: val_loss improved from 0.16785 to 0.12569, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.1447 - precision_1k: 0.4835 - precision_2k: 0.3751 - precision_3k: 0.3083 - precision_5k: 0.2294 - recall_1k: 0.2607 - recall_2k: 0.3870 - recall_3k: 0.4705 - recall_5k: 0.5745 - F1_1k: 0.3387 - F1_2k: 0.3809 - F1_3k: 0.3725 - F1_5k: 0.3278 - accuracy_1k: 0.4835 - accuracy_2k: 0.6161 - accuracy_3k: 0.6960 - accuracy_5k: 0.7815 - hamming_loss_k: 0.0433 - val_loss: 0.1257 - val_precision_1k: 0.4872 - val_precision_2k: 0.4531 - val_precision_3k: 0.3597 - val_precision_5k: 0.2548 - val_recall_1k: 0.2441 - val_recall_2k: 0.4578 - val_recall_3k: 0.5382 - val_recall_5k: 0.6290 - val_F1_1k: 0.3249 - val_F1_2k: 0.4545 - val_F1_3k: 0.4305 - val_F1_5k: 0.3623 - val_accuracy_1k: 0.4872 - val_accuracy_2k: 0.6746 - val_accuracy_3k: 0.7475 - val_accuracy_5k: 0.8268 - val_hamming_loss_k: 0.0434
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1213 - precision_1k: 0.5504 - precision_2k: 0.4586 - precision_3k: 0.3804 - precision_5k: 0.2673 - recall_1k: 0.2992 - recall_2k: 0.4798 - recall_3k: 0.5824 - recall_5k: 0.6747 - F1_1k: 0.3876 - F1_2k: 0.4689 - F1_3k: 0.4602 - F1_5k: 0.3829 - accuracy_1k: 0.5504 - accuracy_2k: 0.7156 - accuracy_3k: 0.7859 - accuracy_5k: 0.8540 - hamming_loss_k: 0.0405
Epoch 00003: val_loss improved from 0.12569 to 0.11386, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 103ms/step - loss: 0.1211 - precision_1k: 0.5536 - precision_2k: 0.4602 - precision_3k: 0.3822 - precision_5k: 0.2679 - recall_1k: 0.3030 - recall_2k: 0.4835 - recall_3k: 0.5862 - recall_5k: 0.6765 - F1_1k: 0.3916 - F1_2k: 0.4714 - F1_3k: 0.4626 - F1_5k: 0.3838 - accuracy_1k: 0.5536 - accuracy_2k: 0.7196 - accuracy_3k: 0.7917 - accuracy_5k: 0.8580 - hamming_loss_k: 0.0405 - val_loss: 0.1139 - val_precision_1k: 0.5926 - val_precision_2k: 0.4889 - val_precision_3k: 0.3904 - val_precision_5k: 0.2792 - val_recall_1k: 0.3159 - val_recall_2k: 0.5046 - val_recall_3k: 0.5954 - val_recall_5k: 0.6875 - val_F1_1k: 0.4117 - val_F1_2k: 0.4960 - val_F1_3k: 0.4711 - val_F1_5k: 0.3969 - val_accuracy_1k: 0.5926 - val_accuracy_2k: 0.7466 - val_accuracy_3k: 0.8049 - val_accuracy_5k: 0.8610 - val_hamming_loss_k: 0.0391
Epoch 4/100
12/12 [==============================] - ETA: 0s - loss: 0.1084 - precision_1k: 0.6182 - precision_2k: 0.5013 - precision_3k: 0.4085 - precision_5k: 0.2937 - recall_1k: 0.3382 - recall_2k: 0.5282 - recall_3k: 0.6277 - recall_5k: 0.7343 - F1_1k: 0.4370 - F1_2k: 0.5142 - F1_3k: 0.4948 - F1_5k: 0.4195 - accuracy_1k: 0.6182 - accuracy_2k: 0.7749 - accuracy_3k: 0.8328 - accuracy_5k: 0.8948 - hamming_loss_k: 0.0378
Epoch 00004: val_loss improved from 0.11386 to 0.10352, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.1084 - precision_1k: 0.6182 - precision_2k: 0.5013 - precision_3k: 0.4085 - precision_5k: 0.2937 - recall_1k: 0.3382 - recall_2k: 0.5282 - recall_3k: 0.6277 - recall_5k: 0.7343 - F1_1k: 0.4370 - F1_2k: 0.5142 - F1_3k: 0.4948 - F1_5k: 0.4195 - accuracy_1k: 0.6182 - accuracy_2k: 0.7749 - accuracy_3k: 0.8328 - accuracy_5k: 0.8948 - hamming_loss_k: 0.0378 - val_loss: 0.1035 - val_precision_1k: 0.6549 - val_precision_2k: 0.5434 - val_precision_3k: 0.4295 - val_precision_5k: 0.3096 - val_recall_1k: 0.3535 - val_recall_2k: 0.5620 - val_recall_3k: 0.6522 - val_recall_5k: 0.7641 - val_F1_1k: 0.4585 - val_F1_2k: 0.5519 - val_F1_3k: 0.5175 - val_F1_5k: 0.4404 - val_accuracy_1k: 0.6549 - val_accuracy_2k: 0.7912 - val_accuracy_3k: 0.8488 - val_accuracy_5k: 0.9117 - val_hamming_loss_k: 0.0365
Epoch 5/100
12/12 [==============================] - ETA: 0s - loss: 0.0991 - precision_1k: 0.6851 - precision_2k: 0.5499 - precision_3k: 0.4399 - precision_5k: 0.3109 - recall_1k: 0.3808 - recall_2k: 0.5806 - recall_3k: 0.6792 - recall_5k: 0.7791 - F1_1k: 0.4893 - F1_2k: 0.5647 - F1_3k: 0.5338 - F1_5k: 0.4443 - accuracy_1k: 0.6851 - accuracy_2k: 0.8194 - accuracy_3k: 0.8690 - accuracy_5k: 0.9213 - hamming_loss_k: 0.0351
Epoch 00005: val_loss improved from 0.10352 to 0.09715, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.0991 - precision_1k: 0.6851 - precision_2k: 0.5499 - precision_3k: 0.4399 - precision_5k: 0.3109 - recall_1k: 0.3808 - recall_2k: 0.5806 - recall_3k: 0.6792 - recall_5k: 0.7791 - F1_1k: 0.4893 - F1_2k: 0.5647 - F1_3k: 0.5338 - F1_5k: 0.4443 - accuracy_1k: 0.6851 - accuracy_2k: 0.8194 - accuracy_3k: 0.8690 - accuracy_5k: 0.9213 - hamming_loss_k: 0.0351 - val_loss: 0.0972 - val_precision_1k: 0.7080 - val_precision_2k: 0.5803 - val_precision_3k: 0.4536 - val_precision_5k: 0.3231 - val_recall_1k: 0.3874 - val_recall_2k: 0.6040 - val_recall_3k: 0.6855 - val_recall_5k: 0.7844 - val_F1_1k: 0.5003 - val_F1_2k: 0.5915 - val_F1_3k: 0.5456 - val_F1_5k: 0.4573 - val_accuracy_1k: 0.7080 - val_accuracy_2k: 0.8401 - val_accuracy_3k: 0.8727 - val_accuracy_5k: 0.9088 - val_hamming_loss_k: 0.0344
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0914 - precision_1k: 0.7266 - precision_2k: 0.5813 - precision_3k: 0.4618 - precision_5k: 0.3270 - recall_1k: 0.4091 - recall_2k: 0.6136 - recall_3k: 0.7094 - recall_5k: 0.8143 - F1_1k: 0.5234 - F1_2k: 0.5969 - F1_3k: 0.5593 - F1_5k: 0.4665 - accuracy_1k: 0.7266 - accuracy_2k: 0.8484 - accuracy_3k: 0.8888 - accuracy_5k: 0.9332 - hamming_loss_k: 0.0332
Epoch 00006: val_loss improved from 0.09715 to 0.09180, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.0917 - precision_1k: 0.7281 - precision_2k: 0.5852 - precision_3k: 0.4645 - precision_5k: 0.3286 - recall_1k: 0.4103 - recall_2k: 0.6167 - recall_3k: 0.7114 - recall_5k: 0.8144 - F1_1k: 0.5247 - F1_2k: 0.6004 - F1_3k: 0.5619 - F1_5k: 0.4682 - accuracy_1k: 0.7281 - accuracy_2k: 0.8528 - accuracy_3k: 0.8910 - accuracy_5k: 0.9344 - hamming_loss_k: 0.0334 - val_loss: 0.0918 - val_precision_1k: 0.7364 - val_precision_2k: 0.5870 - val_precision_3k: 0.4636 - val_precision_5k: 0.3260 - val_recall_1k: 0.4051 - val_recall_2k: 0.6082 - val_recall_3k: 0.7035 - val_recall_5k: 0.7992 - val_F1_1k: 0.5222 - val_F1_2k: 0.5968 - val_F1_3k: 0.5584 - val_F1_5k: 0.4627 - val_accuracy_1k: 0.7364 - val_accuracy_2k: 0.8383 - val_accuracy_3k: 0.8925 - val_accuracy_5k: 0.9318 - val_hamming_loss_k: 0.0332
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0852 - precision_1k: 0.7596 - precision_2k: 0.6092 - precision_3k: 0.4850 - precision_5k: 0.3388 - recall_1k: 0.4283 - recall_2k: 0.6376 - recall_3k: 0.7396 - recall_5k: 0.8372 - F1_1k: 0.5476 - F1_2k: 0.6230 - F1_3k: 0.5857 - F1_5k: 0.4823 - accuracy_1k: 0.7596 - accuracy_2k: 0.8718 - accuracy_3k: 0.9119 - accuracy_5k: 0.9485 - hamming_loss_k: 0.0321
Epoch 00007: val_loss improved from 0.09180 to 0.08685, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.0850 - precision_1k: 0.7617 - precision_2k: 0.6099 - precision_3k: 0.4827 - precision_5k: 0.3371 - recall_1k: 0.4309 - recall_2k: 0.6417 - recall_3k: 0.7395 - recall_5k: 0.8369 - F1_1k: 0.5502 - F1_2k: 0.6253 - F1_3k: 0.5840 - F1_5k: 0.4806 - accuracy_1k: 0.7617 - accuracy_2k: 0.8743 - accuracy_3k: 0.9122 - accuracy_5k: 0.9490 - hamming_loss_k: 0.0319 - val_loss: 0.0869 - val_precision_1k: 0.7718 - val_precision_2k: 0.6181 - val_precision_3k: 0.4832 - val_precision_5k: 0.3369 - val_recall_1k: 0.4257 - val_recall_2k: 0.6401 - val_recall_3k: 0.7278 - val_recall_5k: 0.8262 - val_F1_1k: 0.5481 - val_F1_2k: 0.6282 - val_F1_3k: 0.5803 - val_F1_5k: 0.4783 - val_accuracy_1k: 0.7718 - val_accuracy_2k: 0.8704 - val_accuracy_3k: 0.9023 - val_accuracy_5k: 0.9338 - val_hamming_loss_k: 0.0318
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0803 - precision_1k: 0.7841 - precision_2k: 0.6209 - precision_3k: 0.4957 - precision_5k: 0.3460 - recall_1k: 0.4438 - recall_2k: 0.6519 - recall_3k: 0.7582 - recall_5k: 0.8565 - F1_1k: 0.5667 - F1_2k: 0.6359 - F1_3k: 0.5994 - F1_5k: 0.4927 - accuracy_1k: 0.7841 - accuracy_2k: 0.8771 - accuracy_3k: 0.9265 - accuracy_5k: 0.9602 - hamming_loss_k: 0.0311
Epoch 00008: val_loss improved from 0.08685 to 0.08448, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 90ms/step - loss: 0.0802 - precision_1k: 0.7847 - precision_2k: 0.6201 - precision_3k: 0.4940 - precision_5k: 0.3451 - recall_1k: 0.4459 - recall_2k: 0.6526 - recall_3k: 0.7572 - recall_5k: 0.8562 - F1_1k: 0.5686 - F1_2k: 0.6358 - F1_3k: 0.5978 - F1_5k: 0.4918 - accuracy_1k: 0.7847 - accuracy_2k: 0.8759 - accuracy_3k: 0.9261 - accuracy_5k: 0.9592 - hamming_loss_k: 0.0310 - val_loss: 0.0845 - val_precision_1k: 0.7867 - val_precision_2k: 0.6140 - val_precision_3k: 0.4891 - val_precision_5k: 0.3408 - val_recall_1k: 0.4351 - val_recall_2k: 0.6333 - val_recall_3k: 0.7368 - val_recall_5k: 0.8393 - val_F1_1k: 0.5596 - val_F1_2k: 0.6229 - val_F1_3k: 0.5874 - val_F1_5k: 0.4843 - val_accuracy_1k: 0.7867 - val_accuracy_2k: 0.8691 - val_accuracy_3k: 0.9129 - val_accuracy_5k: 0.9567 - val_hamming_loss_k: 0.0312
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0768 - precision_1k: 0.8068 - precision_2k: 0.6335 - precision_3k: 0.5071 - precision_5k: 0.3520 - recall_1k: 0.4587 - recall_2k: 0.6670 - recall_3k: 0.7776 - recall_5k: 0.8712 - F1_1k: 0.5848 - F1_2k: 0.6498 - F1_3k: 0.6138 - F1_5k: 0.5014 - accuracy_1k: 0.8068 - accuracy_2k: 0.8942 - accuracy_3k: 0.9357 - accuracy_5k: 0.9641 - hamming_loss_k: 0.0301
Epoch 00009: val_loss improved from 0.08448 to 0.08246, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.0766 - precision_1k: 0.8088 - precision_2k: 0.6357 - precision_3k: 0.5077 - precision_5k: 0.3513 - recall_1k: 0.4606 - recall_2k: 0.6703 - recall_3k: 0.7784 - recall_5k: 0.8702 - F1_1k: 0.5869 - F1_2k: 0.6525 - F1_3k: 0.6145 - F1_5k: 0.5005 - accuracy_1k: 0.8088 - accuracy_2k: 0.8965 - accuracy_3k: 0.9356 - accuracy_5k: 0.9639 - hamming_loss_k: 0.0300 - val_loss: 0.0825 - val_precision_1k: 0.7881 - val_precision_2k: 0.6232 - val_precision_3k: 0.4919 - val_precision_5k: 0.3414 - val_recall_1k: 0.4346 - val_recall_2k: 0.6408 - val_recall_3k: 0.7416 - val_recall_5k: 0.8342 - val_F1_1k: 0.5596 - val_F1_2k: 0.6313 - val_F1_3k: 0.5909 - val_F1_5k: 0.4842 - val_accuracy_1k: 0.7881 - val_accuracy_2k: 0.8691 - val_accuracy_3k: 0.9096 - val_accuracy_5k: 0.9439 - val_hamming_loss_k: 0.0311
Epoch 10/100
12/12 [==============================] - ETA: 0s - loss: 0.0725 - precision_1k: 0.8188 - precision_2k: 0.6481 - precision_3k: 0.5168 - precision_5k: 0.3561 - recall_1k: 0.4692 - recall_2k: 0.6859 - recall_3k: 0.7924 - recall_5k: 0.8831 - F1_1k: 0.5964 - F1_2k: 0.6663 - F1_3k: 0.6254 - F1_5k: 0.5074 - accuracy_1k: 0.8188 - accuracy_2k: 0.9039 - accuracy_3k: 0.9435 - accuracy_5k: 0.9695 - hamming_loss_k: 0.0295
Epoch 00010: val_loss improved from 0.08246 to 0.07959, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 93ms/step - loss: 0.0725 - precision_1k: 0.8188 - precision_2k: 0.6481 - precision_3k: 0.5168 - precision_5k: 0.3561 - recall_1k: 0.4692 - recall_2k: 0.6859 - recall_3k: 0.7924 - recall_5k: 0.8831 - F1_1k: 0.5964 - F1_2k: 0.6663 - F1_3k: 0.6254 - F1_5k: 0.5074 - accuracy_1k: 0.8188 - accuracy_2k: 0.9039 - accuracy_3k: 0.9435 - accuracy_5k: 0.9695 - hamming_loss_k: 0.0295 - val_loss: 0.0796 - val_precision_1k: 0.8058 - val_precision_2k: 0.6380 - val_precision_3k: 0.5096 - val_precision_5k: 0.3504 - val_recall_1k: 0.4451 - val_recall_2k: 0.6561 - val_recall_3k: 0.7603 - val_recall_5k: 0.8540 - val_F1_1k: 0.5729 - val_F1_2k: 0.6463 - val_F1_3k: 0.6096 - val_F1_5k: 0.4966 - val_accuracy_1k: 0.8058 - val_accuracy_2k: 0.8847 - val_accuracy_3k: 0.9156 - val_accuracy_5k: 0.9496 - val_hamming_loss_k: 0.0304
Epoch 11/100
12/12 [==============================] - ETA: 0s - loss: 0.0695 - precision_1k: 0.8346 - precision_2k: 0.6613 - precision_3k: 0.5278 - precision_5k: 0.3619 - recall_1k: 0.4773 - recall_2k: 0.6972 - recall_3k: 0.8054 - recall_5k: 0.8937 - F1_1k: 0.6072 - F1_2k: 0.6787 - F1_3k: 0.6376 - F1_5k: 0.5150 - accuracy_1k: 0.8346 - accuracy_2k: 0.9128 - accuracy_3k: 0.9494 - accuracy_5k: 0.9739 - hamming_loss_k: 0.0289
Epoch 00011: val_loss improved from 0.07959 to 0.07791, saving model to logs/hlfleu-lab-0604-114546/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.0695 - precision_1k: 0.8346 - precision_2k: 0.6613 - precision_3k: 0.5278 - precision_5k: 0.3619 - recall_1k: 0.4773 - recall_2k: 0.6972 - recall_3k: 0.8054 - recall_5k: 0.8937 - F1_1k: 0.6072 - F1_2k: 0.6787 - F1_3k: 0.6376 - F1_5k: 0.5150 - accuracy_1k: 0.8346 - accuracy_2k: 0.9128 - accuracy_3k: 0.9494 - accuracy_5k: 0.9739 - hamming_loss_k: 0.0289 - val_loss: 0.0779 - val_precision_1k: 0.8113 - val_precision_2k: 0.6435 - val_precision_3k: 0.5115 - val_precision_5k: 0.3487 - val_recall_1k: 0.4489 - val_recall_2k: 0.6615 - val_recall_3k: 0.7678 - val_recall_5k: 0.8513 - val_F1_1k: 0.5772 - val_F1_2k: 0.6516 - val_F1_3k: 0.6133 - val_F1_5k: 0.4943 - val_accuracy_1k: 0.8113 - val_accuracy_2k: 0.8908 - val_accuracy_3k: 0.9263 - val_accuracy_5k: 0.9575 - val_hamming_loss_k: 0.0302
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0670 - precision_1k: 0.8480 - precision_2k: 0.6688 - precision_3k: 0.5323 - precision_5k: 0.3636 - recall_1k: 0.4890 - recall_2k: 0.7083 - recall_3k: 0.8152 - recall_5k: 0.9010 - F1_1k: 0.6201 - F1_2k: 0.6879 - F1_3k: 0.6439 - F1_5k: 0.5181 - accuracy_1k: 0.8480 - accuracy_2k: 0.9251 - accuracy_3k: 0.9542 - accuracy_5k: 0.9791 - hamming_loss_k: 0.0283
Epoch 00012: val_loss did not improve from 0.07791
12/12 [==============================] - 1s 64ms/step - loss: 0.0670 - precision_1k: 0.8525 - precision_2k: 0.6700 - precision_3k: 0.5343 - precision_5k: 0.3652 - recall_1k: 0.4907 - recall_2k: 0.7081 - recall_3k: 0.8153 - recall_5k: 0.9014 - F1_1k: 0.6228 - F1_2k: 0.6884 - F1_3k: 0.6454 - F1_5k: 0.5198 - accuracy_1k: 0.8525 - accuracy_2k: 0.9275 - accuracy_3k: 0.9547 - accuracy_5k: 0.9786 - hamming_loss_k: 0.0283 - val_loss: 0.0795 - val_precision_1k: 0.8120 - val_precision_2k: 0.6348 - val_precision_3k: 0.5138 - val_precision_5k: 0.3540 - val_recall_1k: 0.4470 - val_recall_2k: 0.6546 - val_recall_3k: 0.7703 - val_recall_5k: 0.8630 - val_F1_1k: 0.5760 - val_F1_2k: 0.6440 - val_F1_3k: 0.6158 - val_F1_5k: 0.5017 - val_accuracy_1k: 0.8120 - val_accuracy_2k: 0.8929 - val_accuracy_3k: 0.9282 - val_accuracy_5k: 0.9593 - val_hamming_loss_k: 0.0301
Epoch 13/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0663 - precision_1k: 0.8540 - precision_2k: 0.6680 - precision_3k: 0.5356 - precision_5k: 0.3662 - recall_1k: 0.4899 - recall_2k: 0.7057 - recall_3k: 0.8172 - recall_5k: 0.9044 - F1_1k: 0.6225 - F1_2k: 0.6861 - F1_3k: 0.6469 - F1_5k: 0.5212 - accuracy_1k: 0.8540 - accuracy_2k: 0.9243 - accuracy_3k: 0.9556 - accuracy_5k: 0.9780 - hamming_loss_k: 0.0281
Epoch 00013: val_loss did not improve from 0.07791
12/12 [==============================] - 1s 69ms/step - loss: 0.0663 - precision_1k: 0.8548 - precision_2k: 0.6692 - precision_3k: 0.5362 - precision_5k: 0.3669 - recall_1k: 0.4888 - recall_2k: 0.7052 - recall_3k: 0.8162 - recall_5k: 0.9034 - F1_1k: 0.6217 - F1_2k: 0.6865 - F1_3k: 0.6471 - F1_5k: 0.5218 - accuracy_1k: 0.8548 - accuracy_2k: 0.9247 - accuracy_3k: 0.9550 - accuracy_5k: 0.9766 - hamming_loss_k: 0.0282 - val_loss: 0.0800 - val_precision_1k: 0.7939 - val_precision_2k: 0.6387 - val_precision_3k: 0.5050 - val_precision_5k: 0.3512 - val_recall_1k: 0.4381 - val_recall_2k: 0.6606 - val_recall_3k: 0.7600 - val_recall_5k: 0.8615 - val_F1_1k: 0.5641 - val_F1_2k: 0.6489 - val_F1_3k: 0.6064 - val_F1_5k: 0.4985 - val_accuracy_1k: 0.7939 - val_accuracy_2k: 0.8881 - val_accuracy_3k: 0.9234 - val_accuracy_5k: 0.9639 - val_hamming_loss_k: 0.0309
Epoch 00013: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.0729 - precision_1k: 0.8542 - precision_2k: 0.6764 - precision_3k: 0.5343 - precision_5k: 0.3647 - recall_1k: 0.4771 - recall_2k: 0.6979 - recall_3k: 0.8021 - recall_5k: 0.8841 - F1_1k: 0.6113 - F1_2k: 0.6860 - F1_3k: 0.6404 - F1_5k: 0.5156 - accuracy_1k: 0.8542 - accuracy_2k: 0.9232 - accuracy_3k: 0.9568 - accuracy_5k: 0.9760 - hamming_loss_k: 0.0290
Best model result:  [0.07285986095666885, 0.8542383909225464, 0.6763744354248047, 0.5343307852745056, 0.36465638875961304, 0.47706159949302673, 0.6978821754455566, 0.8021255135536194, 0.8840973973274231, 0.6112679243087769, 0.6859568357467651, 0.6404430270195007, 0.5156164765357971, 0.8542383909225464, 0.923153817653656, 0.9568101763725281, 0.9759768843650818, 0.02900818921625614]
2970
742
1238
Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_4 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_4[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_5 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_5[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3466 - precision_1k: 0.2493 - precision_2k: 0.1928 - precision_3k: 0.1618 - precision_5k: 0.1321 - recall_1k: 0.1339 - recall_2k: 0.1991 - recall_3k: 0.2457 - recall_5k: 0.3262 - F1_1k: 0.1738 - F1_2k: 0.1954 - F1_3k: 0.1948 - F1_5k: 0.1879 - accuracy_1k: 0.2493 - accuracy_2k: 0.3491 - accuracy_3k: 0.4155 - accuracy_5k: 0.5171 - hamming_loss_k: 0.0528
Epoch 00001: val_loss improved from inf to 0.16007, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 3s 107ms/step - loss: 0.3377 - precision_1k: 0.2594 - precision_2k: 0.2052 - precision_3k: 0.1696 - precision_5k: 0.1373 - recall_1k: 0.1380 - recall_2k: 0.2105 - recall_3k: 0.2562 - recall_5k: 0.3368 - F1_1k: 0.1797 - F1_2k: 0.2074 - F1_3k: 0.2038 - F1_5k: 0.1949 - accuracy_1k: 0.2594 - accuracy_2k: 0.3649 - accuracy_3k: 0.4279 - accuracy_5k: 0.5286 - hamming_loss_k: 0.0525 - val_loss: 0.1601 - val_precision_1k: 0.4884 - val_precision_2k: 0.3418 - val_precision_3k: 0.2571 - val_precision_5k: 0.1968 - val_recall_1k: 0.2601 - val_recall_2k: 0.3596 - val_recall_3k: 0.3980 - val_recall_5k: 0.4912 - val_F1_1k: 0.3383 - val_F1_2k: 0.3498 - val_F1_3k: 0.3118 - val_F1_5k: 0.2806 - val_accuracy_1k: 0.4884 - val_accuracy_2k: 0.5959 - val_accuracy_3k: 0.6440 - val_accuracy_5k: 0.7280 - val_hamming_loss_k: 0.0433
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1420 - precision_1k: 0.4851 - precision_2k: 0.3699 - precision_3k: 0.2957 - precision_5k: 0.2158 - recall_1k: 0.2645 - recall_2k: 0.3842 - recall_3k: 0.4480 - recall_5k: 0.5377 - F1_1k: 0.3422 - F1_2k: 0.3768 - F1_3k: 0.3562 - F1_5k: 0.3079 - accuracy_1k: 0.4851 - accuracy_2k: 0.6112 - accuracy_3k: 0.6758 - accuracy_5k: 0.7482 - hamming_loss_k: 0.0432
Epoch 00002: val_loss improved from 0.16007 to 0.12690, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.1417 - precision_1k: 0.4869 - precision_2k: 0.3710 - precision_3k: 0.2968 - precision_5k: 0.2171 - recall_1k: 0.2636 - recall_2k: 0.3841 - recall_3k: 0.4491 - recall_5k: 0.5407 - F1_1k: 0.3418 - F1_2k: 0.3773 - F1_3k: 0.3574 - F1_5k: 0.3097 - accuracy_1k: 0.4869 - accuracy_2k: 0.6111 - accuracy_3k: 0.6774 - accuracy_5k: 0.7508 - hamming_loss_k: 0.0432 - val_loss: 0.1269 - val_precision_1k: 0.5195 - val_precision_2k: 0.3982 - val_precision_3k: 0.3164 - val_precision_5k: 0.2441 - val_recall_1k: 0.2630 - val_recall_2k: 0.3970 - val_recall_3k: 0.4758 - val_recall_5k: 0.5968 - val_F1_1k: 0.3480 - val_F1_2k: 0.3968 - val_F1_3k: 0.3797 - val_F1_5k: 0.3461 - val_accuracy_1k: 0.5195 - val_accuracy_2k: 0.6337 - val_accuracy_3k: 0.7046 - val_accuracy_5k: 0.7877 - val_hamming_loss_k: 0.0420
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1193 - precision_1k: 0.5643 - precision_2k: 0.4474 - precision_3k: 0.3652 - precision_5k: 0.2648 - recall_1k: 0.3036 - recall_2k: 0.4648 - recall_3k: 0.5539 - recall_5k: 0.6573 - F1_1k: 0.3945 - F1_2k: 0.4558 - F1_3k: 0.4401 - F1_5k: 0.3775 - accuracy_1k: 0.5643 - accuracy_2k: 0.7113 - accuracy_3k: 0.7660 - accuracy_5k: 0.8328 - hamming_loss_k: 0.0400
Epoch 00003: val_loss improved from 0.12690 to 0.11343, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 90ms/step - loss: 0.1189 - precision_1k: 0.5676 - precision_2k: 0.4524 - precision_3k: 0.3687 - precision_5k: 0.2663 - recall_1k: 0.3075 - recall_2k: 0.4724 - recall_3k: 0.5626 - recall_5k: 0.6641 - F1_1k: 0.3986 - F1_2k: 0.4620 - F1_3k: 0.4453 - F1_5k: 0.3800 - accuracy_1k: 0.5676 - accuracy_2k: 0.7175 - accuracy_3k: 0.7736 - accuracy_5k: 0.8391 - hamming_loss_k: 0.0399 - val_loss: 0.1134 - val_precision_1k: 0.5870 - val_precision_2k: 0.4868 - val_precision_3k: 0.3977 - val_precision_5k: 0.2821 - val_recall_1k: 0.3117 - val_recall_2k: 0.5047 - val_recall_3k: 0.6100 - val_recall_5k: 0.7065 - val_F1_1k: 0.4068 - val_F1_2k: 0.4954 - val_F1_3k: 0.4813 - val_F1_5k: 0.4031 - val_accuracy_1k: 0.5870 - val_accuracy_2k: 0.7498 - val_accuracy_3k: 0.8169 - val_accuracy_5k: 0.8779 - val_hamming_loss_k: 0.0393
Epoch 4/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1071 - precision_1k: 0.6335 - precision_2k: 0.5089 - precision_3k: 0.4097 - precision_5k: 0.2977 - recall_1k: 0.3439 - recall_2k: 0.5270 - recall_3k: 0.6270 - recall_5k: 0.7397 - F1_1k: 0.4457 - F1_2k: 0.5177 - F1_3k: 0.4955 - F1_5k: 0.4244 - accuracy_1k: 0.6335 - accuracy_2k: 0.7670 - accuracy_3k: 0.8292 - accuracy_5k: 0.8856 - hamming_loss_k: 0.0373
Epoch 00004: val_loss improved from 0.11343 to 0.10228, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.1065 - precision_1k: 0.6386 - precision_2k: 0.5117 - precision_3k: 0.4107 - precision_5k: 0.2984 - recall_1k: 0.3498 - recall_2k: 0.5330 - recall_3k: 0.6308 - recall_5k: 0.7442 - F1_1k: 0.4518 - F1_2k: 0.5220 - F1_3k: 0.4974 - F1_5k: 0.4259 - accuracy_1k: 0.6386 - accuracy_2k: 0.7713 - accuracy_3k: 0.8331 - accuracy_5k: 0.8898 - hamming_loss_k: 0.0370 - val_loss: 0.1023 - val_precision_1k: 0.6684 - val_precision_2k: 0.5480 - val_precision_3k: 0.4343 - val_precision_5k: 0.3030 - val_recall_1k: 0.3646 - val_recall_2k: 0.5758 - val_recall_3k: 0.6689 - val_recall_5k: 0.7608 - val_F1_1k: 0.4716 - val_F1_2k: 0.5613 - val_F1_3k: 0.5264 - val_F1_5k: 0.4332 - val_accuracy_1k: 0.6684 - val_accuracy_2k: 0.8220 - val_accuracy_3k: 0.8671 - val_accuracy_5k: 0.9121 - val_hamming_loss_k: 0.0359
Epoch 5/100
12/12 [==============================] - ETA: 0s - loss: 0.0968 - precision_1k: 0.6955 - precision_2k: 0.5591 - precision_3k: 0.4477 - precision_5k: 0.3174 - recall_1k: 0.3890 - recall_2k: 0.5885 - recall_3k: 0.6901 - recall_5k: 0.7908 - F1_1k: 0.4989 - F1_2k: 0.5734 - F1_3k: 0.5430 - F1_5k: 0.4529 - accuracy_1k: 0.6955 - accuracy_2k: 0.8207 - accuracy_3k: 0.8757 - accuracy_5k: 0.9228 - hamming_loss_k: 0.0346
Epoch 00005: val_loss improved from 0.10228 to 0.09514, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.0968 - precision_1k: 0.6955 - precision_2k: 0.5591 - precision_3k: 0.4477 - precision_5k: 0.3174 - recall_1k: 0.3890 - recall_2k: 0.5885 - recall_3k: 0.6901 - recall_5k: 0.7908 - F1_1k: 0.4989 - F1_2k: 0.5734 - F1_3k: 0.5430 - F1_5k: 0.4529 - accuracy_1k: 0.6955 - accuracy_2k: 0.8207 - accuracy_3k: 0.8757 - accuracy_5k: 0.9228 - hamming_loss_k: 0.0346 - val_loss: 0.0951 - val_precision_1k: 0.6980 - val_precision_2k: 0.5578 - val_precision_3k: 0.4477 - val_precision_5k: 0.3183 - val_recall_1k: 0.3818 - val_recall_2k: 0.5792 - val_recall_3k: 0.6855 - val_recall_5k: 0.7899 - val_F1_1k: 0.4933 - val_F1_2k: 0.5680 - val_F1_3k: 0.5413 - val_F1_5k: 0.4536 - val_accuracy_1k: 0.6980 - val_accuracy_2k: 0.8028 - val_accuracy_3k: 0.8627 - val_accuracy_5k: 0.9282 - val_hamming_loss_k: 0.0347
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0891 - precision_1k: 0.7379 - precision_2k: 0.5893 - precision_3k: 0.4694 - precision_5k: 0.3285 - recall_1k: 0.4145 - recall_2k: 0.6150 - recall_3k: 0.7132 - recall_5k: 0.8127 - F1_1k: 0.5306 - F1_2k: 0.6018 - F1_3k: 0.5660 - F1_5k: 0.4678 - accuracy_1k: 0.7379 - accuracy_2k: 0.8466 - accuracy_3k: 0.8910 - accuracy_5k: 0.9325 - hamming_loss_k: 0.0329
Epoch 00006: val_loss improved from 0.09514 to 0.08804, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.0889 - precision_1k: 0.7408 - precision_2k: 0.5924 - precision_3k: 0.4721 - precision_5k: 0.3304 - recall_1k: 0.4164 - recall_2k: 0.6186 - recall_3k: 0.7174 - recall_5k: 0.8180 - F1_1k: 0.5329 - F1_2k: 0.6051 - F1_3k: 0.5693 - F1_5k: 0.4706 - accuracy_1k: 0.7408 - accuracy_2k: 0.8513 - accuracy_3k: 0.8952 - accuracy_5k: 0.9360 - hamming_loss_k: 0.0328 - val_loss: 0.0880 - val_precision_1k: 0.7332 - val_precision_2k: 0.5969 - val_precision_3k: 0.4770 - val_precision_5k: 0.3326 - val_recall_1k: 0.4084 - val_recall_2k: 0.6258 - val_recall_3k: 0.7285 - val_recall_5k: 0.8226 - val_F1_1k: 0.5244 - val_F1_2k: 0.6109 - val_F1_3k: 0.5763 - val_F1_5k: 0.4734 - val_accuracy_1k: 0.7332 - val_accuracy_2k: 0.8577 - val_accuracy_3k: 0.9000 - val_accuracy_5k: 0.9404 - val_hamming_loss_k: 0.0333
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0821 - precision_1k: 0.7582 - precision_2k: 0.6062 - precision_3k: 0.4896 - precision_5k: 0.3437 - recall_1k: 0.4303 - recall_2k: 0.6358 - recall_3k: 0.7453 - recall_5k: 0.8495 - F1_1k: 0.5488 - F1_2k: 0.6205 - F1_3k: 0.5908 - F1_5k: 0.4893 - accuracy_1k: 0.7582 - accuracy_2k: 0.8651 - accuracy_3k: 0.9126 - accuracy_5k: 0.9510 - hamming_loss_k: 0.0321
Epoch 00007: val_loss improved from 0.08804 to 0.08652, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.0820 - precision_1k: 0.7561 - precision_2k: 0.6065 - precision_3k: 0.4899 - precision_5k: 0.3437 - recall_1k: 0.4275 - recall_2k: 0.6344 - recall_3k: 0.7434 - recall_5k: 0.8475 - F1_1k: 0.5460 - F1_2k: 0.6200 - F1_3k: 0.5904 - F1_5k: 0.4890 - accuracy_1k: 0.7561 - accuracy_2k: 0.8628 - accuracy_3k: 0.9102 - accuracy_5k: 0.9480 - hamming_loss_k: 0.0322 - val_loss: 0.0865 - val_precision_1k: 0.7305 - val_precision_2k: 0.5922 - val_precision_3k: 0.4664 - val_precision_5k: 0.3344 - val_recall_1k: 0.4029 - val_recall_2k: 0.6155 - val_recall_3k: 0.7083 - val_recall_5k: 0.8225 - val_F1_1k: 0.5190 - val_F1_2k: 0.6032 - val_F1_3k: 0.5620 - val_F1_5k: 0.4752 - val_accuracy_1k: 0.7305 - val_accuracy_2k: 0.8418 - val_accuracy_3k: 0.8853 - val_accuracy_5k: 0.9360 - val_hamming_loss_k: 0.0334
Epoch 8/100
12/12 [==============================] - ETA: 0s - loss: 0.0782 - precision_1k: 0.7892 - precision_2k: 0.6253 - precision_3k: 0.5011 - precision_5k: 0.3499 - recall_1k: 0.4452 - recall_2k: 0.6524 - recall_3k: 0.7618 - recall_5k: 0.8606 - F1_1k: 0.5691 - F1_2k: 0.6384 - F1_3k: 0.6044 - F1_5k: 0.4974 - accuracy_1k: 0.7892 - accuracy_2k: 0.8802 - accuracy_3k: 0.9265 - accuracy_5k: 0.9571 - hamming_loss_k: 0.0310 ETA: 0s - loss: 0.0762 - precision_1k: 0.7968 - precision_2k: 0.6270 - precision_3k: 0.4967 - precision_5k: 0.3480 - recall_1k: 0.4489 - recall_2k: 0.6599 - recall_3k: 0.7569 - recall_5k: 0.8662 - F1_1k: 0.5742 - F1_2k: 0.6430 - F1_3k: 0.5998 - F1_5k: 0.4966 - accuracy_1k: 0.7968 - accuracy_2k: 0.8848 - accuracy_3k: 0.9199 - accuracy_5k: 0.9590 - hamming_
Epoch 00008: val_loss improved from 0.08652 to 0.08398, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.0782 - precision_1k: 0.7892 - precision_2k: 0.6253 - precision_3k: 0.5011 - precision_5k: 0.3499 - recall_1k: 0.4452 - recall_2k: 0.6524 - recall_3k: 0.7618 - recall_5k: 0.8606 - F1_1k: 0.5691 - F1_2k: 0.6384 - F1_3k: 0.6044 - F1_5k: 0.4974 - accuracy_1k: 0.7892 - accuracy_2k: 0.8802 - accuracy_3k: 0.9265 - accuracy_5k: 0.9571 - hamming_loss_k: 0.0310 - val_loss: 0.0840 - val_precision_1k: 0.7634 - val_precision_2k: 0.6121 - val_precision_3k: 0.4880 - val_precision_5k: 0.3355 - val_recall_1k: 0.4330 - val_recall_2k: 0.6427 - val_recall_3k: 0.7429 - val_recall_5k: 0.8255 - val_F1_1k: 0.5525 - val_F1_2k: 0.6269 - val_F1_3k: 0.5888 - val_F1_5k: 0.4769 - val_accuracy_1k: 0.7634 - val_accuracy_2k: 0.8754 - val_accuracy_3k: 0.9162 - val_accuracy_5k: 0.9367 - val_hamming_loss_k: 0.0321
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0739 - precision_1k: 0.8079 - precision_2k: 0.6458 - precision_3k: 0.5153 - precision_5k: 0.3559 - recall_1k: 0.4587 - recall_2k: 0.6776 - recall_3k: 0.7857 - recall_5k: 0.8753 - F1_1k: 0.5850 - F1_2k: 0.6612 - F1_3k: 0.6223 - F1_5k: 0.5060 - accuracy_1k: 0.8079 - accuracy_2k: 0.9013 - accuracy_3k: 0.9407 - accuracy_5k: 0.9624 - hamming_loss_k: 0.0301
Epoch 00009: val_loss improved from 0.08398 to 0.08161, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 103ms/step - loss: 0.0739 - precision_1k: 0.8071 - precision_2k: 0.6447 - precision_3k: 0.5149 - precision_5k: 0.3556 - recall_1k: 0.4586 - recall_2k: 0.6758 - recall_3k: 0.7848 - recall_5k: 0.8756 - F1_1k: 0.5847 - F1_2k: 0.6598 - F1_3k: 0.6218 - F1_5k: 0.5057 - accuracy_1k: 0.8071 - accuracy_2k: 0.8965 - accuracy_3k: 0.9386 - accuracy_5k: 0.9617 - hamming_loss_k: 0.0301 - val_loss: 0.0816 - val_precision_1k: 0.7847 - val_precision_2k: 0.6233 - val_precision_3k: 0.4925 - val_precision_5k: 0.3438 - val_recall_1k: 0.4400 - val_recall_2k: 0.6478 - val_recall_3k: 0.7449 - val_recall_5k: 0.8412 - val_F1_1k: 0.5636 - val_F1_2k: 0.6349 - val_F1_3k: 0.5926 - val_F1_5k: 0.4878 - val_accuracy_1k: 0.7847 - val_accuracy_2k: 0.8736 - val_accuracy_3k: 0.9053 - val_accuracy_5k: 0.9404 - val_hamming_loss_k: 0.0312
Epoch 10/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0704 - precision_1k: 0.8214 - precision_2k: 0.6564 - precision_3k: 0.5234 - precision_5k: 0.3624 - recall_1k: 0.4700 - recall_2k: 0.6905 - recall_3k: 0.7968 - recall_5k: 0.8919 - F1_1k: 0.5976 - F1_2k: 0.6729 - F1_3k: 0.6317 - F1_5k: 0.5153 - accuracy_1k: 0.8214 - accuracy_2k: 0.9119 - accuracy_3k: 0.9439 - accuracy_5k: 0.9702 - hamming_loss_k: 0.0296
Epoch 00010: val_loss improved from 0.08161 to 0.07994, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.0703 - precision_1k: 0.8217 - precision_2k: 0.6564 - precision_3k: 0.5224 - precision_5k: 0.3610 - recall_1k: 0.4713 - recall_2k: 0.6921 - recall_3k: 0.7973 - recall_5k: 0.8913 - F1_1k: 0.5988 - F1_2k: 0.6736 - F1_3k: 0.6311 - F1_5k: 0.5138 - accuracy_1k: 0.8217 - accuracy_2k: 0.9128 - accuracy_3k: 0.9442 - accuracy_5k: 0.9700 - hamming_loss_k: 0.0295 - val_loss: 0.0799 - val_precision_1k: 0.7824 - val_precision_2k: 0.6161 - val_precision_3k: 0.4978 - val_precision_5k: 0.3433 - val_recall_1k: 0.4391 - val_recall_2k: 0.6463 - val_recall_3k: 0.7582 - val_recall_5k: 0.8435 - val_F1_1k: 0.5623 - val_F1_2k: 0.6306 - val_F1_3k: 0.6007 - val_F1_5k: 0.4878 - val_accuracy_1k: 0.7824 - val_accuracy_2k: 0.8779 - val_accuracy_3k: 0.9214 - val_accuracy_5k: 0.9462 - val_hamming_loss_k: 0.0313
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0682 - precision_1k: 0.8292 - precision_2k: 0.6614 - precision_3k: 0.5308 - precision_5k: 0.3634 - recall_1k: 0.4741 - recall_2k: 0.6948 - recall_3k: 0.8081 - recall_5k: 0.8953 - F1_1k: 0.6031 - F1_2k: 0.6775 - F1_3k: 0.6406 - F1_5k: 0.5169 - accuracy_1k: 0.8292 - accuracy_2k: 0.9094 - accuracy_3k: 0.9432 - accuracy_5k: 0.9691 - hamming_loss_k: 0.0291
Epoch 00011: val_loss improved from 0.07994 to 0.07759, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.0687 - precision_1k: 0.8288 - precision_2k: 0.6628 - precision_3k: 0.5320 - precision_5k: 0.3638 - recall_1k: 0.4738 - recall_2k: 0.6943 - recall_3k: 0.8075 - recall_5k: 0.8931 - F1_1k: 0.6028 - F1_2k: 0.6781 - F1_3k: 0.6413 - F1_5k: 0.5169 - accuracy_1k: 0.8288 - accuracy_2k: 0.9100 - accuracy_3k: 0.9436 - accuracy_5k: 0.9690 - hamming_loss_k: 0.0293 - val_loss: 0.0776 - val_precision_1k: 0.7928 - val_precision_2k: 0.6329 - val_precision_3k: 0.5025 - val_precision_5k: 0.3462 - val_recall_1k: 0.4519 - val_recall_2k: 0.6651 - val_recall_3k: 0.7639 - val_recall_5k: 0.8531 - val_F1_1k: 0.5755 - val_F1_2k: 0.6483 - val_F1_3k: 0.6059 - val_F1_5k: 0.4923 - val_accuracy_1k: 0.7928 - val_accuracy_2k: 0.8956 - val_accuracy_3k: 0.9288 - val_accuracy_5k: 0.9530 - val_hamming_loss_k: 0.0309
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0655 - precision_1k: 0.8438 - precision_2k: 0.6733 - precision_3k: 0.5342 - precision_5k: 0.3681 - recall_1k: 0.4832 - recall_2k: 0.7061 - recall_3k: 0.8127 - recall_5k: 0.9066 - F1_1k: 0.6144 - F1_2k: 0.6892 - F1_3k: 0.6445 - F1_5k: 0.5234 - accuracy_1k: 0.8438 - accuracy_2k: 0.9201 - accuracy_3k: 0.9528 - accuracy_5k: 0.9776 - hamming_loss_k: 0.0285
Epoch 00012: val_loss did not improve from 0.07759
12/12 [==============================] - 1s 63ms/step - loss: 0.0656 - precision_1k: 0.8449 - precision_2k: 0.6748 - precision_3k: 0.5373 - precision_5k: 0.3692 - recall_1k: 0.4815 - recall_2k: 0.7055 - recall_3k: 0.8152 - recall_5k: 0.9068 - F1_1k: 0.6133 - F1_2k: 0.6897 - F1_3k: 0.6476 - F1_5k: 0.5246 - accuracy_1k: 0.8449 - accuracy_2k: 0.9197 - accuracy_3k: 0.9540 - accuracy_5k: 0.9779 - hamming_loss_k: 0.0286 - val_loss: 0.0790 - val_precision_1k: 0.8158 - val_precision_2k: 0.6307 - val_precision_3k: 0.4968 - val_precision_5k: 0.3450 - val_recall_1k: 0.4572 - val_recall_2k: 0.6539 - val_recall_3k: 0.7549 - val_recall_5k: 0.8552 - val_F1_1k: 0.5854 - val_F1_2k: 0.6416 - val_F1_3k: 0.5988 - val_F1_5k: 0.4914 - val_accuracy_1k: 0.8158 - val_accuracy_2k: 0.8827 - val_accuracy_3k: 0.9279 - val_accuracy_5k: 0.9634 - val_hamming_loss_k: 0.0299
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.0656 - precision_1k: 0.8413 - precision_2k: 0.6730 - precision_3k: 0.5391 - precision_5k: 0.3690 - recall_1k: 0.4815 - recall_2k: 0.7069 - recall_3k: 0.8184 - recall_5k: 0.9080 - F1_1k: 0.6123 - F1_2k: 0.6895 - F1_3k: 0.6499 - F1_5k: 0.5247 - accuracy_1k: 0.8413 - accuracy_2k: 0.9204 - accuracy_3k: 0.9532 - accuracy_5k: 0.9786 - hamming_loss_k: 0.0287
Epoch 00013: val_loss improved from 0.07759 to 0.07576, saving model to logs/pdzwki-lab-0604-114603/model/checkpoint_lab.h5
12/12 [==============================] - 1s 87ms/step - loss: 0.0656 - precision_1k: 0.8413 - precision_2k: 0.6730 - precision_3k: 0.5391 - precision_5k: 0.3690 - recall_1k: 0.4815 - recall_2k: 0.7069 - recall_3k: 0.8184 - recall_5k: 0.9080 - F1_1k: 0.6123 - F1_2k: 0.6895 - F1_3k: 0.6499 - F1_5k: 0.5247 - accuracy_1k: 0.8413 - accuracy_2k: 0.9204 - accuracy_3k: 0.9532 - accuracy_5k: 0.9786 - hamming_loss_k: 0.0287 - val_loss: 0.0758 - val_precision_1k: 0.8150 - val_precision_2k: 0.6464 - val_precision_3k: 0.5118 - val_precision_5k: 0.3480 - val_recall_1k: 0.4617 - val_recall_2k: 0.6769 - val_recall_3k: 0.7793 - val_recall_5k: 0.8565 - val_F1_1k: 0.5891 - val_F1_2k: 0.6610 - val_F1_3k: 0.6176 - val_F1_5k: 0.4947 - val_accuracy_1k: 0.8150 - val_accuracy_2k: 0.8955 - val_accuracy_3k: 0.9395 - val_accuracy_5k: 0.9583 - val_hamming_loss_k: 0.0300
Epoch 14/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0632 - precision_1k: 0.8619 - precision_2k: 0.6836 - precision_3k: 0.5472 - precision_5k: 0.3716 - recall_1k: 0.4962 - recall_2k: 0.7218 - recall_3k: 0.8334 - recall_5k: 0.9125 - F1_1k: 0.6297 - F1_2k: 0.7020 - F1_3k: 0.6605 - F1_5k: 0.5280 - accuracy_1k: 0.8619 - accuracy_2k: 0.9308 - accuracy_3k: 0.9620 - accuracy_5k: 0.9798 - hamming_loss_k: 0.0279
Epoch 00014: val_loss did not improve from 0.07576
12/12 [==============================] - 1s 60ms/step - loss: 0.0633 - precision_1k: 0.8593 - precision_2k: 0.6826 - precision_3k: 0.5471 - precision_5k: 0.3714 - recall_1k: 0.4923 - recall_2k: 0.7189 - recall_3k: 0.8323 - recall_5k: 0.9112 - F1_1k: 0.6258 - F1_2k: 0.7001 - F1_3k: 0.6601 - F1_5k: 0.5276 - accuracy_1k: 0.8593 - accuracy_2k: 0.9279 - accuracy_3k: 0.9597 - accuracy_5k: 0.9788 - hamming_loss_k: 0.0280 - val_loss: 0.0786 - val_precision_1k: 0.7983 - val_precision_2k: 0.6379 - val_precision_3k: 0.5059 - val_precision_5k: 0.3451 - val_recall_1k: 0.4501 - val_recall_2k: 0.6625 - val_recall_3k: 0.7669 - val_recall_5k: 0.8523 - val_F1_1k: 0.5754 - val_F1_2k: 0.6497 - val_F1_3k: 0.6091 - val_F1_5k: 0.4910 - val_accuracy_1k: 0.7983 - val_accuracy_2k: 0.9011 - val_accuracy_3k: 0.9360 - val_accuracy_5k: 0.9563 - val_hamming_loss_k: 0.0306
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0608 - precision_1k: 0.8690 - precision_2k: 0.6896 - precision_3k: 0.5508 - precision_5k: 0.3741 - recall_1k: 0.4992 - recall_2k: 0.7280 - recall_3k: 0.8378 - recall_5k: 0.9224 - F1_1k: 0.6340 - F1_2k: 0.7081 - F1_3k: 0.6645 - F1_5k: 0.5322 - accuracy_1k: 0.8690 - accuracy_2k: 0.9347 - accuracy_3k: 0.9627 - accuracy_5k: 0.9851 - hamming_loss_k: 0.0275
Epoch 00015: val_loss did not improve from 0.07576
12/12 [==============================] - 1s 67ms/step - loss: 0.0610 - precision_1k: 0.8680 - precision_2k: 0.6922 - precision_3k: 0.5519 - precision_5k: 0.3751 - recall_1k: 0.4977 - recall_2k: 0.7287 - recall_3k: 0.8377 - recall_5k: 0.9226 - F1_1k: 0.6325 - F1_2k: 0.7098 - F1_3k: 0.6653 - F1_5k: 0.5333 - accuracy_1k: 0.8680 - accuracy_2k: 0.9347 - accuracy_3k: 0.9631 - accuracy_5k: 0.9858 - hamming_loss_k: 0.0277 - val_loss: 0.0775 - val_precision_1k: 0.8245 - val_precision_2k: 0.6508 - val_precision_3k: 0.5109 - val_precision_5k: 0.3487 - val_recall_1k: 0.4667 - val_recall_2k: 0.6780 - val_recall_3k: 0.7756 - val_recall_5k: 0.8593 - val_F1_1k: 0.5957 - val_F1_2k: 0.6638 - val_F1_3k: 0.6156 - val_F1_5k: 0.4959 - val_accuracy_1k: 0.8245 - val_accuracy_2k: 0.9039 - val_accuracy_3k: 0.9338 - val_accuracy_5k: 0.9554 - val_hamming_loss_k: 0.0296
Epoch 00015: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.0692 - precision_1k: 0.8647 - precision_2k: 0.6852 - precision_3k: 0.5466 - precision_5k: 0.3706 - recall_1k: 0.4872 - recall_2k: 0.7126 - recall_3k: 0.8209 - recall_5k: 0.9010 - F1_1k: 0.6222 - F1_2k: 0.6975 - F1_3k: 0.6552 - F1_5k: 0.5244 - accuracy_1k: 0.8647 - accuracy_2k: 0.9331 - accuracy_3k: 0.9676 - accuracy_5k: 0.9832 - hamming_loss_k: 0.0286
Best model result:  [0.06916557997465134, 0.864730715751648, 0.6851717829704285, 0.5466128587722778, 0.370574414730072, 0.4871896505355835, 0.7125566005706787, 0.8208998441696167, 0.9009794592857361, 0.6221625208854675, 0.6975078582763672, 0.6551990509033203, 0.524406909942627, 0.864730715751648, 0.9331230521202087, 0.9675948023796082, 0.9831871390342712, 0.02858004905283451]
2970
742
1238
Model: "model_3"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_6 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_6[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_7 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_7[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Epoch 1/100
12/12 [==============================] - ETA: 0s - loss: 0.3365 - precision_1k: 0.2355 - precision_2k: 0.1859 - precision_3k: 0.1521 - precision_5k: 0.1234 - recall_1k: 0.1235 - recall_2k: 0.1913 - recall_3k: 0.2348 - recall_5k: 0.3120 - F1_1k: 0.1614 - F1_2k: 0.1883 - F1_3k: 0.1843 - F1_5k: 0.1768 - accuracy_1k: 0.2355 - accuracy_2k: 0.3498 - accuracy_3k: 0.4082 - accuracy_5k: 0.5052 - hamming_loss_k: 0.0534 ETA: 0s - loss: 0.4158 - precision_1k: 0.1738 - precision_2k: 0.1364 - precision_3k: 0.1172 - precision_5k: 0.1018 - recall_1k: 0.0906 - recall_2k: 0.1396 - recall_3k: 0.1831 - recall_5k: 0.2580 - F1_1k: 0.1187 - F1_2k: 0.1377 - F1_3k: 0.1427 - F1_5k: 0.1459 - accuracy_1k: 0.1738 - accuracy_2k: 0.2585 - accuracy_3k: 0.3177 - accuracy_5k: 0.4167 - hamming_loss_k
Epoch 00001: val_loss improved from inf to 0.17439, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 3s 103ms/step - loss: 0.3365 - precision_1k: 0.2355 - precision_2k: 0.1859 - precision_3k: 0.1521 - precision_5k: 0.1234 - recall_1k: 0.1235 - recall_2k: 0.1913 - recall_3k: 0.2348 - recall_5k: 0.3120 - F1_1k: 0.1614 - F1_2k: 0.1883 - F1_3k: 0.1843 - F1_5k: 0.1768 - accuracy_1k: 0.2355 - accuracy_2k: 0.3498 - accuracy_3k: 0.4082 - accuracy_5k: 0.5052 - hamming_loss_k: 0.0534 - val_loss: 0.1744 - val_precision_1k: 0.4202 - val_precision_2k: 0.3306 - val_precision_3k: 0.2667 - val_precision_5k: 0.1910 - val_recall_1k: 0.2299 - val_recall_2k: 0.3477 - val_recall_3k: 0.4136 - val_recall_5k: 0.4874 - val_F1_1k: 0.2971 - val_F1_2k: 0.3387 - val_F1_3k: 0.3240 - val_F1_5k: 0.2742 - val_accuracy_1k: 0.4202 - val_accuracy_2k: 0.5890 - val_accuracy_3k: 0.6578 - val_accuracy_5k: 0.7412 - val_hamming_loss_k: 0.0465
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1446 - precision_1k: 0.3974 - precision_2k: 0.3248 - precision_3k: 0.2826 - precision_5k: 0.2168 - recall_1k: 0.2027 - recall_2k: 0.3284 - recall_3k: 0.4241 - recall_5k: 0.5365 - F1_1k: 0.2681 - F1_2k: 0.3264 - F1_3k: 0.3390 - F1_5k: 0.3087 - accuracy_1k: 0.3974 - accuracy_2k: 0.5575 - accuracy_3k: 0.6449 - accuracy_5k: 0.7408 - hamming_loss_k: 0.0469
Epoch 00002: val_loss improved from 0.17439 to 0.13465, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 89ms/step - loss: 0.1437 - precision_1k: 0.4043 - precision_2k: 0.3253 - precision_3k: 0.2801 - precision_5k: 0.2158 - recall_1k: 0.2089 - recall_2k: 0.3342 - recall_3k: 0.4261 - recall_5k: 0.5392 - F1_1k: 0.2751 - F1_2k: 0.3293 - F1_3k: 0.3378 - F1_5k: 0.3081 - accuracy_1k: 0.4043 - accuracy_2k: 0.5603 - accuracy_3k: 0.6442 - accuracy_5k: 0.7418 - hamming_loss_k: 0.0464 - val_loss: 0.1347 - val_precision_1k: 0.4898 - val_precision_2k: 0.3782 - val_precision_3k: 0.3057 - val_precision_5k: 0.2285 - val_recall_1k: 0.2720 - val_recall_2k: 0.4030 - val_recall_3k: 0.4702 - val_recall_5k: 0.5740 - val_F1_1k: 0.3494 - val_F1_2k: 0.3898 - val_F1_3k: 0.3700 - val_F1_5k: 0.3265 - val_accuracy_1k: 0.4898 - val_accuracy_2k: 0.6618 - val_accuracy_3k: 0.6980 - val_accuracy_5k: 0.7879 - val_hamming_loss_k: 0.0437
Epoch 3/100
12/12 [==============================] - ETA: 0s - loss: 0.1242 - precision_1k: 0.5365 - precision_2k: 0.4309 - precision_3k: 0.3456 - precision_5k: 0.2482 - recall_1k: 0.2899 - recall_2k: 0.4499 - recall_3k: 0.5358 - recall_5k: 0.6294 - F1_1k: 0.3760 - F1_2k: 0.4398 - F1_3k: 0.4199 - F1_5k: 0.3558 - accuracy_1k: 0.5365 - accuracy_2k: 0.6821 - accuracy_3k: 0.7483 - accuracy_5k: 0.8220 - hamming_loss_k: 0.0412
Epoch 00003: val_loss improved from 0.13465 to 0.12119, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.1242 - precision_1k: 0.5365 - precision_2k: 0.4309 - precision_3k: 0.3456 - precision_5k: 0.2482 - recall_1k: 0.2899 - recall_2k: 0.4499 - recall_3k: 0.5358 - recall_5k: 0.6294 - F1_1k: 0.3760 - F1_2k: 0.4398 - F1_3k: 0.4199 - F1_5k: 0.3558 - accuracy_1k: 0.5365 - accuracy_2k: 0.6821 - accuracy_3k: 0.7483 - accuracy_5k: 0.8220 - hamming_loss_k: 0.0412 - val_loss: 0.1212 - val_precision_1k: 0.5309 - val_precision_2k: 0.4223 - val_precision_3k: 0.3482 - val_precision_5k: 0.2641 - val_recall_1k: 0.2746 - val_recall_2k: 0.4227 - val_recall_3k: 0.5157 - val_recall_5k: 0.6533 - val_F1_1k: 0.3606 - val_F1_2k: 0.4209 - val_F1_3k: 0.4144 - val_F1_5k: 0.3756 - val_accuracy_1k: 0.5309 - val_accuracy_2k: 0.6678 - val_accuracy_3k: 0.7189 - val_accuracy_5k: 0.8464 - val_hamming_loss_k: 0.0420
Epoch 4/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1104 - precision_1k: 0.6133 - precision_2k: 0.4865 - precision_3k: 0.3896 - precision_5k: 0.2831 - recall_1k: 0.3385 - recall_2k: 0.5120 - recall_3k: 0.6006 - recall_5k: 0.7107 - F1_1k: 0.4361 - F1_2k: 0.4987 - F1_3k: 0.4725 - F1_5k: 0.4048 - accuracy_1k: 0.6133 - accuracy_2k: 0.7511 - accuracy_3k: 0.8072 - accuracy_5k: 0.8796 - hamming_loss_k: 0.0378
Epoch 00004: val_loss improved from 0.12119 to 0.10983, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.1108 - precision_1k: 0.6179 - precision_2k: 0.4895 - precision_3k: 0.3908 - precision_5k: 0.2843 - recall_1k: 0.3401 - recall_2k: 0.5159 - recall_3k: 0.6029 - recall_5k: 0.7131 - F1_1k: 0.4385 - F1_2k: 0.5022 - F1_3k: 0.4741 - F1_5k: 0.4065 - accuracy_1k: 0.6179 - accuracy_2k: 0.7572 - accuracy_3k: 0.8103 - accuracy_5k: 0.8815 - hamming_loss_k: 0.0378 - val_loss: 0.1098 - val_precision_1k: 0.6215 - val_precision_2k: 0.5008 - val_precision_3k: 0.4081 - val_precision_5k: 0.2933 - val_recall_1k: 0.3349 - val_recall_2k: 0.5130 - val_recall_3k: 0.6184 - val_recall_5k: 0.7239 - val_F1_1k: 0.4341 - val_F1_2k: 0.5060 - val_F1_3k: 0.4912 - val_F1_5k: 0.4172 - val_accuracy_1k: 0.6215 - val_accuracy_2k: 0.7502 - val_accuracy_3k: 0.8170 - val_accuracy_5k: 0.8769 - val_hamming_loss_k: 0.0383
Epoch 5/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1009 - precision_1k: 0.6712 - precision_2k: 0.5391 - precision_3k: 0.4314 - precision_5k: 0.3095 - recall_1k: 0.3754 - recall_2k: 0.5719 - recall_3k: 0.6704 - recall_5k: 0.7766 - F1_1k: 0.4814 - F1_2k: 0.5548 - F1_3k: 0.5248 - F1_5k: 0.4425 - accuracy_1k: 0.6712 - accuracy_2k: 0.8061 - accuracy_3k: 0.8665 - accuracy_5k: 0.9187 - hamming_loss_k: 0.0354
Epoch 00005: val_loss improved from 0.10983 to 0.10256, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.1008 - precision_1k: 0.6721 - precision_2k: 0.5431 - precision_3k: 0.4346 - precision_5k: 0.3121 - recall_1k: 0.3744 - recall_2k: 0.5737 - recall_3k: 0.6717 - recall_5k: 0.7789 - F1_1k: 0.4808 - F1_2k: 0.5578 - F1_3k: 0.5275 - F1_5k: 0.4455 - accuracy_1k: 0.6721 - accuracy_2k: 0.8098 - accuracy_3k: 0.8695 - accuracy_5k: 0.9211 - hamming_loss_k: 0.0356 - val_loss: 0.1026 - val_precision_1k: 0.6708 - val_precision_2k: 0.5337 - val_precision_3k: 0.4289 - val_precision_5k: 0.3067 - val_recall_1k: 0.3649 - val_recall_2k: 0.5448 - val_recall_3k: 0.6457 - val_recall_5k: 0.7525 - val_F1_1k: 0.4714 - val_F1_2k: 0.5382 - val_F1_3k: 0.5148 - val_F1_5k: 0.4354 - val_accuracy_1k: 0.6708 - val_accuracy_2k: 0.7764 - val_accuracy_3k: 0.8345 - val_accuracy_5k: 0.8986 - val_hamming_loss_k: 0.0363
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0939 - precision_1k: 0.7166 - precision_2k: 0.5732 - precision_3k: 0.4583 - precision_5k: 0.3244 - recall_1k: 0.4015 - recall_2k: 0.6014 - recall_3k: 0.7025 - recall_5k: 0.8052 - F1_1k: 0.5146 - F1_2k: 0.5869 - F1_3k: 0.5547 - F1_5k: 0.4624 - accuracy_1k: 0.7166 - accuracy_2k: 0.8317 - accuracy_3k: 0.8920 - accuracy_5k: 0.9343 - hamming_loss_k: 0.0339
Epoch 00006: val_loss improved from 0.10256 to 0.09722, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.0934 - precision_1k: 0.7153 - precision_2k: 0.5725 - precision_3k: 0.4593 - precision_5k: 0.3237 - recall_1k: 0.4018 - recall_2k: 0.6028 - recall_3k: 0.7077 - recall_5k: 0.8070 - F1_1k: 0.5145 - F1_2k: 0.5872 - F1_3k: 0.5570 - F1_5k: 0.4620 - accuracy_1k: 0.7153 - accuracy_2k: 0.8311 - accuracy_3k: 0.8940 - accuracy_5k: 0.9338 - hamming_loss_k: 0.0337 - val_loss: 0.0972 - val_precision_1k: 0.6867 - val_precision_2k: 0.5573 - val_precision_3k: 0.4473 - val_precision_5k: 0.3209 - val_recall_1k: 0.3760 - val_recall_2k: 0.5670 - val_recall_3k: 0.6739 - val_recall_5k: 0.7819 - val_F1_1k: 0.4849 - val_F1_2k: 0.5611 - val_F1_3k: 0.5371 - val_F1_5k: 0.4545 - val_accuracy_1k: 0.6867 - val_accuracy_2k: 0.7871 - val_accuracy_3k: 0.8519 - val_accuracy_5k: 0.9091 - val_hamming_loss_k: 0.0356
Epoch 7/100
12/12 [==============================] - ETA: 0s - loss: 0.0858 - precision_1k: 0.7658 - precision_2k: 0.6031 - precision_3k: 0.4779 - precision_5k: 0.3359 - recall_1k: 0.4357 - recall_2k: 0.6357 - recall_3k: 0.7366 - recall_5k: 0.8357 - F1_1k: 0.5554 - F1_2k: 0.6189 - F1_3k: 0.5796 - F1_5k: 0.4791 - accuracy_1k: 0.7658 - accuracy_2k: 0.8695 - accuracy_3k: 0.9153 - accuracy_5k: 0.9483 - hamming_loss_k: 0.0317
Epoch 00007: val_loss improved from 0.09722 to 0.09414, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 86ms/step - loss: 0.0858 - precision_1k: 0.7658 - precision_2k: 0.6031 - precision_3k: 0.4779 - precision_5k: 0.3359 - recall_1k: 0.4357 - recall_2k: 0.6357 - recall_3k: 0.7366 - recall_5k: 0.8357 - F1_1k: 0.5554 - F1_2k: 0.6189 - F1_3k: 0.5796 - F1_5k: 0.4791 - accuracy_1k: 0.7658 - accuracy_2k: 0.8695 - accuracy_3k: 0.9153 - accuracy_5k: 0.9483 - hamming_loss_k: 0.0317 - val_loss: 0.0941 - val_precision_1k: 0.6980 - val_precision_2k: 0.5655 - val_precision_3k: 0.4516 - val_precision_5k: 0.3248 - val_recall_1k: 0.3962 - val_recall_2k: 0.5874 - val_recall_3k: 0.6797 - val_recall_5k: 0.7927 - val_F1_1k: 0.5051 - val_F1_2k: 0.5759 - val_F1_3k: 0.5423 - val_F1_5k: 0.4605 - val_accuracy_1k: 0.6980 - val_accuracy_2k: 0.8114 - val_accuracy_3k: 0.8535 - val_accuracy_5k: 0.9158 - val_hamming_loss_k: 0.0352
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0815 - precision_1k: 0.7837 - precision_2k: 0.6198 - precision_3k: 0.4930 - precision_5k: 0.3428 - recall_1k: 0.4443 - recall_2k: 0.6503 - recall_3k: 0.7547 - recall_5k: 0.8492 - F1_1k: 0.5669 - F1_2k: 0.6346 - F1_3k: 0.5963 - F1_5k: 0.4883 - accuracy_1k: 0.7837 - accuracy_2k: 0.8750 - accuracy_3k: 0.9233 - accuracy_5k: 0.9560 - hamming_loss_k: 0.0310
Epoch 00008: val_loss improved from 0.09414 to 0.08800, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.0812 - precision_1k: 0.7817 - precision_2k: 0.6207 - precision_3k: 0.4951 - precision_5k: 0.3437 - recall_1k: 0.4427 - recall_2k: 0.6498 - recall_3k: 0.7566 - recall_5k: 0.8519 - F1_1k: 0.5651 - F1_2k: 0.6348 - F1_3k: 0.5984 - F1_5k: 0.4898 - accuracy_1k: 0.7817 - accuracy_2k: 0.8724 - accuracy_3k: 0.9226 - accuracy_5k: 0.9558 - hamming_loss_k: 0.0310 - val_loss: 0.0880 - val_precision_1k: 0.7153 - val_precision_2k: 0.5914 - val_precision_3k: 0.4771 - val_precision_5k: 0.3351 - val_recall_1k: 0.4015 - val_recall_2k: 0.6112 - val_recall_3k: 0.7175 - val_recall_5k: 0.8143 - val_F1_1k: 0.5138 - val_F1_2k: 0.6007 - val_F1_3k: 0.5727 - val_F1_5k: 0.4745 - val_accuracy_1k: 0.7153 - val_accuracy_2k: 0.8415 - val_accuracy_3k: 0.8889 - val_accuracy_5k: 0.9243 - val_hamming_loss_k: 0.0345
Epoch 9/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0773 - precision_1k: 0.7947 - precision_2k: 0.6374 - precision_3k: 0.5078 - precision_5k: 0.3511 - recall_1k: 0.4514 - recall_2k: 0.6674 - recall_3k: 0.7746 - recall_5k: 0.8680 - F1_1k: 0.5756 - F1_2k: 0.6519 - F1_3k: 0.6133 - F1_5k: 0.4998 - accuracy_1k: 0.7947 - accuracy_2k: 0.8881 - accuracy_3k: 0.9329 - accuracy_5k: 0.9627 - hamming_loss_k: 0.0306 ETA: 0s - loss: 0.0768 - precision_1k: 0.7878 - precision_2k: 0.6377 - precision_3k: 0.5080 - precision_5k: 0.3520 - recall_1k: 0.4481 - recall_2k: 0.6687 - recall_3k: 0.7758 - recall_5k: 0.8695 - F1_1k: 0.5711 - F1_2k: 0.6527 - F1_3k: 0.6138 - F1_5k: 0.5010 - accuracy_1k: 0.7878 - accuracy_2k: 0.8860 - accuracy_3k: 0.9316 - accuracy_5k: 0.9622 - hamming_loss_k: 
Epoch 00009: val_loss improved from 0.08800 to 0.08724, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 84ms/step - loss: 0.0772 - precision_1k: 0.7951 - precision_2k: 0.6344 - precision_3k: 0.5072 - precision_5k: 0.3510 - recall_1k: 0.4526 - recall_2k: 0.6660 - recall_3k: 0.7753 - recall_5k: 0.8708 - F1_1k: 0.5767 - F1_2k: 0.6496 - F1_3k: 0.6131 - F1_5k: 0.5003 - accuracy_1k: 0.7951 - accuracy_2k: 0.8872 - accuracy_3k: 0.9336 - accuracy_5k: 0.9653 - hamming_loss_k: 0.0305 - val_loss: 0.0872 - val_precision_1k: 0.7381 - val_precision_2k: 0.6033 - val_precision_3k: 0.4795 - val_precision_5k: 0.3404 - val_recall_1k: 0.4015 - val_recall_2k: 0.6186 - val_recall_3k: 0.7185 - val_recall_5k: 0.8339 - val_F1_1k: 0.5191 - val_F1_2k: 0.6101 - val_F1_3k: 0.5746 - val_F1_5k: 0.4831 - val_accuracy_1k: 0.7381 - val_accuracy_2k: 0.8468 - val_accuracy_3k: 0.8916 - val_accuracy_5k: 0.9431 - val_hamming_loss_k: 0.0335
Epoch 10/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0752 - precision_1k: 0.8153 - precision_2k: 0.6438 - precision_3k: 0.5156 - precision_5k: 0.3536 - recall_1k: 0.4640 - recall_2k: 0.6765 - recall_3k: 0.7873 - recall_5k: 0.8752 - F1_1k: 0.5914 - F1_2k: 0.6597 - F1_3k: 0.6231 - F1_5k: 0.5037 - accuracy_1k: 0.8153 - accuracy_2k: 0.8999 - accuracy_3k: 0.9432 - accuracy_5k: 0.9691 - hamming_loss_k: 0.0297
Epoch 00010: val_loss improved from 0.08724 to 0.08709, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 91ms/step - loss: 0.0754 - precision_1k: 0.8123 - precision_2k: 0.6405 - precision_3k: 0.5134 - precision_5k: 0.3529 - recall_1k: 0.4624 - recall_2k: 0.6751 - recall_3k: 0.7851 - recall_5k: 0.8744 - F1_1k: 0.5893 - F1_2k: 0.6573 - F1_3k: 0.6208 - F1_5k: 0.5029 - accuracy_1k: 0.8123 - accuracy_2k: 0.9001 - accuracy_3k: 0.9414 - accuracy_5k: 0.9684 - hamming_loss_k: 0.0298 - val_loss: 0.0871 - val_precision_1k: 0.7417 - val_precision_2k: 0.5840 - val_precision_3k: 0.4735 - val_precision_5k: 0.3364 - val_recall_1k: 0.4152 - val_recall_2k: 0.6104 - val_recall_3k: 0.7171 - val_recall_5k: 0.8195 - val_F1_1k: 0.5317 - val_F1_2k: 0.5963 - val_F1_3k: 0.5698 - val_F1_5k: 0.4767 - val_accuracy_1k: 0.7417 - val_accuracy_2k: 0.8584 - val_accuracy_3k: 0.8929 - val_accuracy_5k: 0.9266 - val_hamming_loss_k: 0.0334
Epoch 11/100
12/12 [==============================] - ETA: 0s - loss: 0.0723 - precision_1k: 0.8245 - precision_2k: 0.6524 - precision_3k: 0.5225 - precision_5k: 0.3591 - recall_1k: 0.4681 - recall_2k: 0.6836 - recall_3k: 0.7962 - recall_5k: 0.8871 - F1_1k: 0.5968 - F1_2k: 0.6673 - F1_3k: 0.6307 - F1_5k: 0.5110 - accuracy_1k: 0.8245 - accuracy_2k: 0.9000 - accuracy_3k: 0.9458 - accuracy_5k: 0.9711 - hamming_loss_k: 0.0294
Epoch 00011: val_loss improved from 0.08709 to 0.08452, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 98ms/step - loss: 0.0723 - precision_1k: 0.8245 - precision_2k: 0.6524 - precision_3k: 0.5225 - precision_5k: 0.3591 - recall_1k: 0.4681 - recall_2k: 0.6836 - recall_3k: 0.7962 - recall_5k: 0.8871 - F1_1k: 0.5968 - F1_2k: 0.6673 - F1_3k: 0.6307 - F1_5k: 0.5110 - accuracy_1k: 0.8245 - accuracy_2k: 0.9000 - accuracy_3k: 0.9458 - accuracy_5k: 0.9711 - hamming_loss_k: 0.0294 - val_loss: 0.0845 - val_precision_1k: 0.7523 - val_precision_2k: 0.6034 - val_precision_3k: 0.4790 - val_precision_5k: 0.3403 - val_recall_1k: 0.4220 - val_recall_2k: 0.6288 - val_recall_3k: 0.7236 - val_recall_5k: 0.8349 - val_F1_1k: 0.5400 - val_F1_2k: 0.6152 - val_F1_3k: 0.5759 - val_F1_5k: 0.4832 - val_accuracy_1k: 0.7523 - val_accuracy_2k: 0.8756 - val_accuracy_3k: 0.9071 - val_accuracy_5k: 0.9473 - val_hamming_loss_k: 0.0330
Epoch 12/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0692 - precision_1k: 0.8395 - precision_2k: 0.6630 - precision_3k: 0.5302 - precision_5k: 0.3627 - recall_1k: 0.4777 - recall_2k: 0.6976 - recall_3k: 0.8070 - recall_5k: 0.8935 - F1_1k: 0.6088 - F1_2k: 0.6797 - F1_3k: 0.6398 - F1_5k: 0.5158 - accuracy_1k: 0.8395 - accuracy_2k: 0.9194 - accuracy_3k: 0.9552 - accuracy_5k: 0.9752 - hamming_loss_k: 0.0288
Epoch 00012: val_loss improved from 0.08452 to 0.08197, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 97ms/step - loss: 0.0693 - precision_1k: 0.8356 - precision_2k: 0.6621 - precision_3k: 0.5279 - precision_5k: 0.3612 - recall_1k: 0.4771 - recall_2k: 0.6991 - recall_3k: 0.8064 - recall_5k: 0.8933 - F1_1k: 0.6072 - F1_2k: 0.6800 - F1_3k: 0.6379 - F1_5k: 0.5143 - accuracy_1k: 0.8356 - accuracy_2k: 0.9191 - accuracy_3k: 0.9535 - accuracy_5k: 0.9756 - hamming_loss_k: 0.0288 - val_loss: 0.0820 - val_precision_1k: 0.7843 - val_precision_2k: 0.6154 - val_precision_3k: 0.4910 - val_precision_5k: 0.3454 - val_recall_1k: 0.4472 - val_recall_2k: 0.6467 - val_recall_3k: 0.7454 - val_recall_5k: 0.8469 - val_F1_1k: 0.5691 - val_F1_2k: 0.6301 - val_F1_3k: 0.5915 - val_F1_5k: 0.4903 - val_accuracy_1k: 0.7843 - val_accuracy_2k: 0.8723 - val_accuracy_3k: 0.9117 - val_accuracy_5k: 0.9527 - val_hamming_loss_k: 0.0317
Epoch 13/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0661 - precision_1k: 0.8548 - precision_2k: 0.6770 - precision_3k: 0.5385 - precision_5k: 0.3675 - recall_1k: 0.4919 - recall_2k: 0.7145 - recall_3k: 0.8213 - recall_5k: 0.9054 - F1_1k: 0.6244 - F1_2k: 0.6952 - F1_3k: 0.6503 - F1_5k: 0.5228 - accuracy_1k: 0.8548 - accuracy_2k: 0.9279 - accuracy_3k: 0.9560 - accuracy_5k: 0.9777 - hamming_loss_k: 0.0280
Epoch 00013: val_loss improved from 0.08197 to 0.08077, saving model to logs/vbgscd-lab-0604-114622/model/checkpoint_lab.h5
12/12 [==============================] - 1s 95ms/step - loss: 0.0659 - precision_1k: 0.8528 - precision_2k: 0.6793 - precision_3k: 0.5394 - precision_5k: 0.3685 - recall_1k: 0.4894 - recall_2k: 0.7154 - recall_3k: 0.8218 - recall_5k: 0.9071 - F1_1k: 0.6217 - F1_2k: 0.6968 - F1_3k: 0.6512 - F1_5k: 0.5240 - accuracy_1k: 0.8528 - accuracy_2k: 0.9280 - accuracy_3k: 0.9569 - accuracy_5k: 0.9784 - hamming_loss_k: 0.0282 - val_loss: 0.0808 - val_precision_1k: 0.7805 - val_precision_2k: 0.6234 - val_precision_3k: 0.4935 - val_precision_5k: 0.3500 - val_recall_1k: 0.4324 - val_recall_2k: 0.6465 - val_recall_3k: 0.7455 - val_recall_5k: 0.8580 - val_F1_1k: 0.5556 - val_F1_2k: 0.6340 - val_F1_3k: 0.5932 - val_F1_5k: 0.4968 - val_accuracy_1k: 0.7805 - val_accuracy_2k: 0.8769 - val_accuracy_3k: 0.9108 - val_accuracy_5k: 0.9541 - val_hamming_loss_k: 0.0318
Epoch 14/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0641 - precision_1k: 0.8590 - precision_2k: 0.6799 - precision_3k: 0.5388 - precision_5k: 0.3683 - recall_1k: 0.4920 - recall_2k: 0.7155 - recall_3k: 0.8224 - recall_5k: 0.9097 - F1_1k: 0.6255 - F1_2k: 0.6971 - F1_3k: 0.6510 - F1_5k: 0.5243 - accuracy_1k: 0.8590 - accuracy_2k: 0.9308 - accuracy_3k: 0.9613 - accuracy_5k: 0.9809 - hamming_loss_k: 0.0279
Epoch 00014: val_loss did not improve from 0.08077
12/12 [==============================] - 1s 59ms/step - loss: 0.0639 - precision_1k: 0.8583 - precision_2k: 0.6822 - precision_3k: 0.5414 - precision_5k: 0.3700 - recall_1k: 0.4922 - recall_2k: 0.7173 - recall_3k: 0.8247 - recall_5k: 0.9122 - F1_1k: 0.6255 - F1_2k: 0.6992 - F1_3k: 0.6536 - F1_5k: 0.5265 - accuracy_1k: 0.8583 - accuracy_2k: 0.9317 - accuracy_3k: 0.9618 - accuracy_5k: 0.9808 - hamming_loss_k: 0.0279 - val_loss: 0.0828 - val_precision_1k: 0.7714 - val_precision_2k: 0.6192 - val_precision_3k: 0.5014 - val_precision_5k: 0.3477 - val_recall_1k: 0.4373 - val_recall_2k: 0.6447 - val_recall_3k: 0.7556 - val_recall_5k: 0.8475 - val_F1_1k: 0.5577 - val_F1_2k: 0.6312 - val_F1_3k: 0.6023 - val_F1_5k: 0.4928 - val_accuracy_1k: 0.7714 - val_accuracy_2k: 0.8736 - val_accuracy_3k: 0.9079 - val_accuracy_5k: 0.9446 - val_hamming_loss_k: 0.0322
Epoch 15/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0616 - precision_1k: 0.8715 - precision_2k: 0.6934 - precision_3k: 0.5469 - precision_5k: 0.3729 - recall_1k: 0.5003 - recall_2k: 0.7298 - recall_3k: 0.8344 - recall_5k: 0.9186 - F1_1k: 0.6355 - F1_2k: 0.7110 - F1_3k: 0.6606 - F1_5k: 0.5304 - accuracy_1k: 0.8715 - accuracy_2k: 0.9375 - accuracy_3k: 0.9663 - accuracy_5k: 0.9823 - hamming_loss_k: 0.0273
Epoch 00015: val_loss did not improve from 0.08077
12/12 [==============================] - 1s 64ms/step - loss: 0.0618 - precision_1k: 0.8713 - precision_2k: 0.6929 - precision_3k: 0.5462 - precision_5k: 0.3724 - recall_1k: 0.5008 - recall_2k: 0.7291 - recall_3k: 0.8320 - recall_5k: 0.9162 - F1_1k: 0.6359 - F1_2k: 0.7105 - F1_3k: 0.6594 - F1_5k: 0.5295 - accuracy_1k: 0.8713 - accuracy_2k: 0.9384 - accuracy_3k: 0.9653 - accuracy_5k: 0.9821 - hamming_loss_k: 0.0274 - val_loss: 0.0822 - val_precision_1k: 0.7847 - val_precision_2k: 0.6274 - val_precision_3k: 0.4988 - val_precision_5k: 0.3475 - val_recall_1k: 0.4456 - val_recall_2k: 0.6515 - val_recall_3k: 0.7503 - val_recall_5k: 0.8454 - val_F1_1k: 0.5678 - val_F1_2k: 0.6385 - val_F1_3k: 0.5987 - val_F1_5k: 0.4922 - val_accuracy_1k: 0.7847 - val_accuracy_2k: 0.8710 - val_accuracy_3k: 0.9088 - val_accuracy_5k: 0.9444 - val_hamming_loss_k: 0.0316
Epoch 00015: early stopping
39/39 [==============================] - 1s 19ms/step - loss: 0.0712 - precision_1k: 0.8522 - precision_2k: 0.6772 - precision_3k: 0.5360 - precision_5k: 0.3702 - recall_1k: 0.4729 - recall_2k: 0.7005 - recall_3k: 0.8043 - recall_5k: 0.8999 - F1_1k: 0.6071 - F1_2k: 0.6875 - F1_3k: 0.6423 - F1_5k: 0.5238 - accuracy_1k: 0.8522 - accuracy_2k: 0.9231 - accuracy_3k: 0.9567 - accuracy_5k: 0.9816 - hamming_loss_k: 0.0291
Best model result:  [0.07120233029127121, 0.8522076606750488, 0.6771590113639832, 0.53603595495224, 0.37017175555229187, 0.4728640913963318, 0.7005333304405212, 0.8042845726013184, 0.8999050855636597, 0.6071164011955261, 0.6875458359718323, 0.6422747373580933, 0.5238196849822998, 0.8522076606750488, 0.9230718612670898, 0.9567358493804932, 0.9815870523452759, 0.029091443866491318]
2970
742
1238
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 49)]         0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 120)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 49, 300)      14700       ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 120, 300)     1064400     ['text_input[0][0]']             
                                                                                                  
 tf.__operators__.getitem_8 (Sl  (None, 49, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 BiLSTM (Bidirectional)         (None, 120, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 0_level_label_emb (Dense)      (None, 49, 1024)     308224      ['tf.__operators__.getitem_8[0][0
                                                                 ]']                              
                                                                                                  
 permute_4 (Permute)            (None, 1024, 120)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_8 (Lambda)              (None, 49, 120)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_4[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 49, 120)     14520       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 49, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 49)           50225       ['0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 4,782,117
Trainable params: 3,717,717
Non-trainable params: 1,064,400
__________________________________________________________________________________________________
None
2 patience
Epoch 1/100
11/12 [==========================>...] - ETA: 0s - loss: 0.3332 - precision_1k: 0.2557 - precision_2k: 0.1914 - precision_3k: 0.1604 - precision_5k: 0.1253 - recall_1k: 0.1256 - recall_2k: 0.1861 - recall_3k: 0.2314 - recall_5k: 0.2954 - F1_1k: 0.1682 - F1_2k: 0.1884 - F1_3k: 0.1892 - F1_5k: 0.1758 - accuracy_1k: 0.2557 - accuracy_2k: 0.3455 - accuracy_3k: 0.4102 - accuracy_5k: 0.4950 - hamming_loss_k: 0.0529
Epoch 00001: val_loss improved from inf to 0.16552, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 3s 125ms/step - loss: 0.3260 - precision_1k: 0.2593 - precision_2k: 0.1949 - precision_3k: 0.1662 - precision_5k: 0.1316 - recall_1k: 0.1283 - recall_2k: 0.1899 - recall_3k: 0.2402 - recall_5k: 0.3103 - F1_1k: 0.1714 - F1_2k: 0.1921 - F1_3k: 0.1962 - F1_5k: 0.1846 - accuracy_1k: 0.2593 - accuracy_2k: 0.3519 - accuracy_3k: 0.4225 - accuracy_5k: 0.5128 - hamming_loss_k: 0.0528 - val_loss: 0.1655 - val_precision_1k: 0.2796 - val_precision_2k: 0.2224 - val_precision_3k: 0.2208 - val_precision_5k: 0.1863 - val_recall_1k: 0.1496 - val_recall_2k: 0.2295 - val_recall_3k: 0.3327 - val_recall_5k: 0.4662 - val_F1_1k: 0.1938 - val_F1_2k: 0.2250 - val_F1_3k: 0.2642 - val_F1_5k: 0.2652 - val_accuracy_1k: 0.2796 - val_accuracy_2k: 0.4132 - val_accuracy_3k: 0.5485 - val_accuracy_5k: 0.6887 - val_hamming_loss_k: 0.0506
Epoch 2/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1406 - precision_1k: 0.4514 - precision_2k: 0.3892 - precision_3k: 0.3175 - precision_5k: 0.2342 - recall_1k: 0.2399 - recall_2k: 0.4010 - recall_3k: 0.4815 - recall_5k: 0.5844 - F1_1k: 0.3130 - F1_2k: 0.3948 - F1_3k: 0.3824 - F1_5k: 0.3343 - accuracy_1k: 0.4514 - accuracy_2k: 0.6257 - accuracy_3k: 0.6946 - accuracy_5k: 0.7781 - hamming_loss_k: 0.0450
Epoch 00002: val_loss improved from 0.16552 to 0.12425, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 101ms/step - loss: 0.1399 - precision_1k: 0.4603 - precision_2k: 0.3906 - precision_3k: 0.3181 - precision_5k: 0.2350 - recall_1k: 0.2452 - recall_2k: 0.4025 - recall_3k: 0.4814 - recall_5k: 0.5856 - F1_1k: 0.3197 - F1_2k: 0.3962 - F1_3k: 0.3828 - F1_5k: 0.3353 - accuracy_1k: 0.4603 - accuracy_2k: 0.6298 - accuracy_3k: 0.6989 - accuracy_5k: 0.7820 - hamming_loss_k: 0.0446 - val_loss: 0.1242 - val_precision_1k: 0.5226 - val_precision_2k: 0.3909 - val_precision_3k: 0.3168 - val_precision_5k: 0.2381 - val_recall_1k: 0.2921 - val_recall_2k: 0.4091 - val_recall_3k: 0.4825 - val_recall_5k: 0.5973 - val_F1_1k: 0.3723 - val_F1_2k: 0.3978 - val_F1_3k: 0.3810 - val_F1_5k: 0.3397 - val_accuracy_1k: 0.5226 - val_accuracy_2k: 0.6403 - val_accuracy_3k: 0.7117 - val_accuracy_5k: 0.7829 - val_hamming_loss_k: 0.0407
Epoch 3/100
11/12 [==========================>...] - ETA: 0s - loss: 0.1175 - precision_1k: 0.5742 - precision_2k: 0.4517 - precision_3k: 0.3646 - precision_5k: 0.2739 - recall_1k: 0.3040 - recall_2k: 0.4608 - recall_3k: 0.5480 - recall_5k: 0.6762 - F1_1k: 0.3974 - F1_2k: 0.4560 - F1_3k: 0.4377 - F1_5k: 0.3899 - accuracy_1k: 0.5742 - accuracy_2k: 0.7109 - accuracy_3k: 0.7745 - accuracy_5k: 0.8544 - hamming_loss_k: 0.0397
Epoch 00003: val_loss improved from 0.12425 to 0.11049, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 91ms/step - loss: 0.1176 - precision_1k: 0.5778 - precision_2k: 0.4560 - precision_3k: 0.3672 - precision_5k: 0.2760 - recall_1k: 0.3057 - recall_2k: 0.4642 - recall_3k: 0.5511 - recall_5k: 0.6781 - F1_1k: 0.3997 - F1_2k: 0.4599 - F1_3k: 0.4406 - F1_5k: 0.3923 - accuracy_1k: 0.5778 - accuracy_2k: 0.7150 - accuracy_3k: 0.7776 - accuracy_5k: 0.8573 - hamming_loss_k: 0.0399 - val_loss: 0.1105 - val_precision_1k: 0.5812 - val_precision_2k: 0.4583 - val_precision_3k: 0.3808 - val_precision_5k: 0.2799 - val_recall_1k: 0.3303 - val_recall_2k: 0.4984 - val_recall_3k: 0.6115 - val_recall_5k: 0.7235 - val_F1_1k: 0.4197 - val_F1_2k: 0.4766 - val_F1_3k: 0.4687 - val_F1_5k: 0.4032 - val_accuracy_1k: 0.5812 - val_accuracy_2k: 0.7288 - val_accuracy_3k: 0.8151 - val_accuracy_5k: 0.8913 - val_hamming_loss_k: 0.0383
Epoch 4/100
12/12 [==============================] - ETA: 0s - loss: 0.1052 - precision_1k: 0.6473 - precision_2k: 0.5240 - precision_3k: 0.4247 - precision_5k: 0.3057 - recall_1k: 0.3485 - recall_2k: 0.5381 - recall_3k: 0.6424 - recall_5k: 0.7555 - F1_1k: 0.4528 - F1_2k: 0.5308 - F1_3k: 0.5112 - F1_5k: 0.4352 - accuracy_1k: 0.6473 - accuracy_2k: 0.7758 - accuracy_3k: 0.8449 - accuracy_5k: 0.9053 - hamming_loss_k: 0.0370
Epoch 00004: val_loss improved from 0.11049 to 0.09997, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 100ms/step - loss: 0.1052 - precision_1k: 0.6473 - precision_2k: 0.5240 - precision_3k: 0.4247 - precision_5k: 0.3057 - recall_1k: 0.3485 - recall_2k: 0.5381 - recall_3k: 0.6424 - recall_5k: 0.7555 - F1_1k: 0.4528 - F1_2k: 0.5308 - F1_3k: 0.5112 - F1_5k: 0.4352 - accuracy_1k: 0.6473 - accuracy_2k: 0.7758 - accuracy_3k: 0.8449 - accuracy_5k: 0.9053 - hamming_loss_k: 0.0370 - val_loss: 0.1000 - val_precision_1k: 0.6408 - val_precision_2k: 0.5247 - val_precision_3k: 0.4195 - val_precision_5k: 0.3061 - val_recall_1k: 0.3721 - val_recall_2k: 0.5721 - val_recall_3k: 0.6699 - val_recall_5k: 0.7854 - val_F1_1k: 0.4699 - val_F1_2k: 0.5466 - val_F1_3k: 0.5152 - val_F1_5k: 0.4400 - val_accuracy_1k: 0.6408 - val_accuracy_2k: 0.8064 - val_accuracy_3k: 0.8697 - val_accuracy_5k: 0.9224 - val_hamming_loss_k: 0.0359
Epoch 5/100
12/12 [==============================] - ETA: 0s - loss: 0.0961 - precision_1k: 0.7045 - precision_2k: 0.5612 - precision_3k: 0.4484 - precision_5k: 0.3197 - recall_1k: 0.3857 - recall_2k: 0.5824 - recall_3k: 0.6802 - recall_5k: 0.7867 - F1_1k: 0.4983 - F1_2k: 0.5715 - F1_3k: 0.5404 - F1_5k: 0.4546 - accuracy_1k: 0.7045 - accuracy_2k: 0.8166 - accuracy_3k: 0.8684 - accuracy_5k: 0.9190 - hamming_loss_k: 0.0346
Epoch 00005: val_loss improved from 0.09997 to 0.09426, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 96ms/step - loss: 0.0961 - precision_1k: 0.7045 - precision_2k: 0.5612 - precision_3k: 0.4484 - precision_5k: 0.3197 - recall_1k: 0.3857 - recall_2k: 0.5824 - recall_3k: 0.6802 - recall_5k: 0.7867 - F1_1k: 0.4983 - F1_2k: 0.5715 - F1_3k: 0.5404 - F1_5k: 0.4546 - accuracy_1k: 0.7045 - accuracy_2k: 0.8166 - accuracy_3k: 0.8684 - accuracy_5k: 0.9190 - hamming_loss_k: 0.0346 - val_loss: 0.0943 - val_precision_1k: 0.6722 - val_precision_2k: 0.5364 - val_precision_3k: 0.4401 - val_precision_5k: 0.3139 - val_recall_1k: 0.3933 - val_recall_2k: 0.5842 - val_recall_3k: 0.6989 - val_recall_5k: 0.8016 - val_F1_1k: 0.4949 - val_F1_2k: 0.5581 - val_F1_3k: 0.5393 - val_F1_5k: 0.4506 - val_accuracy_1k: 0.6722 - val_accuracy_2k: 0.7996 - val_accuracy_3k: 0.8762 - val_accuracy_5k: 0.9275 - val_hamming_loss_k: 0.0346
Epoch 6/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0893 - precision_1k: 0.7369 - precision_2k: 0.5909 - precision_3k: 0.4712 - precision_5k: 0.3335 - recall_1k: 0.4045 - recall_2k: 0.6110 - recall_3k: 0.7132 - recall_5k: 0.8164 - F1_1k: 0.5222 - F1_2k: 0.6007 - F1_3k: 0.5674 - F1_5k: 0.4735 - accuracy_1k: 0.7369 - accuracy_2k: 0.8384 - accuracy_3k: 0.8885 - accuracy_5k: 0.9340 - hamming_loss_k: 0.0334
Epoch 00006: val_loss improved from 0.09426 to 0.08771, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 84ms/step - loss: 0.0891 - precision_1k: 0.7377 - precision_2k: 0.5882 - precision_3k: 0.4691 - precision_5k: 0.3317 - recall_1k: 0.4065 - recall_2k: 0.6106 - recall_3k: 0.7130 - recall_5k: 0.8157 - F1_1k: 0.5240 - F1_2k: 0.5991 - F1_3k: 0.5658 - F1_5k: 0.4716 - accuracy_1k: 0.7377 - accuracy_2k: 0.8373 - accuracy_3k: 0.8875 - accuracy_5k: 0.9346 - hamming_loss_k: 0.0332 - val_loss: 0.0877 - val_precision_1k: 0.7441 - val_precision_2k: 0.5770 - val_precision_3k: 0.4642 - val_precision_5k: 0.3264 - val_recall_1k: 0.4407 - val_recall_2k: 0.6292 - val_recall_3k: 0.7371 - val_recall_5k: 0.8334 - val_F1_1k: 0.5523 - val_F1_2k: 0.6006 - val_F1_3k: 0.5688 - val_F1_5k: 0.4685 - val_accuracy_1k: 0.7441 - val_accuracy_2k: 0.8511 - val_accuracy_3k: 0.9110 - val_accuracy_5k: 0.9496 - val_hamming_loss_k: 0.0316
Epoch 7/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0831 - precision_1k: 0.7656 - precision_2k: 0.6083 - precision_3k: 0.4911 - precision_5k: 0.3445 - recall_1k: 0.4251 - recall_2k: 0.6319 - recall_3k: 0.7460 - recall_5k: 0.8478 - F1_1k: 0.5465 - F1_2k: 0.6197 - F1_3k: 0.5922 - F1_5k: 0.4899 - accuracy_1k: 0.7656 - accuracy_2k: 0.8594 - accuracy_3k: 0.9084 - accuracy_5k: 0.9499 - hamming_loss_k: 0.0319
Epoch 00007: val_loss improved from 0.08771 to 0.08581, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 99ms/step - loss: 0.0837 - precision_1k: 0.7646 - precision_2k: 0.6090 - precision_3k: 0.4917 - precision_5k: 0.3455 - recall_1k: 0.4224 - recall_2k: 0.6277 - recall_3k: 0.7412 - recall_5k: 0.8434 - F1_1k: 0.5440 - F1_2k: 0.6180 - F1_3k: 0.5910 - F1_5k: 0.4900 - accuracy_1k: 0.7646 - accuracy_2k: 0.8608 - accuracy_3k: 0.9095 - accuracy_5k: 0.9492 - hamming_loss_k: 0.0323 - val_loss: 0.0858 - val_precision_1k: 0.7574 - val_precision_2k: 0.5714 - val_precision_3k: 0.4620 - val_precision_5k: 0.3248 - val_recall_1k: 0.4530 - val_recall_2k: 0.6247 - val_recall_3k: 0.7294 - val_recall_5k: 0.8296 - val_F1_1k: 0.5662 - val_F1_2k: 0.5960 - val_F1_3k: 0.5647 - val_F1_5k: 0.4662 - val_accuracy_1k: 0.7574 - val_accuracy_2k: 0.8608 - val_accuracy_3k: 0.9071 - val_accuracy_5k: 0.9479 - val_hamming_loss_k: 0.0311
Epoch 8/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0813 - precision_1k: 0.7852 - precision_2k: 0.6188 - precision_3k: 0.4941 - precision_5k: 0.3481 - recall_1k: 0.4365 - recall_2k: 0.6439 - recall_3k: 0.7499 - recall_5k: 0.8542 - F1_1k: 0.5610 - F1_2k: 0.6310 - F1_3k: 0.5956 - F1_5k: 0.4945 - accuracy_1k: 0.7852 - accuracy_2k: 0.8739 - accuracy_3k: 0.9194 - accuracy_5k: 0.9567 - hamming_loss_k: 0.0313
Epoch 00008: val_loss improved from 0.08581 to 0.08126, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 85ms/step - loss: 0.0815 - precision_1k: 0.7809 - precision_2k: 0.6159 - precision_3k: 0.4928 - precision_5k: 0.3472 - recall_1k: 0.4345 - recall_2k: 0.6409 - recall_3k: 0.7481 - recall_5k: 0.8520 - F1_1k: 0.5582 - F1_2k: 0.6280 - F1_3k: 0.5941 - F1_5k: 0.4933 - accuracy_1k: 0.7809 - accuracy_2k: 0.8698 - accuracy_3k: 0.9185 - accuracy_5k: 0.9560 - hamming_loss_k: 0.0315 - val_loss: 0.0813 - val_precision_1k: 0.7928 - val_precision_2k: 0.6034 - val_precision_3k: 0.4751 - val_precision_5k: 0.3300 - val_recall_1k: 0.4689 - val_recall_2k: 0.6552 - val_recall_3k: 0.7478 - val_recall_5k: 0.8411 - val_F1_1k: 0.5880 - val_F1_2k: 0.6269 - val_F1_3k: 0.5802 - val_F1_5k: 0.4735 - val_accuracy_1k: 0.7928 - val_accuracy_2k: 0.8842 - val_accuracy_3k: 0.9236 - val_accuracy_5k: 0.9509 - val_hamming_loss_k: 0.0297
Epoch 9/100
12/12 [==============================] - ETA: 0s - loss: 0.0774 - precision_1k: 0.7964 - precision_2k: 0.6366 - precision_3k: 0.5074 - precision_5k: 0.3524 - recall_1k: 0.4437 - recall_2k: 0.6592 - recall_3k: 0.7644 - recall_5k: 0.8624 - F1_1k: 0.5697 - F1_2k: 0.6475 - F1_3k: 0.6098 - F1_5k: 0.5002 - accuracy_1k: 0.7964 - accuracy_2k: 0.8852 - accuracy_3k: 0.9229 - accuracy_5k: 0.9595 - hamming_loss_k: 0.0309
Epoch 00009: val_loss improved from 0.08126 to 0.08064, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 82ms/step - loss: 0.0774 - precision_1k: 0.7964 - precision_2k: 0.6366 - precision_3k: 0.5074 - precision_5k: 0.3524 - recall_1k: 0.4437 - recall_2k: 0.6592 - recall_3k: 0.7644 - recall_5k: 0.8624 - F1_1k: 0.5697 - F1_2k: 0.6475 - F1_3k: 0.6098 - F1_5k: 0.5002 - accuracy_1k: 0.7964 - accuracy_2k: 0.8852 - accuracy_3k: 0.9229 - accuracy_5k: 0.9595 - hamming_loss_k: 0.0309 - val_loss: 0.0806 - val_precision_1k: 0.7741 - val_precision_2k: 0.5987 - val_precision_3k: 0.4829 - val_precision_5k: 0.3346 - val_recall_1k: 0.4539 - val_recall_2k: 0.6511 - val_recall_3k: 0.7609 - val_recall_5k: 0.8516 - val_F1_1k: 0.5716 - val_F1_2k: 0.6230 - val_F1_3k: 0.5901 - val_F1_5k: 0.4800 - val_accuracy_1k: 0.7741 - val_accuracy_2k: 0.8788 - val_accuracy_3k: 0.9254 - val_accuracy_5k: 0.9579 - val_hamming_loss_k: 0.0304
Epoch 10/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0748 - precision_1k: 0.8096 - precision_2k: 0.6404 - precision_3k: 0.5132 - precision_5k: 0.3582 - recall_1k: 0.4493 - recall_2k: 0.6621 - recall_3k: 0.7734 - recall_5k: 0.8749 - F1_1k: 0.5778 - F1_2k: 0.6511 - F1_3k: 0.6170 - F1_5k: 0.5083 - accuracy_1k: 0.8096 - accuracy_2k: 0.8860 - accuracy_3k: 0.9293 - accuracy_5k: 0.9652 - hamming_loss_k: 0.0304
Epoch 00010: val_loss improved from 0.08064 to 0.07592, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 92ms/step - loss: 0.0748 - precision_1k: 0.8093 - precision_2k: 0.6382 - precision_3k: 0.5111 - precision_5k: 0.3575 - recall_1k: 0.4497 - recall_2k: 0.6623 - recall_3k: 0.7727 - recall_5k: 0.8746 - F1_1k: 0.5781 - F1_2k: 0.6500 - F1_3k: 0.6152 - F1_5k: 0.5075 - accuracy_1k: 0.8093 - accuracy_2k: 0.8874 - accuracy_3k: 0.9298 - accuracy_5k: 0.9654 - hamming_loss_k: 0.0303 - val_loss: 0.0759 - val_precision_1k: 0.8005 - val_precision_2k: 0.6181 - val_precision_3k: 0.4955 - val_precision_5k: 0.3441 - val_recall_1k: 0.4794 - val_recall_2k: 0.6771 - val_recall_3k: 0.7810 - val_recall_5k: 0.8745 - val_F1_1k: 0.5991 - val_F1_2k: 0.6455 - val_F1_3k: 0.6053 - val_F1_5k: 0.4933 - val_accuracy_1k: 0.8005 - val_accuracy_2k: 0.9104 - val_accuracy_3k: 0.9425 - val_accuracy_5k: 0.9655 - val_hamming_loss_k: 0.0293
Epoch 11/100
11/12 [==========================>...] - ETA: 0s - loss: 0.0713 - precision_1k: 0.8292 - precision_2k: 0.6593 - precision_3k: 0.5277 - precision_5k: 0.3627 - recall_1k: 0.4672 - recall_2k: 0.6843 - recall_3k: 0.7969 - recall_5k: 0.8871 - F1_1k: 0.5976 - F1_2k: 0.6715 - F1_3k: 0.6349 - F1_5k: 0.5149 - accuracy_1k: 0.8292 - accuracy_2k: 0.9087 - accuracy_3k: 0.9439 - accuracy_5k: 0.9691 - hamming_loss_k: 0.0295
Epoch 00011: val_loss improved from 0.07592 to 0.07389, saving model to logs/zdfmxr-lab-0604-114642/model/checkpoint_lab.h5
12/12 [==============================] - 1s 94ms/step - loss: 0.0715 - precision_1k: 0.8277 - precision_2k: 0.6582 - precision_3k: 0.5272 - precision_5k: 0.3636 - recall_1k: 0.4658 - recall_2k: 0.6825 - recall_3k: 0.7947 - recall_5k: 0.8866 - F1_1k: 0.5961 - F1_2k: 0.6701 - F1_3k: 0.6338 - F1_5k: 0.5156 - accuracy_1k: 0.8277 - accuracy_2k: 0.9082 - accuracy_3k: 0.9432 - accuracy_5k: 0.9684 - hamming_loss_k: 0.0296 - val_loss: 0.0739 - val_precision_1k: 0.8167 - val_precision_2k: 0.6299 - val_precision_3k: 0.5007 - val_precision_5k: 0.3449 - val_recall_1k: 0.4807 - val_recall_2k: 0.6775 - val_recall_3k: 0.7867 - val_recall_5k: 0.8744 - val_F1_1k: 0.6042 - val_F1_2k: 0.6517 - val_F1_3k: 0.6110 - val_F1_5k: 0.4942 - val_accuracy_1k: 0.8167 - val_accuracy_2k: 0.8979 - val_accuracy_3k: 0.9438 - val_accuracy_5k: 0.9641 - val_hamming_loss_k: 0.0287
Epoch 12/100
12/12 [==============================] - ETA: 0s - loss: 0.0680 - precision_1k: 0.8365 - precision_2k: 0.6747 - precision_3k: 0.5360 - precision_5k: 0.3683 - recall_1k: 0.4703 - recall_2k: 0.6992 - recall_3k: 0.8105 - recall_5k: 0.9020 - F1_1k: 0.6020 - F1_2k: 0.6867 - F1_3k: 0.6452 - F1_5k: 0.5230 - accuracy_1k: 0.8365 - accuracy_2k: 0.9160 - accuracy_3k: 0.9530 - accuracy_5k: 0.9773 - hamming_loss_k: 0.0292
Epoch 00012: val_loss did not improve from 0.07389
12/12 [==============================] - 1s 70ms/step - loss: 0.0680 - precision_1k: 0.8365 - precision_2k: 0.6747 - precision_3k: 0.5360 - precision_5k: 0.3683 - recall_1k: 0.4703 - recall_2k: 0.6992 - recall_3k: 0.8105 - recall_5k: 0.9020 - F1_1k: 0.6020 - F1_2k: 0.6867 - F1_3k: 0.6452 - F1_5k: 0.5230 - accuracy_1k: 0.8365 - accuracy_2k: 0.9160 - accuracy_3k: 0.9530 - accuracy_5k: 0.9773 - hamming_loss_k: 0.0292 - val_loss: 0.0745 - val_precision_1k: 0.8100 - val_precision_2k: 0.6296 - val_precision_3k: 0.5033 - val_precision_5k: 0.3455 - val_recall_1k: 0.4793 - val_recall_2k: 0.6803 - val_recall_3k: 0.7867 - val_recall_5k: 0.8722 - val_F1_1k: 0.6011 - val_F1_2k: 0.6530 - val_F1_3k: 0.6130 - val_F1_5k: 0.4944 - val_accuracy_1k: 0.8100 - val_accuracy_2k: 0.8932 - val_accuracy_3k: 0.9398 - val_accuracy_5k: 0.9606 - val_hamming_loss_k: 0.0289
Epoch 13/100
12/12 [==============================] - ETA: 0s - loss: 0.0670 - precision_1k: 0.8454 - precision_2k: 0.6736 - precision_3k: 0.5387 - precision_5k: 0.3693 - recall_1k: 0.4750 - recall_2k: 0.6961 - recall_3k: 0.8099 - recall_5k: 0.9018 - F1_1k: 0.6081 - F1_2k: 0.6846 - F1_3k: 0.6469 - F1_5k: 0.5239 - accuracy_1k: 0.8454 - accuracy_2k: 0.9135 - accuracy_3k: 0.9486 - accuracy_5k: 0.9763 - hamming_loss_k: 0.0288
Epoch 00013: val_loss did not improve from 0.07389
12/12 [==============================] - 1s 69ms/step - loss: 0.0670 - precision_1k: 0.8454 - precision_2k: 0.6736 - precision_3k: 0.5387 - precision_5k: 0.3693 - recall_1k: 0.4750 - recall_2k: 0.6961 - recall_3k: 0.8099 - recall_5k: 0.9018 - F1_1k: 0.6081 - F1_2k: 0.6846 - F1_3k: 0.6469 - F1_5k: 0.5239 - accuracy_1k: 0.8454 - accuracy_2k: 0.9135 - accuracy_3k: 0.9486 - accuracy_5k: 0.9763 - hamming_loss_k: 0.0288 - val_loss: 0.0740 - val_precision_1k: 0.8225 - val_precision_2k: 0.6339 - val_precision_3k: 0.4971 - val_precision_5k: 0.3413 - val_recall_1k: 0.4873 - val_recall_2k: 0.6888 - val_recall_3k: 0.7833 - val_recall_5k: 0.8689 - val_F1_1k: 0.6109 - val_F1_2k: 0.6592 - val_F1_3k: 0.6074 - val_F1_5k: 0.4896 - val_accuracy_1k: 0.8225 - val_accuracy_2k: 0.9017 - val_accuracy_3k: 0.9456 - val_accuracy_5k: 0.9686 - val_hamming_loss_k: 0.0284
Epoch 00013: early stopping
39/39 [==============================] - 1s 20ms/step - loss: 0.0721 - precision_1k: 0.8483 - precision_2k: 0.6786 - precision_3k: 0.5363 - precision_5k: 0.3658 - recall_1k: 0.4718 - recall_2k: 0.6967 - recall_3k: 0.8048 - recall_5k: 0.8898 - F1_1k: 0.6053 - F1_2k: 0.6865 - F1_3k: 0.6427 - F1_5k: 0.5178 - accuracy_1k: 0.8483 - accuracy_2k: 0.9224 - accuracy_3k: 0.9564 - accuracy_5k: 0.9752 - hamming_loss_k: 0.0293
Best model result:  [0.07212239503860474, 0.8482667207717896, 0.6785718202590942, 0.5363102555274963, 0.3658332824707031, 0.4718358814716339, 0.6967360377311707, 0.8047768473625183, 0.8898460268974304, 0.6053109169006348, 0.6865419745445251, 0.6426646113395691, 0.5178032517433167, 0.8482667207717896, 0.9223513603210449, 0.9563742280006409, 0.9751742482185364, 0.02925199642777443]
fold_result:  [[0.07121290266513824, 0.8458691835403442, 0.6703769564628601, 0.5273845791816711, 0.36553332209587097, 0.47485893964767456, 0.6976229548454285, 0.792594850063324, 0.8914691209793091, 0.6072724461555481, 0.6826519966125488, 0.6323443651199341, 0.5177146196365356, 0.8458691835403442, 0.9255487322807312, 0.9531742930412292, 0.9823870658874512, 0.029350105673074722], [0.07285986095666885, 0.8542383909225464, 0.6763744354248047, 0.5343307852745056, 0.36465638875961304, 0.47706159949302673, 0.6978821754455566, 0.8021255135536194, 0.8840973973274231, 0.6112679243087769, 0.6859568357467651, 0.6404430270195007, 0.5156164765357971, 0.8542383909225464, 0.923153817653656, 0.9568101763725281, 0.9759768843650818, 0.02900818921625614], [0.06916557997465134, 0.864730715751648, 0.6851717829704285, 0.5466128587722778, 0.370574414730072, 0.4871896505355835, 0.7125566005706787, 0.8208998441696167, 0.9009794592857361, 0.6221625208854675, 0.6975078582763672, 0.6551990509033203, 0.524406909942627, 0.864730715751648, 0.9331230521202087, 0.9675948023796082, 0.9831871390342712, 0.02858004905283451], [0.07120233029127121, 0.8522076606750488, 0.6771590113639832, 0.53603595495224, 0.37017175555229187, 0.4728640913963318, 0.7005333304405212, 0.8042845726013184, 0.8999050855636597, 0.6071164011955261, 0.6875458359718323, 0.6422747373580933, 0.5238196849822998, 0.8522076606750488, 0.9230718612670898, 0.9567358493804932, 0.9815870523452759, 0.029091443866491318], [0.07212239503860474, 0.8482667207717896, 0.6785718202590942, 0.5363102555274963, 0.3658332824707031, 0.4718358814716339, 0.6967360377311707, 0.8047768473625183, 0.8898460268974304, 0.6053109169006348, 0.6865419745445251, 0.6426646113395691, 0.5178032517433167, 0.8482667207717896, 0.9223513603210449, 0.9563742280006409, 0.9751742482185364, 0.02925199642777443]]
average_result:  [0.07131261378526688, 0.8530625343322754, 0.6775308012962341, 0.5361348867416382, 0.3673538327217102, 0.4767620325088501, 0.7010662198066712, 0.8049363255500793, 0.8932594180107116, 0.6106260418891907, 0.6880409002304078, 0.6425851583480835, 0.5198721885681152, 0.8530625343322754, 0.9254497647285461, 0.9581378698348999, 0.9796624779701233, 0.029056356847286226]
2024-06-04 11:46:59,376 : INFO : =======End=======
