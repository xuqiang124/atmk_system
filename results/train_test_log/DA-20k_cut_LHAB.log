/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2024-06-04 13:28:32,859 : INFO : Loading config...
2024-06-04 13:28:32,860 : INFO : {'cache_file_h5py': '../file_data/a30/math_data_cut.h5', 'cache_file_pickle': '../file_data/a32/vocab_label.pkl', 'embeddings': '../file_data/a32/embeddings.pkl', 'maxlen': 150, 'emb_size': 300, 'epochs': 150, 'batch_size': 512, 'alpha': 4, 'hidden_size': 512, 'num_classes_list': [15, 427], 'l_patience': 2, 'b_patience': 10}
2024-06-04 13:28:32,860 : INFO : Loading data...
2024-06-04 13:28:32,942 : INFO : Loading embeddings...
2024-06-04 13:28:33,013 : INFO : model name lab
TOTAL: 22498 TRAIN: [[ 126    3 1315 ...    0    0    0]
 [  44    3   17 ...    0    0    0]
 [ 216    3   11 ...    0    0    0]
 ...
 [ 130    3   78 ...    0    0    0]
 [ 238    3  134 ...    0    0    0]
 [  59    3   78 ...    0    0    0]] 16873 TEST: [[ 105    3 1490 ...   38    0    0]
 [ 238    3 2235 ...    0    0    0]
 [ 172    3  134 ...    0    0    0]
 ...
 [ 161    3 2144 ...    0    0    0]
 [ 161    3 4753 ...    0    0    0]
 [ 189    3   17 ...    0    0    0]] 5625
2024-06-04 13:28:33,046 : INFO : =====Start final=====
13498
3375
5625
2024-06-04 13:28:33.477187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.499941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.500045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.500283: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-04 13:28:33.502278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.502411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.502468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.836256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.836384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.836457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 13:28:33.836528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22102 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 15, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean (TFOpLambd  (None, 1024)        0           ['BiLSTM[0][0]']                 
 a)                                                                                               
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat (TFOpLambda)         (None, 2048)         0           ['tf.math.reduce_mean[0][0]',    
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense (Dense)                  (None, 1024)         2098176     ['tf.concat[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 15)           15375       ['dense[0][0]']                  
                                                                                                  
 tf.nn.softmax (TFOpLambda)     (None, 15)           0           ['dense_1[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 15, 1)        0           ['tf.nn.softmax[0][0]']          
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims[0][0]']         
 (Dense)                                                                                          
                                                                                                  
 permute_1 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15, 150)      22650       ['lambda_2[0][0]']               
                                                                                                  
 tf.math.reduce_mean_1 (TFOpLam  (None, 150)         0           ['dense_2[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_1[0][0]']  
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply (TFOpLambda)  (None, 150, 1024)    0           ['BiLSTM[0][0]',                 
                                                                  'tf.expand_dims_1[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply[0][0]']       
                                                                                                  
 lambda_3 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_3[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_4 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply[0][0]']       
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_4[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Epoch 1/150
2024-06-04 13:28:36.981824: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2024-06-04 13:28:37.017107: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8906
27/27 [==============================] - ETA: 0s - loss: 0.0829 - precision_1k: 0.0620 - precision_2k: 0.0500 - precision_3k: 0.0454 - precision_5k: 0.0356 - recall_1k: 0.0306 - recall_2k: 0.0486 - recall_3k: 0.0681 - recall_5k: 0.0889 - F1_1k: 0.0407 - F1_2k: 0.0491 - F1_3k: 0.0544 - F1_5k: 0.0508 - accuracy_1k: 0.0620 - accuracy_2k: 0.0976 - accuracy_3k: 0.1294 - accuracy_5k: 0.1628 - hamming_loss_k: 0.0065
Epoch 00001: val_loss improved from inf to 0.02548, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 13s 386ms/step - loss: 0.0829 - precision_1k: 0.0620 - precision_2k: 0.0500 - precision_3k: 0.0454 - precision_5k: 0.0356 - recall_1k: 0.0306 - recall_2k: 0.0486 - recall_3k: 0.0681 - recall_5k: 0.0889 - F1_1k: 0.0407 - F1_2k: 0.0491 - F1_3k: 0.0544 - F1_5k: 0.0508 - accuracy_1k: 0.0620 - accuracy_2k: 0.0976 - accuracy_3k: 0.1294 - accuracy_5k: 0.1628 - hamming_loss_k: 0.0065 - val_loss: 0.0255 - val_precision_1k: 0.1143 - val_precision_2k: 0.0917 - val_precision_3k: 0.0780 - val_precision_5k: 0.0718 - val_recall_1k: 0.0543 - val_recall_2k: 0.0852 - val_recall_3k: 0.1090 - val_recall_5k: 0.1798 - val_F1_1k: 0.0736 - val_F1_2k: 0.0883 - val_F1_3k: 0.0909 - val_F1_5k: 0.1026 - val_accuracy_1k: 0.1143 - val_accuracy_2k: 0.1573 - val_accuracy_3k: 0.1909 - val_accuracy_5k: 0.2897 - val_hamming_loss_k: 0.0062
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.0246 - precision_1k: 0.1573 - precision_2k: 0.1317 - precision_3k: 0.1121 - precision_5k: 0.0915 - recall_1k: 0.0743 - recall_2k: 0.1270 - recall_3k: 0.1634 - recall_5k: 0.2271 - F1_1k: 0.1009 - F1_2k: 0.1292 - F1_3k: 0.1329 - F1_5k: 0.1304 - accuracy_1k: 0.1573 - accuracy_2k: 0.2262 - accuracy_3k: 0.2745 - accuracy_5k: 0.3504 - hamming_loss_k: 0.0061 ETA: 3s - loss: 0.0250 - precision_1k: 0.1438 - precision_2k: 0.1272 - precision_3k: 0.1090 - precision_5k: 0.0872 - recall_1k: 0.0663 - recall_2k: 0.1216 - recall_3k: 0.1586 - recall_5k: 0.2169 - F1_1k: 0.0907 - F1_2k: 0.1243 - F1_3k: 0.1291 - F1_5k: 0.1244 - accuracy_1k: 0.1438 - accuracy_2k: 0.2205 - accuracy_3k: 0.2703 - accuracy_5k: 0.3387 - ha
Epoch 00002: val_loss improved from 0.02548 to 0.02316, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0246 - precision_1k: 0.1573 - precision_2k: 0.1317 - precision_3k: 0.1121 - precision_5k: 0.0915 - recall_1k: 0.0743 - recall_2k: 0.1270 - recall_3k: 0.1634 - recall_5k: 0.2271 - F1_1k: 0.1009 - F1_2k: 0.1292 - F1_3k: 0.1329 - F1_5k: 0.1304 - accuracy_1k: 0.1573 - accuracy_2k: 0.2262 - accuracy_3k: 0.2745 - accuracy_5k: 0.3504 - hamming_loss_k: 0.0061 - val_loss: 0.0232 - val_precision_1k: 0.1643 - val_precision_2k: 0.1408 - val_precision_3k: 0.1196 - val_precision_5k: 0.1069 - val_recall_1k: 0.0811 - val_recall_2k: 0.1403 - val_recall_3k: 0.1785 - val_recall_5k: 0.2719 - val_F1_1k: 0.1085 - val_F1_2k: 0.1404 - val_F1_3k: 0.1431 - val_F1_5k: 0.1534 - val_accuracy_1k: 0.1643 - val_accuracy_2k: 0.2383 - val_accuracy_3k: 0.2871 - val_accuracy_5k: 0.3995 - val_hamming_loss_k: 0.0060
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.0218 - precision_1k: 0.2571 - precision_2k: 0.2081 - precision_3k: 0.1726 - precision_5k: 0.1362 - recall_1k: 0.1357 - recall_2k: 0.2155 - recall_3k: 0.2646 - recall_5k: 0.3477 - F1_1k: 0.1776 - F1_2k: 0.2116 - F1_3k: 0.2088 - F1_5k: 0.1957 - accuracy_1k: 0.2571 - accuracy_2k: 0.3478 - accuracy_3k: 0.3998 - accuracy_5k: 0.4831 - hamming_loss_k: 0.0056
Epoch 00003: val_loss improved from 0.02316 to 0.01981, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0218 - precision_1k: 0.2571 - precision_2k: 0.2081 - precision_3k: 0.1726 - precision_5k: 0.1362 - recall_1k: 0.1357 - recall_2k: 0.2155 - recall_3k: 0.2646 - recall_5k: 0.3477 - F1_1k: 0.1776 - F1_2k: 0.2116 - F1_3k: 0.2088 - F1_5k: 0.1957 - accuracy_1k: 0.2571 - accuracy_2k: 0.3478 - accuracy_3k: 0.3998 - accuracy_5k: 0.4831 - hamming_loss_k: 0.0056 - val_loss: 0.0198 - val_precision_1k: 0.2889 - val_precision_2k: 0.2382 - val_precision_3k: 0.1973 - val_precision_5k: 0.1546 - val_recall_1k: 0.1605 - val_recall_2k: 0.2671 - val_recall_3k: 0.3270 - val_recall_5k: 0.4162 - val_F1_1k: 0.2063 - val_F1_2k: 0.2517 - val_F1_3k: 0.2460 - val_F1_5k: 0.2254 - val_accuracy_1k: 0.2889 - val_accuracy_2k: 0.4026 - val_accuracy_3k: 0.4707 - val_accuracy_5k: 0.5670 - val_hamming_loss_k: 0.0054
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.0186 - precision_1k: 0.3414 - precision_2k: 0.2784 - precision_3k: 0.2343 - precision_5k: 0.1812 - recall_1k: 0.1946 - recall_2k: 0.3082 - recall_3k: 0.3810 - recall_5k: 0.4815 - F1_1k: 0.2477 - F1_2k: 0.2925 - F1_3k: 0.2901 - F1_5k: 0.2632 - accuracy_1k: 0.3414 - accuracy_2k: 0.4692 - accuracy_3k: 0.5454 - accuracy_5k: 0.6341 - hamming_loss_k: 0.0052 ETA: 2s - loss: 0.0189 - precision_1k: 0.3361 - precision_2k: 0.2748 - precision_3k: 0.2313 - precision_5k: 0.1799 - recall_1k: 0.1879 - recall_2k: 0.3007 - recall_3k: 0.3713 - recall_5k: 0.4722 - F1_1k: 0.2410 - F1_2k: 0.2871 - F1_3k: 0.2849 - F1_5k: 0.2605 - accuracy_1k: 0.3361 - accuracy_2k: 0.4611 - accuracy_3k: 0.5353 - accuracy_5k: 0.6247 - hammin
Epoch 00004: val_loss improved from 0.01981 to 0.01745, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0186 - precision_1k: 0.3414 - precision_2k: 0.2784 - precision_3k: 0.2343 - precision_5k: 0.1812 - recall_1k: 0.1946 - recall_2k: 0.3082 - recall_3k: 0.3810 - recall_5k: 0.4815 - F1_1k: 0.2477 - F1_2k: 0.2925 - F1_3k: 0.2901 - F1_5k: 0.2632 - accuracy_1k: 0.3414 - accuracy_2k: 0.4692 - accuracy_3k: 0.5454 - accuracy_5k: 0.6341 - hamming_loss_k: 0.0052 - val_loss: 0.0174 - val_precision_1k: 0.3668 - val_precision_2k: 0.2879 - val_precision_3k: 0.2404 - val_precision_5k: 0.1855 - val_recall_1k: 0.2175 - val_recall_2k: 0.3315 - val_recall_3k: 0.4073 - val_recall_5k: 0.5138 - val_F1_1k: 0.2730 - val_F1_2k: 0.3080 - val_F1_3k: 0.3022 - val_F1_5k: 0.2725 - val_accuracy_1k: 0.3668 - val_accuracy_2k: 0.4910 - val_accuracy_3k: 0.5716 - val_accuracy_5k: 0.6702 - val_hamming_loss_k: 0.0050
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.0166 - precision_1k: 0.4042 - precision_2k: 0.3236 - precision_3k: 0.2675 - precision_5k: 0.2026 - recall_1k: 0.2353 - recall_2k: 0.3626 - recall_3k: 0.4422 - recall_5k: 0.5452 - F1_1k: 0.2973 - F1_2k: 0.3419 - F1_3k: 0.3333 - F1_5k: 0.2953 - accuracy_1k: 0.4042 - accuracy_2k: 0.5378 - accuracy_3k: 0.6134 - accuracy_5k: 0.7008 - hamming_loss_k: 0.0049
Epoch 00005: val_loss improved from 0.01745 to 0.01632, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0166 - precision_1k: 0.4042 - precision_2k: 0.3236 - precision_3k: 0.2675 - precision_5k: 0.2026 - recall_1k: 0.2353 - recall_2k: 0.3626 - recall_3k: 0.4422 - recall_5k: 0.5452 - F1_1k: 0.2973 - F1_2k: 0.3419 - F1_3k: 0.3333 - F1_5k: 0.2953 - accuracy_1k: 0.4042 - accuracy_2k: 0.5378 - accuracy_3k: 0.6134 - accuracy_5k: 0.7008 - hamming_loss_k: 0.0049 - val_loss: 0.0163 - val_precision_1k: 0.3898 - val_precision_2k: 0.3107 - val_precision_3k: 0.2596 - val_precision_5k: 0.1970 - val_recall_1k: 0.2374 - val_recall_2k: 0.3591 - val_recall_3k: 0.4422 - val_recall_5k: 0.5491 - val_F1_1k: 0.2950 - val_F1_2k: 0.3330 - val_F1_3k: 0.3270 - val_F1_5k: 0.2899 - val_accuracy_1k: 0.3898 - val_accuracy_2k: 0.5221 - val_accuracy_3k: 0.6061 - val_accuracy_5k: 0.6981 - val_hamming_loss_k: 0.0049
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.0156 - precision_1k: 0.4393 - precision_2k: 0.3477 - precision_3k: 0.2902 - precision_5k: 0.2190 - recall_1k: 0.2609 - recall_2k: 0.3950 - recall_3k: 0.4857 - recall_5k: 0.5946 - F1_1k: 0.3273 - F1_2k: 0.3698 - F1_3k: 0.3633 - F1_5k: 0.3201 - accuracy_1k: 0.4393 - accuracy_2k: 0.5761 - accuracy_3k: 0.6621 - accuracy_5k: 0.7484 - hamming_loss_k: 0.0048
Epoch 00006: val_loss improved from 0.01632 to 0.01547, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0156 - precision_1k: 0.4393 - precision_2k: 0.3477 - precision_3k: 0.2902 - precision_5k: 0.2190 - recall_1k: 0.2609 - recall_2k: 0.3950 - recall_3k: 0.4857 - recall_5k: 0.5946 - F1_1k: 0.3273 - F1_2k: 0.3698 - F1_3k: 0.3633 - F1_5k: 0.3201 - accuracy_1k: 0.4393 - accuracy_2k: 0.5761 - accuracy_3k: 0.6621 - accuracy_5k: 0.7484 - hamming_loss_k: 0.0048 - val_loss: 0.0155 - val_precision_1k: 0.4356 - val_precision_2k: 0.3426 - val_precision_3k: 0.2821 - val_precision_5k: 0.2125 - val_recall_1k: 0.2681 - val_recall_2k: 0.4040 - val_recall_3k: 0.4870 - val_recall_5k: 0.5929 - val_F1_1k: 0.3318 - val_F1_2k: 0.3706 - val_F1_3k: 0.3572 - val_F1_5k: 0.3128 - val_accuracy_1k: 0.4356 - val_accuracy_2k: 0.5811 - val_accuracy_3k: 0.6590 - val_accuracy_5k: 0.7440 - val_hamming_loss_k: 0.0047
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.0147 - precision_1k: 0.4781 - precision_2k: 0.3754 - precision_3k: 0.3099 - precision_5k: 0.2298 - recall_1k: 0.2890 - recall_2k: 0.4312 - recall_3k: 0.5213 - recall_5k: 0.6278 - F1_1k: 0.3601 - F1_2k: 0.4013 - F1_3k: 0.3886 - F1_5k: 0.3364 - accuracy_1k: 0.4781 - accuracy_2k: 0.6183 - accuracy_3k: 0.6969 - accuracy_5k: 0.7806 - hamming_loss_k: 0.0046
Epoch 00007: val_loss improved from 0.01547 to 0.01487, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0147 - precision_1k: 0.4781 - precision_2k: 0.3754 - precision_3k: 0.3099 - precision_5k: 0.2298 - recall_1k: 0.2890 - recall_2k: 0.4312 - recall_3k: 0.5213 - recall_5k: 0.6278 - F1_1k: 0.3601 - F1_2k: 0.4013 - F1_3k: 0.3886 - F1_5k: 0.3364 - accuracy_1k: 0.4781 - accuracy_2k: 0.6183 - accuracy_3k: 0.6969 - accuracy_5k: 0.7806 - hamming_loss_k: 0.0046 - val_loss: 0.0149 - val_precision_1k: 0.4648 - val_precision_2k: 0.3677 - val_precision_3k: 0.3034 - val_precision_5k: 0.2234 - val_recall_1k: 0.2890 - val_recall_2k: 0.4339 - val_recall_3k: 0.5258 - val_recall_5k: 0.6285 - val_F1_1k: 0.3563 - val_F1_2k: 0.3980 - val_F1_3k: 0.3847 - val_F1_5k: 0.3295 - val_accuracy_1k: 0.4648 - val_accuracy_2k: 0.6163 - val_accuracy_3k: 0.6988 - val_accuracy_5k: 0.7801 - val_hamming_loss_k: 0.0046
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.0141 - precision_1k: 0.5100 - precision_2k: 0.3979 - precision_3k: 0.3253 - precision_5k: 0.2384 - recall_1k: 0.3109 - recall_2k: 0.4594 - recall_3k: 0.5494 - recall_5k: 0.6521 - F1_1k: 0.3862 - F1_2k: 0.4264 - F1_3k: 0.4086 - F1_5k: 0.3491 - accuracy_1k: 0.5100 - accuracy_2k: 0.6506 - accuracy_3k: 0.7256 - accuracy_5k: 0.8004 - hamming_loss_k: 0.0044
Epoch 00008: val_loss did not improve from 0.01487
27/27 [==============================] - 10s 366ms/step - loss: 0.0141 - precision_1k: 0.5100 - precision_2k: 0.3979 - precision_3k: 0.3253 - precision_5k: 0.2384 - recall_1k: 0.3109 - recall_2k: 0.4594 - recall_3k: 0.5494 - recall_5k: 0.6521 - F1_1k: 0.3862 - F1_2k: 0.4264 - F1_3k: 0.4086 - F1_5k: 0.3491 - accuracy_1k: 0.5100 - accuracy_2k: 0.6506 - accuracy_3k: 0.7256 - accuracy_5k: 0.8004 - hamming_loss_k: 0.0044 - val_loss: 0.0149 - val_precision_1k: 0.4668 - val_precision_2k: 0.3703 - val_precision_3k: 0.3026 - val_precision_5k: 0.2223 - val_recall_1k: 0.2908 - val_recall_2k: 0.4384 - val_recall_3k: 0.5275 - val_recall_5k: 0.6282 - val_F1_1k: 0.3582 - val_F1_2k: 0.4013 - val_F1_3k: 0.3845 - val_F1_5k: 0.3283 - val_accuracy_1k: 0.4668 - val_accuracy_2k: 0.6165 - val_accuracy_3k: 0.6961 - val_accuracy_5k: 0.7781 - val_hamming_loss_k: 0.0046
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.0137 - precision_1k: 0.5199 - precision_2k: 0.4094 - precision_3k: 0.3327 - precision_5k: 0.2445 - recall_1k: 0.3179 - recall_2k: 0.4728 - recall_3k: 0.5633 - recall_5k: 0.6702 - F1_1k: 0.3944 - F1_2k: 0.4388 - F1_3k: 0.4183 - F1_5k: 0.3582 - accuracy_1k: 0.5199 - accuracy_2k: 0.6636 - accuracy_3k: 0.7358 - accuracy_5k: 0.8132 - hamming_loss_k: 0.0044
Epoch 00009: val_loss improved from 0.01487 to 0.01428, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0137 - precision_1k: 0.5199 - precision_2k: 0.4094 - precision_3k: 0.3327 - precision_5k: 0.2445 - recall_1k: 0.3179 - recall_2k: 0.4728 - recall_3k: 0.5633 - recall_5k: 0.6702 - F1_1k: 0.3944 - F1_2k: 0.4388 - F1_3k: 0.4183 - F1_5k: 0.3582 - accuracy_1k: 0.5199 - accuracy_2k: 0.6636 - accuracy_3k: 0.7358 - accuracy_5k: 0.8132 - hamming_loss_k: 0.0044 - val_loss: 0.0143 - val_precision_1k: 0.4879 - val_precision_2k: 0.3837 - val_precision_3k: 0.3117 - val_precision_5k: 0.2289 - val_recall_1k: 0.3032 - val_recall_2k: 0.4560 - val_recall_3k: 0.5407 - val_recall_5k: 0.6447 - val_F1_1k: 0.3739 - val_F1_2k: 0.4166 - val_F1_3k: 0.3953 - val_F1_5k: 0.3377 - val_accuracy_1k: 0.4879 - val_accuracy_2k: 0.6351 - val_accuracy_3k: 0.7114 - val_accuracy_5k: 0.7937 - val_hamming_loss_k: 0.0045
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.0132 - precision_1k: 0.5394 - precision_2k: 0.4243 - precision_3k: 0.3451 - precision_5k: 0.2518 - recall_1k: 0.3310 - recall_2k: 0.4926 - recall_3k: 0.5833 - recall_5k: 0.6905 - F1_1k: 0.4101 - F1_2k: 0.4559 - F1_3k: 0.4336 - F1_5k: 0.3690 - accuracy_1k: 0.5394 - accuracy_2k: 0.6863 - accuracy_3k: 0.7569 - accuracy_5k: 0.8329 - hamming_loss_k: 0.0043
Epoch 00010: val_loss improved from 0.01428 to 0.01403, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0132 - precision_1k: 0.5394 - precision_2k: 0.4243 - precision_3k: 0.3451 - precision_5k: 0.2518 - recall_1k: 0.3310 - recall_2k: 0.4926 - recall_3k: 0.5833 - recall_5k: 0.6905 - F1_1k: 0.4101 - F1_2k: 0.4559 - F1_3k: 0.4336 - F1_5k: 0.3690 - accuracy_1k: 0.5394 - accuracy_2k: 0.6863 - accuracy_3k: 0.7569 - accuracy_5k: 0.8329 - hamming_loss_k: 0.0043 - val_loss: 0.0140 - val_precision_1k: 0.5104 - val_precision_2k: 0.4000 - val_precision_3k: 0.3257 - val_precision_5k: 0.2334 - val_recall_1k: 0.3193 - val_recall_2k: 0.4746 - val_recall_3k: 0.5667 - val_recall_5k: 0.6585 - val_F1_1k: 0.3927 - val_F1_2k: 0.4340 - val_F1_3k: 0.4136 - val_F1_5k: 0.3446 - val_accuracy_1k: 0.5104 - val_accuracy_2k: 0.6567 - val_accuracy_3k: 0.7341 - val_accuracy_5k: 0.8074 - val_hamming_loss_k: 0.0044
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.0128 - precision_1k: 0.5564 - precision_2k: 0.4368 - precision_3k: 0.3554 - precision_5k: 0.2582 - recall_1k: 0.3433 - recall_2k: 0.5076 - recall_3k: 0.6024 - recall_5k: 0.7091 - F1_1k: 0.4245 - F1_2k: 0.4695 - F1_3k: 0.4470 - F1_5k: 0.3785 - accuracy_1k: 0.5564 - accuracy_2k: 0.7035 - accuracy_3k: 0.7764 - accuracy_5k: 0.8488 - hamming_loss_k: 0.0042
Epoch 00011: val_loss improved from 0.01403 to 0.01372, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0128 - precision_1k: 0.5564 - precision_2k: 0.4368 - precision_3k: 0.3554 - precision_5k: 0.2582 - recall_1k: 0.3433 - recall_2k: 0.5076 - recall_3k: 0.6024 - recall_5k: 0.7091 - F1_1k: 0.4245 - F1_2k: 0.4695 - F1_3k: 0.4470 - F1_5k: 0.3785 - accuracy_1k: 0.5564 - accuracy_2k: 0.7035 - accuracy_3k: 0.7764 - accuracy_5k: 0.8488 - hamming_loss_k: 0.0042 - val_loss: 0.0137 - val_precision_1k: 0.5185 - val_precision_2k: 0.4056 - val_precision_3k: 0.3323 - val_precision_5k: 0.2390 - val_recall_1k: 0.3270 - val_recall_2k: 0.4836 - val_recall_3k: 0.5768 - val_recall_5k: 0.6753 - val_F1_1k: 0.4009 - val_F1_2k: 0.4411 - val_F1_3k: 0.4215 - val_F1_5k: 0.3530 - val_accuracy_1k: 0.5185 - val_accuracy_2k: 0.6683 - val_accuracy_3k: 0.7483 - val_accuracy_5k: 0.8192 - val_hamming_loss_k: 0.0043
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.0126 - precision_1k: 0.5669 - precision_2k: 0.4435 - precision_3k: 0.3586 - precision_5k: 0.2608 - recall_1k: 0.3507 - recall_2k: 0.5169 - recall_3k: 0.6107 - recall_5k: 0.7168 - F1_1k: 0.4333 - F1_2k: 0.4773 - F1_3k: 0.4518 - F1_5k: 0.3824 - accuracy_1k: 0.5669 - accuracy_2k: 0.7140 - accuracy_3k: 0.7837 - accuracy_5k: 0.8552 - hamming_loss_k: 0.0042
Epoch 00012: val_loss improved from 0.01372 to 0.01355, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0126 - precision_1k: 0.5669 - precision_2k: 0.4435 - precision_3k: 0.3586 - precision_5k: 0.2608 - recall_1k: 0.3507 - recall_2k: 0.5169 - recall_3k: 0.6107 - recall_5k: 0.7168 - F1_1k: 0.4333 - F1_2k: 0.4773 - F1_3k: 0.4518 - F1_5k: 0.3824 - accuracy_1k: 0.5669 - accuracy_2k: 0.7140 - accuracy_3k: 0.7837 - accuracy_5k: 0.8552 - hamming_loss_k: 0.0042 - val_loss: 0.0135 - val_precision_1k: 0.5167 - val_precision_2k: 0.4039 - val_precision_3k: 0.3315 - val_precision_5k: 0.2416 - val_recall_1k: 0.3257 - val_recall_2k: 0.4827 - val_recall_3k: 0.5791 - val_recall_5k: 0.6815 - val_F1_1k: 0.3995 - val_F1_2k: 0.4396 - val_F1_3k: 0.4215 - val_F1_5k: 0.3566 - val_accuracy_1k: 0.5167 - val_accuracy_2k: 0.6678 - val_accuracy_3k: 0.7506 - val_accuracy_5k: 0.8287 - val_hamming_loss_k: 0.0043
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.0123 - precision_1k: 0.5827 - precision_2k: 0.4552 - precision_3k: 0.3690 - precision_5k: 0.2667 - recall_1k: 0.3626 - recall_2k: 0.5310 - recall_3k: 0.6267 - recall_5k: 0.7320 - F1_1k: 0.4469 - F1_2k: 0.4901 - F1_3k: 0.4645 - F1_5k: 0.3909 - accuracy_1k: 0.5827 - accuracy_2k: 0.7296 - accuracy_3k: 0.7985 - accuracy_5k: 0.8665 - hamming_loss_k: 0.0041
Epoch 00013: val_loss improved from 0.01355 to 0.01337, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0123 - precision_1k: 0.5827 - precision_2k: 0.4552 - precision_3k: 0.3690 - precision_5k: 0.2667 - recall_1k: 0.3626 - recall_2k: 0.5310 - recall_3k: 0.6267 - recall_5k: 0.7320 - F1_1k: 0.4469 - F1_2k: 0.4901 - F1_3k: 0.4645 - F1_5k: 0.3909 - accuracy_1k: 0.5827 - accuracy_2k: 0.7296 - accuracy_3k: 0.7985 - accuracy_5k: 0.8665 - hamming_loss_k: 0.0041 - val_loss: 0.0134 - val_precision_1k: 0.5308 - val_precision_2k: 0.4184 - val_precision_3k: 0.3382 - val_precision_5k: 0.2439 - val_recall_1k: 0.3350 - val_recall_2k: 0.4989 - val_recall_3k: 0.5901 - val_recall_5k: 0.6915 - val_F1_1k: 0.4107 - val_F1_2k: 0.4549 - val_F1_3k: 0.4298 - val_F1_5k: 0.3606 - val_accuracy_1k: 0.5308 - val_accuracy_2k: 0.6874 - val_accuracy_3k: 0.7582 - val_accuracy_5k: 0.8353 - val_hamming_loss_k: 0.0043
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.0120 - precision_1k: 0.5970 - precision_2k: 0.4673 - precision_3k: 0.3753 - precision_5k: 0.2711 - recall_1k: 0.3697 - recall_2k: 0.5427 - recall_3k: 0.6372 - recall_5k: 0.7443 - F1_1k: 0.4565 - F1_2k: 0.5021 - F1_3k: 0.4723 - F1_5k: 0.3974 - accuracy_1k: 0.5970 - accuracy_2k: 0.7438 - accuracy_3k: 0.8112 - accuracy_5k: 0.8795 - hamming_loss_k: 0.0040
Epoch 00014: val_loss improved from 0.01337 to 0.01319, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0120 - precision_1k: 0.5970 - precision_2k: 0.4673 - precision_3k: 0.3753 - precision_5k: 0.2711 - recall_1k: 0.3697 - recall_2k: 0.5427 - recall_3k: 0.6372 - recall_5k: 0.7443 - F1_1k: 0.4565 - F1_2k: 0.5021 - F1_3k: 0.4723 - F1_5k: 0.3974 - accuracy_1k: 0.5970 - accuracy_2k: 0.7438 - accuracy_3k: 0.8112 - accuracy_5k: 0.8795 - hamming_loss_k: 0.0040 - val_loss: 0.0132 - val_precision_1k: 0.5378 - val_precision_2k: 0.4288 - val_precision_3k: 0.3484 - val_precision_5k: 0.2490 - val_recall_1k: 0.3406 - val_recall_2k: 0.5098 - val_recall_3k: 0.6058 - val_recall_5k: 0.7010 - val_F1_1k: 0.4170 - val_F1_2k: 0.4657 - val_F1_3k: 0.4422 - val_F1_5k: 0.3673 - val_accuracy_1k: 0.5378 - val_accuracy_2k: 0.6994 - val_accuracy_3k: 0.7752 - val_accuracy_5k: 0.8441 - val_hamming_loss_k: 0.0042
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.0116 - precision_1k: 0.6098 - precision_2k: 0.4736 - precision_3k: 0.3825 - precision_5k: 0.2751 - recall_1k: 0.3799 - recall_2k: 0.5545 - recall_3k: 0.6514 - recall_5k: 0.7554 - F1_1k: 0.4681 - F1_2k: 0.5108 - F1_3k: 0.4820 - F1_5k: 0.4032 - accuracy_1k: 0.6098 - accuracy_2k: 0.7553 - accuracy_3k: 0.8239 - accuracy_5k: 0.8875 - hamming_loss_k: 0.0040 ETA: 1s - loss: 0.0116 - precision_1k: 0.6145 - precision_2k: 0.4758 - precision_3k: 0.3836 - precision_5k: 0.2758 - recall_1k: 0.3831 - recall_2k: 0.5583 - recall_3k: 0.6546 - recall_5k: 0.7590 - F1_1k: 0.4719 - F1_2k: 0.5137 - F1_3k: 0.4837 - F1_5k: 0.4046 - accuracy_1k: 0.6145 - accuracy_2k: 0.7589 - accuracy_3k: 0.8270 - accuracy_5k: 0.8906 - hamming_loss_k
Epoch 00015: val_loss improved from 0.01319 to 0.01315, saving model to logs/noypwv-lab-0604-132833/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0116 - precision_1k: 0.6098 - precision_2k: 0.4736 - precision_3k: 0.3825 - precision_5k: 0.2751 - recall_1k: 0.3799 - recall_2k: 0.5545 - recall_3k: 0.6514 - recall_5k: 0.7554 - F1_1k: 0.4681 - F1_2k: 0.5108 - F1_3k: 0.4820 - F1_5k: 0.4032 - accuracy_1k: 0.6098 - accuracy_2k: 0.7553 - accuracy_3k: 0.8239 - accuracy_5k: 0.8875 - hamming_loss_k: 0.0040 - val_loss: 0.0132 - val_precision_1k: 0.5503 - val_precision_2k: 0.4282 - val_precision_3k: 0.3452 - val_precision_5k: 0.2485 - val_recall_1k: 0.3444 - val_recall_2k: 0.5094 - val_recall_3k: 0.6036 - val_recall_5k: 0.7044 - val_F1_1k: 0.4236 - val_F1_2k: 0.4652 - val_F1_3k: 0.4391 - val_F1_5k: 0.3673 - val_accuracy_1k: 0.5503 - val_accuracy_2k: 0.7025 - val_accuracy_3k: 0.7750 - val_accuracy_5k: 0.8453 - val_hamming_loss_k: 0.0042
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.0113 - precision_1k: 0.6290 - precision_2k: 0.4875 - precision_3k: 0.3913 - precision_5k: 0.2812 - recall_1k: 0.3936 - recall_2k: 0.5698 - recall_3k: 0.6650 - recall_5k: 0.7726 - F1_1k: 0.4841 - F1_2k: 0.5254 - F1_3k: 0.4926 - F1_5k: 0.4124 - accuracy_1k: 0.6290 - accuracy_2k: 0.7698 - accuracy_3k: 0.8342 - accuracy_5k: 0.8991 - hamming_loss_k: 0.0039
Epoch 00016: val_loss did not improve from 0.01315
27/27 [==============================] - 10s 367ms/step - loss: 0.0113 - precision_1k: 0.6290 - precision_2k: 0.4875 - precision_3k: 0.3913 - precision_5k: 0.2812 - recall_1k: 0.3936 - recall_2k: 0.5698 - recall_3k: 0.6650 - recall_5k: 0.7726 - F1_1k: 0.4841 - F1_2k: 0.5254 - F1_3k: 0.4926 - F1_5k: 0.4124 - accuracy_1k: 0.6290 - accuracy_2k: 0.7698 - accuracy_3k: 0.8342 - accuracy_5k: 0.8991 - hamming_loss_k: 0.0039 - val_loss: 0.0133 - val_precision_1k: 0.5577 - val_precision_2k: 0.4270 - val_precision_3k: 0.3460 - val_precision_5k: 0.2468 - val_recall_1k: 0.3515 - val_recall_2k: 0.5124 - val_recall_3k: 0.6054 - val_recall_5k: 0.7014 - val_F1_1k: 0.4311 - val_F1_2k: 0.4657 - val_F1_3k: 0.4402 - val_F1_5k: 0.3650 - val_accuracy_1k: 0.5577 - val_accuracy_2k: 0.7026 - val_accuracy_3k: 0.7699 - val_accuracy_5k: 0.8404 - val_hamming_loss_k: 0.0041
Epoch 00016: early stopping
176/176 [==============================] - 8s 40ms/step - loss: 0.0120 - precision_1k: 0.5961 - precision_2k: 0.4629 - precision_3k: 0.3734 - precision_5k: 0.2698 - recall_1k: 0.3745 - recall_2k: 0.5487 - recall_3k: 0.6429 - recall_5k: 0.7514 - F1_1k: 0.4588 - F1_2k: 0.5011 - F1_3k: 0.4715 - F1_5k: 0.3963 - accuracy_1k: 0.5961 - accuracy_2k: 0.7449 - accuracy_3k: 0.8121 - accuracy_5k: 0.8798 - hamming_loss_k: 0.0040 0s - loss: 0.0120 - precision_1k: 0.5954 - precision_2k: 0.4631 - precision_3k: 0.3732 - precision_5k: 0.2698 - recall_1k: 0.3733 - recall_2k: 0.5482 - recall_3k: 0.6422 - recall_5k: 0.7511 - F1_1k: 0.4577 - F1_2k: 0.5010 - F1_3k: 0.4711 - F1_5k: 0.3964 - accuracy_1k: 0.5954 - accuracy_2k: 0.7447 - accuracy_3k: 0.8121 - accuracy_5k: 0.8799 - hamming_loss_k: 0.
Best model result:  [0.01199701614677906, 0.5961385369300842, 0.4629022479057312, 0.3733745515346527, 0.26976144313812256, 0.37446871399879456, 0.5487017631530762, 0.6429423689842224, 0.7514296174049377, 0.4588415026664734, 0.5010805130004883, 0.4714859127998352, 0.3963235020637512, 0.5961385369300842, 0.7448624968528748, 0.8120739459991455, 0.8798190951347351, 0.003993440885096788]
13498
3375
5625
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_5 (Lambda)              (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_5[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_6 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_2 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_6[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_2[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_3 (Dense)                (None, 1024)         2098176     ['tf.concat_1[0][0]']            
                                                                                                  
 dense_4 (Dense)                (None, 15)           15375       ['dense_3[0][0]']                
                                                                                                  
 tf.nn.softmax_1 (TFOpLambda)   (None, 15)           0           ['dense_4[0][0]']                
                                                                                                  
 tf.expand_dims_2 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_1[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_2[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_4 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_7 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_4[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 15, 150)      22650       ['lambda_7[0][0]']               
                                                                                                  
 tf.math.reduce_mean_3 (TFOpLam  (None, 150)         0           ['dense_5[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_3 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_3[0][0]']  
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_1 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_3[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_5 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_1[0][0]']     
                                                                                                  
 lambda_8 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_5[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_1[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.0828 - precision_1k: 0.0450 - precision_2k: 0.0412 - precision_3k: 0.0365 - precision_5k: 0.0317 - recall_1k: 0.0214 - recall_2k: 0.0408 - recall_3k: 0.0554 - recall_5k: 0.0811 - F1_1k: 0.0288 - F1_2k: 0.0409 - F1_3k: 0.0440 - F1_5k: 0.0455 - accuracy_1k: 0.0450 - accuracy_2k: 0.0816 - accuracy_3k: 0.1074 - accuracy_5k: 0.1521 - hamming_loss_k: 0.0066
Epoch 00001: val_loss improved from inf to 0.02607, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 12s 398ms/step - loss: 0.0828 - precision_1k: 0.0450 - precision_2k: 0.0412 - precision_3k: 0.0365 - precision_5k: 0.0317 - recall_1k: 0.0214 - recall_2k: 0.0408 - recall_3k: 0.0554 - recall_5k: 0.0811 - F1_1k: 0.0288 - F1_2k: 0.0409 - F1_3k: 0.0440 - F1_5k: 0.0455 - accuracy_1k: 0.0450 - accuracy_2k: 0.0816 - accuracy_3k: 0.1074 - accuracy_5k: 0.1521 - hamming_loss_k: 0.0066 - val_loss: 0.0261 - val_precision_1k: 0.0841 - val_precision_2k: 0.0834 - val_precision_3k: 0.0771 - val_precision_5k: 0.0596 - val_recall_1k: 0.0381 - val_recall_2k: 0.0773 - val_recall_3k: 0.1137 - val_recall_5k: 0.1514 - val_F1_1k: 0.0524 - val_F1_2k: 0.0802 - val_F1_3k: 0.0918 - val_F1_5k: 0.0855 - val_accuracy_1k: 0.0841 - val_accuracy_2k: 0.1668 - val_accuracy_3k: 0.2282 - val_accuracy_5k: 0.2847 - val_hamming_loss_k: 0.0064
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.0246 - precision_1k: 0.1292 - precision_2k: 0.1218 - precision_3k: 0.1081 - precision_5k: 0.0857 - recall_1k: 0.0604 - recall_2k: 0.1176 - recall_3k: 0.1607 - recall_5k: 0.2138 - F1_1k: 0.0822 - F1_2k: 0.1196 - F1_3k: 0.1292 - F1_5k: 0.1223 - accuracy_1k: 0.1292 - accuracy_2k: 0.2141 - accuracy_3k: 0.2757 - accuracy_5k: 0.3433 - hamming_loss_k: 0.0062
Epoch 00002: val_loss improved from 0.02607 to 0.02323, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 396ms/step - loss: 0.0246 - precision_1k: 0.1292 - precision_2k: 0.1218 - precision_3k: 0.1081 - precision_5k: 0.0857 - recall_1k: 0.0604 - recall_2k: 0.1176 - recall_3k: 0.1607 - recall_5k: 0.2138 - F1_1k: 0.0822 - F1_2k: 0.1196 - F1_3k: 0.1292 - F1_5k: 0.1223 - accuracy_1k: 0.1292 - accuracy_2k: 0.2141 - accuracy_3k: 0.2757 - accuracy_5k: 0.3433 - hamming_loss_k: 0.0062 - val_loss: 0.0232 - val_precision_1k: 0.2003 - val_precision_2k: 0.1767 - val_precision_3k: 0.1529 - val_precision_5k: 0.1217 - val_recall_1k: 0.0965 - val_recall_2k: 0.1715 - val_recall_3k: 0.2299 - val_recall_5k: 0.3032 - val_F1_1k: 0.1302 - val_F1_2k: 0.1739 - val_F1_3k: 0.1835 - val_F1_5k: 0.1736 - val_accuracy_1k: 0.2003 - val_accuracy_2k: 0.2886 - val_accuracy_3k: 0.3617 - val_accuracy_5k: 0.4384 - val_hamming_loss_k: 0.0059
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.0217 - precision_1k: 0.2487 - precision_2k: 0.2021 - precision_3k: 0.1704 - precision_5k: 0.1377 - recall_1k: 0.1336 - recall_2k: 0.2116 - recall_3k: 0.2651 - recall_5k: 0.3524 - F1_1k: 0.1737 - F1_2k: 0.2067 - F1_3k: 0.2073 - F1_5k: 0.1980 - accuracy_1k: 0.2487 - accuracy_2k: 0.3486 - accuracy_3k: 0.4091 - accuracy_5k: 0.4903 - hamming_loss_k: 0.0056
Epoch 00003: val_loss improved from 0.02323 to 0.01990, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0217 - precision_1k: 0.2487 - precision_2k: 0.2021 - precision_3k: 0.1704 - precision_5k: 0.1377 - recall_1k: 0.1336 - recall_2k: 0.2116 - recall_3k: 0.2651 - recall_5k: 0.3524 - F1_1k: 0.1737 - F1_2k: 0.2067 - F1_3k: 0.2073 - F1_5k: 0.1980 - accuracy_1k: 0.2487 - accuracy_2k: 0.3486 - accuracy_3k: 0.4091 - accuracy_5k: 0.4903 - hamming_loss_k: 0.0056 - val_loss: 0.0199 - val_precision_1k: 0.3207 - val_precision_2k: 0.2603 - val_precision_3k: 0.2166 - val_precision_5k: 0.1700 - val_recall_1k: 0.1773 - val_recall_2k: 0.2875 - val_recall_3k: 0.3506 - val_recall_5k: 0.4504 - val_F1_1k: 0.2281 - val_F1_2k: 0.2730 - val_F1_3k: 0.2676 - val_F1_5k: 0.2467 - val_accuracy_1k: 0.3207 - val_accuracy_2k: 0.4525 - val_accuracy_3k: 0.5173 - val_accuracy_5k: 0.6001 - val_hamming_loss_k: 0.0053
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.0187 - precision_1k: 0.3353 - precision_2k: 0.2723 - precision_3k: 0.2277 - precision_5k: 0.1768 - recall_1k: 0.1922 - recall_2k: 0.3051 - recall_3k: 0.3757 - recall_5k: 0.4736 - F1_1k: 0.2442 - F1_2k: 0.2877 - F1_3k: 0.2834 - F1_5k: 0.2574 - accuracy_1k: 0.3353 - accuracy_2k: 0.4640 - accuracy_3k: 0.5377 - accuracy_5k: 0.6241 - hamming_loss_k: 0.0052 ETA: 3s - loss: 0.0192 - precision_1k: 0.3154 - precision_2k: 0.2604 - precision_3k: 0.2183 - precision_5k: 0.1710 - recall_1k: 0.1785 - recall_2k: 0.2894 - recall_3k: 0.3571 - recall_5k: 0.4537 - F1_1k: 0.2279 - F1_2k: 0.2741 - F1_3k: 0.2710 - F1_5k: 0.2484 - accuracy_1k: 0.3154 - accuracy_2k: 0.4446 - accuracy_3k: 0.5153 - accuracy_5k: 0.6019 - 
Epoch 00004: val_loss improved from 0.01990 to 0.01763, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 397ms/step - loss: 0.0187 - precision_1k: 0.3353 - precision_2k: 0.2723 - precision_3k: 0.2277 - precision_5k: 0.1768 - recall_1k: 0.1922 - recall_2k: 0.3051 - recall_3k: 0.3757 - recall_5k: 0.4736 - F1_1k: 0.2442 - F1_2k: 0.2877 - F1_3k: 0.2834 - F1_5k: 0.2574 - accuracy_1k: 0.3353 - accuracy_2k: 0.4640 - accuracy_3k: 0.5377 - accuracy_5k: 0.6241 - hamming_loss_k: 0.0052 - val_loss: 0.0176 - val_precision_1k: 0.3881 - val_precision_2k: 0.3047 - val_precision_3k: 0.2569 - val_precision_5k: 0.1937 - val_recall_1k: 0.2280 - val_recall_2k: 0.3453 - val_recall_3k: 0.4234 - val_recall_5k: 0.5194 - val_F1_1k: 0.2871 - val_F1_2k: 0.3235 - val_F1_3k: 0.3196 - val_F1_5k: 0.2821 - val_accuracy_1k: 0.3881 - val_accuracy_2k: 0.5146 - val_accuracy_3k: 0.5903 - val_accuracy_5k: 0.6722 - val_hamming_loss_k: 0.0050
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.0168 - precision_1k: 0.3949 - precision_2k: 0.3157 - precision_3k: 0.2620 - precision_5k: 0.1998 - recall_1k: 0.2331 - recall_2k: 0.3570 - recall_3k: 0.4362 - recall_5k: 0.5406 - F1_1k: 0.2931 - F1_2k: 0.3350 - F1_3k: 0.3273 - F1_5k: 0.2917 - accuracy_1k: 0.3949 - accuracy_2k: 0.5327 - accuracy_3k: 0.6059 - accuracy_5k: 0.6974 - hamming_loss_k: 0.0050
Epoch 00005: val_loss improved from 0.01763 to 0.01629, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0168 - precision_1k: 0.3949 - precision_2k: 0.3157 - precision_3k: 0.2620 - precision_5k: 0.1998 - recall_1k: 0.2331 - recall_2k: 0.3570 - recall_3k: 0.4362 - recall_5k: 0.5406 - F1_1k: 0.2931 - F1_2k: 0.3350 - F1_3k: 0.3273 - F1_5k: 0.2917 - accuracy_1k: 0.3949 - accuracy_2k: 0.5327 - accuracy_3k: 0.6059 - accuracy_5k: 0.6974 - hamming_loss_k: 0.0050 - val_loss: 0.0163 - val_precision_1k: 0.4119 - val_precision_2k: 0.3399 - val_precision_3k: 0.2799 - val_precision_5k: 0.2097 - val_recall_1k: 0.2399 - val_recall_2k: 0.3827 - val_recall_3k: 0.4647 - val_recall_5k: 0.5659 - val_F1_1k: 0.3031 - val_F1_2k: 0.3599 - val_F1_3k: 0.3493 - val_F1_5k: 0.3059 - val_accuracy_1k: 0.4119 - val_accuracy_2k: 0.5563 - val_accuracy_3k: 0.6285 - val_accuracy_5k: 0.7130 - val_hamming_loss_k: 0.0049
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.0156 - precision_1k: 0.4380 - precision_2k: 0.3475 - precision_3k: 0.2865 - precision_5k: 0.2152 - recall_1k: 0.2595 - recall_2k: 0.3947 - recall_3k: 0.4786 - recall_5k: 0.5865 - F1_1k: 0.3258 - F1_2k: 0.3696 - F1_3k: 0.3584 - F1_5k: 0.3148 - accuracy_1k: 0.4380 - accuracy_2k: 0.5762 - accuracy_3k: 0.6531 - accuracy_5k: 0.7413 - hamming_loss_k: 0.0048
Epoch 00006: val_loss improved from 0.01629 to 0.01552, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0156 - precision_1k: 0.4380 - precision_2k: 0.3475 - precision_3k: 0.2865 - precision_5k: 0.2152 - recall_1k: 0.2595 - recall_2k: 0.3947 - recall_3k: 0.4786 - recall_5k: 0.5865 - F1_1k: 0.3258 - F1_2k: 0.3696 - F1_3k: 0.3584 - F1_5k: 0.3148 - accuracy_1k: 0.4380 - accuracy_2k: 0.5762 - accuracy_3k: 0.6531 - accuracy_5k: 0.7413 - hamming_loss_k: 0.0048 - val_loss: 0.0155 - val_precision_1k: 0.4472 - val_precision_2k: 0.3556 - val_precision_3k: 0.2952 - val_precision_5k: 0.2225 - val_recall_1k: 0.2632 - val_recall_2k: 0.4035 - val_recall_3k: 0.4917 - val_recall_5k: 0.5988 - val_F1_1k: 0.3312 - val_F1_2k: 0.3779 - val_F1_3k: 0.3687 - val_F1_5k: 0.3243 - val_accuracy_1k: 0.4472 - val_accuracy_2k: 0.5819 - val_accuracy_3k: 0.6670 - val_accuracy_5k: 0.7482 - val_hamming_loss_k: 0.0047
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.0148 - precision_1k: 0.4770 - precision_2k: 0.3736 - precision_3k: 0.3053 - precision_5k: 0.2267 - recall_1k: 0.2881 - recall_2k: 0.4305 - recall_3k: 0.5162 - recall_5k: 0.6227 - F1_1k: 0.3592 - F1_2k: 0.4000 - F1_3k: 0.3836 - F1_5k: 0.3324 - accuracy_1k: 0.4770 - accuracy_2k: 0.6137 - accuracy_3k: 0.6892 - accuracy_5k: 0.7750 - hamming_loss_k: 0.0046
Epoch 00007: val_loss improved from 0.01552 to 0.01494, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0148 - precision_1k: 0.4770 - precision_2k: 0.3736 - precision_3k: 0.3053 - precision_5k: 0.2267 - recall_1k: 0.2881 - recall_2k: 0.4305 - recall_3k: 0.5162 - recall_5k: 0.6227 - F1_1k: 0.3592 - F1_2k: 0.4000 - F1_3k: 0.3836 - F1_5k: 0.3324 - accuracy_1k: 0.4770 - accuracy_2k: 0.6137 - accuracy_3k: 0.6892 - accuracy_5k: 0.7750 - hamming_loss_k: 0.0046 - val_loss: 0.0149 - val_precision_1k: 0.4800 - val_precision_2k: 0.3780 - val_precision_3k: 0.3127 - val_precision_5k: 0.2295 - val_recall_1k: 0.2872 - val_recall_2k: 0.4329 - val_recall_3k: 0.5250 - val_recall_5k: 0.6269 - val_F1_1k: 0.3591 - val_F1_2k: 0.4034 - val_F1_3k: 0.3918 - val_F1_5k: 0.3359 - val_accuracy_1k: 0.4800 - val_accuracy_2k: 0.6201 - val_accuracy_3k: 0.6986 - val_accuracy_5k: 0.7783 - val_hamming_loss_k: 0.0046
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.0141 - precision_1k: 0.4957 - precision_2k: 0.3924 - precision_3k: 0.3217 - precision_5k: 0.2364 - recall_1k: 0.3024 - recall_2k: 0.4543 - recall_3k: 0.5451 - recall_5k: 0.6499 - F1_1k: 0.3755 - F1_2k: 0.4210 - F1_3k: 0.4046 - F1_5k: 0.3466 - accuracy_1k: 0.4957 - accuracy_2k: 0.6399 - accuracy_3k: 0.7182 - accuracy_5k: 0.7991 - hamming_loss_k: 0.0045 ETA: 0s - loss: 0.0141 - precision_1k: 0.4956 - precision_2k: 0.3920 - precision_3k: 0.3216 - precision_5k: 0.2364 - recall_1k: 0.3017 - recall_2k: 0.4531 - recall_3k: 0.5443 - recall_5k: 0.6492 - F1_1k: 0.3750 - F1_2k: 0.4202 - F1_3k: 0.4042 - F1_5k: 0.3465 - accuracy_1k: 0.4956 - accuracy_2k: 0.6391 - accuracy_3k: 0.7183 - accuracy_5k: 0.7986 - hamming_loss_k: 0.00
Epoch 00008: val_loss improved from 0.01494 to 0.01480, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0141 - precision_1k: 0.4957 - precision_2k: 0.3924 - precision_3k: 0.3217 - precision_5k: 0.2364 - recall_1k: 0.3024 - recall_2k: 0.4543 - recall_3k: 0.5451 - recall_5k: 0.6499 - F1_1k: 0.3755 - F1_2k: 0.4210 - F1_3k: 0.4046 - F1_5k: 0.3466 - accuracy_1k: 0.4957 - accuracy_2k: 0.6399 - accuracy_3k: 0.7182 - accuracy_5k: 0.7991 - hamming_loss_k: 0.0045 - val_loss: 0.0148 - val_precision_1k: 0.4886 - val_precision_2k: 0.3818 - val_precision_3k: 0.3081 - val_precision_5k: 0.2293 - val_recall_1k: 0.2964 - val_recall_2k: 0.4427 - val_recall_3k: 0.5215 - val_recall_5k: 0.6264 - val_F1_1k: 0.3689 - val_F1_2k: 0.4098 - val_F1_3k: 0.3872 - val_F1_5k: 0.3356 - val_accuracy_1k: 0.4886 - val_accuracy_2k: 0.6297 - val_accuracy_3k: 0.7003 - val_accuracy_5k: 0.7817 - val_hamming_loss_k: 0.0045
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.0137 - precision_1k: 0.5182 - precision_2k: 0.4051 - precision_3k: 0.3305 - precision_5k: 0.2430 - recall_1k: 0.3176 - recall_2k: 0.4727 - recall_3k: 0.5628 - recall_5k: 0.6711 - F1_1k: 0.3938 - F1_2k: 0.4363 - F1_3k: 0.4164 - F1_5k: 0.3567 - accuracy_1k: 0.5182 - accuracy_2k: 0.6652 - accuracy_3k: 0.7394 - accuracy_5k: 0.8193 - hamming_loss_k: 0.0044 ETA: 6s - loss: 0.0138 - precision_1k: 0.5163 - precision_2k: 0.4076 - precision_3k: 0.3308 - precision_5k: 0.2411 - recall_1k: 0.3177 - recall_2k: 0.4742 - recall_3k: 0.5652 - recall_5k: 0.6658 - F1_1k: 0.3933 - F1_2k: 0.4383 - F1_3k: 0.4173 - F1_5k: 0.3540 - accuracy_1k: 0.5163 - accuracy_2k: 0.6657 - accuracy_3k: 0.7409 - accura - ETA: 0s - loss: 0.0137 - precision_1k: 0.5189 - precision_2k: 0.4045 - precision_3k: 0.3299 - precision_5k: 0.2426 - recall_1k: 0.3187 - recall_2k: 0.4723 - recall_3k: 0.5624 - recall_5k: 0.6703 - F1_1k: 0.3948 - F1_2k: 0.4357 - F1_3k: 0.4158 - F1_5k: 0.3563 - accuracy_1k: 0.5189 - accuracy_2k: 0.6645 - accuracy_3k: 0.7394 - accuracy_5k: 0.8184 - hamming_loss_k: 0.
Epoch 00009: val_loss improved from 0.01480 to 0.01431, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0137 - precision_1k: 0.5182 - precision_2k: 0.4051 - precision_3k: 0.3305 - precision_5k: 0.2430 - recall_1k: 0.3176 - recall_2k: 0.4727 - recall_3k: 0.5628 - recall_5k: 0.6711 - F1_1k: 0.3938 - F1_2k: 0.4363 - F1_3k: 0.4164 - F1_5k: 0.3567 - accuracy_1k: 0.5182 - accuracy_2k: 0.6652 - accuracy_3k: 0.7394 - accuracy_5k: 0.8193 - hamming_loss_k: 0.0044 - val_loss: 0.0143 - val_precision_1k: 0.4970 - val_precision_2k: 0.3984 - val_precision_3k: 0.3297 - val_precision_5k: 0.2417 - val_recall_1k: 0.2983 - val_recall_2k: 0.4587 - val_recall_3k: 0.5554 - val_recall_5k: 0.6604 - val_F1_1k: 0.3728 - val_F1_2k: 0.4263 - val_F1_3k: 0.4136 - val_F1_5k: 0.3538 - val_accuracy_1k: 0.4970 - val_accuracy_2k: 0.6492 - val_accuracy_3k: 0.7338 - val_accuracy_5k: 0.8077 - val_hamming_loss_k: 0.0045
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.0132 - precision_1k: 0.5434 - precision_2k: 0.4248 - precision_3k: 0.3436 - precision_5k: 0.2506 - recall_1k: 0.3361 - recall_2k: 0.4966 - recall_3k: 0.5863 - recall_5k: 0.6919 - F1_1k: 0.4153 - F1_2k: 0.4578 - F1_3k: 0.4332 - F1_5k: 0.3679 - accuracy_1k: 0.5434 - accuracy_2k: 0.6897 - accuracy_3k: 0.7624 - accuracy_5k: 0.8360 - hamming_loss_k: 0.0043
Epoch 00010: val_loss improved from 0.01431 to 0.01393, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0132 - precision_1k: 0.5434 - precision_2k: 0.4248 - precision_3k: 0.3436 - precision_5k: 0.2506 - recall_1k: 0.3361 - recall_2k: 0.4966 - recall_3k: 0.5863 - recall_5k: 0.6919 - F1_1k: 0.4153 - F1_2k: 0.4578 - F1_3k: 0.4332 - F1_5k: 0.3679 - accuracy_1k: 0.5434 - accuracy_2k: 0.6897 - accuracy_3k: 0.7624 - accuracy_5k: 0.8360 - hamming_loss_k: 0.0043 - val_loss: 0.0139 - val_precision_1k: 0.5307 - val_precision_2k: 0.4064 - val_precision_3k: 0.3327 - val_precision_5k: 0.2461 - val_recall_1k: 0.3230 - val_recall_2k: 0.4708 - val_recall_3k: 0.5643 - val_recall_5k: 0.6716 - val_F1_1k: 0.4014 - val_F1_2k: 0.4361 - val_F1_3k: 0.4184 - val_F1_5k: 0.3601 - val_accuracy_1k: 0.5307 - val_accuracy_2k: 0.6674 - val_accuracy_3k: 0.7455 - val_accuracy_5k: 0.8174 - val_hamming_loss_k: 0.0043
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.0128 - precision_1k: 0.5567 - precision_2k: 0.4319 - precision_3k: 0.3523 - precision_5k: 0.2563 - recall_1k: 0.3461 - recall_2k: 0.5062 - recall_3k: 0.6029 - recall_5k: 0.7087 - F1_1k: 0.4268 - F1_2k: 0.4660 - F1_3k: 0.4447 - F1_5k: 0.3764 - accuracy_1k: 0.5567 - accuracy_2k: 0.7022 - accuracy_3k: 0.7778 - accuracy_5k: 0.8503 - hamming_loss_k: 0.0042
Epoch 00011: val_loss improved from 0.01393 to 0.01379, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0128 - precision_1k: 0.5567 - precision_2k: 0.4319 - precision_3k: 0.3523 - precision_5k: 0.2563 - recall_1k: 0.3461 - recall_2k: 0.5062 - recall_3k: 0.6029 - recall_5k: 0.7087 - F1_1k: 0.4268 - F1_2k: 0.4660 - F1_3k: 0.4447 - F1_5k: 0.3764 - accuracy_1k: 0.5567 - accuracy_2k: 0.7022 - accuracy_3k: 0.7778 - accuracy_5k: 0.8503 - hamming_loss_k: 0.0042 - val_loss: 0.0138 - val_precision_1k: 0.5355 - val_precision_2k: 0.4208 - val_precision_3k: 0.3441 - val_precision_5k: 0.2505 - val_recall_1k: 0.3282 - val_recall_2k: 0.4891 - val_recall_3k: 0.5823 - val_recall_5k: 0.6890 - val_F1_1k: 0.4069 - val_F1_2k: 0.4523 - val_F1_3k: 0.4324 - val_F1_5k: 0.3673 - val_accuracy_1k: 0.5355 - val_accuracy_2k: 0.6872 - val_accuracy_3k: 0.7601 - val_accuracy_5k: 0.8365 - val_hamming_loss_k: 0.0043
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.0125 - precision_1k: 0.5704 - precision_2k: 0.4455 - precision_3k: 0.3608 - precision_5k: 0.2615 - recall_1k: 0.3564 - recall_2k: 0.5247 - recall_3k: 0.6179 - recall_5k: 0.7222 - F1_1k: 0.4386 - F1_2k: 0.4818 - F1_3k: 0.4555 - F1_5k: 0.3839 - accuracy_1k: 0.5704 - accuracy_2k: 0.7231 - accuracy_3k: 0.7926 - accuracy_5k: 0.8612 - hamming_loss_k: 0.0041
Epoch 00012: val_loss improved from 0.01379 to 0.01363, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0125 - precision_1k: 0.5704 - precision_2k: 0.4455 - precision_3k: 0.3608 - precision_5k: 0.2615 - recall_1k: 0.3564 - recall_2k: 0.5247 - recall_3k: 0.6179 - recall_5k: 0.7222 - F1_1k: 0.4386 - F1_2k: 0.4818 - F1_3k: 0.4555 - F1_5k: 0.3839 - accuracy_1k: 0.5704 - accuracy_2k: 0.7231 - accuracy_3k: 0.7926 - accuracy_5k: 0.8612 - hamming_loss_k: 0.0041 - val_loss: 0.0136 - val_precision_1k: 0.5369 - val_precision_2k: 0.4199 - val_precision_3k: 0.3433 - val_precision_5k: 0.2503 - val_recall_1k: 0.3262 - val_recall_2k: 0.4828 - val_recall_3k: 0.5783 - val_recall_5k: 0.6863 - val_F1_1k: 0.4057 - val_F1_2k: 0.4490 - val_F1_3k: 0.4307 - val_F1_5k: 0.3667 - val_accuracy_1k: 0.5369 - val_accuracy_2k: 0.6816 - val_accuracy_3k: 0.7550 - val_accuracy_5k: 0.8315 - val_hamming_loss_k: 0.0043
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.0121 - precision_1k: 0.5838 - precision_2k: 0.4549 - precision_3k: 0.3678 - precision_5k: 0.2654 - recall_1k: 0.3647 - recall_2k: 0.5351 - recall_3k: 0.6307 - recall_5k: 0.7345 - F1_1k: 0.4489 - F1_2k: 0.4917 - F1_3k: 0.4646 - F1_5k: 0.3898 - accuracy_1k: 0.5838 - accuracy_2k: 0.7324 - accuracy_3k: 0.8028 - accuracy_5k: 0.8710 - hamming_loss_k: 0.0041
Epoch 00013: val_loss improved from 0.01363 to 0.01345, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0121 - precision_1k: 0.5838 - precision_2k: 0.4549 - precision_3k: 0.3678 - precision_5k: 0.2654 - recall_1k: 0.3647 - recall_2k: 0.5351 - recall_3k: 0.6307 - recall_5k: 0.7345 - F1_1k: 0.4489 - F1_2k: 0.4917 - F1_3k: 0.4646 - F1_5k: 0.3898 - accuracy_1k: 0.5838 - accuracy_2k: 0.7324 - accuracy_3k: 0.8028 - accuracy_5k: 0.8710 - hamming_loss_k: 0.0041 - val_loss: 0.0135 - val_precision_1k: 0.5524 - val_precision_2k: 0.4297 - val_precision_3k: 0.3466 - val_precision_5k: 0.2501 - val_recall_1k: 0.3413 - val_recall_2k: 0.5020 - val_recall_3k: 0.5885 - val_recall_5k: 0.6862 - val_F1_1k: 0.4218 - val_F1_2k: 0.4629 - val_F1_3k: 0.4361 - val_F1_5k: 0.3664 - val_accuracy_1k: 0.5524 - val_accuracy_2k: 0.7029 - val_accuracy_3k: 0.7642 - val_accuracy_5k: 0.8340 - val_hamming_loss_k: 0.0042
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.0118 - precision_1k: 0.5978 - precision_2k: 0.4625 - precision_3k: 0.3741 - precision_5k: 0.2711 - recall_1k: 0.3753 - recall_2k: 0.5466 - recall_3k: 0.6432 - recall_5k: 0.7505 - F1_1k: 0.4610 - F1_2k: 0.5010 - F1_3k: 0.4730 - F1_5k: 0.3983 - accuracy_1k: 0.5978 - accuracy_2k: 0.7443 - accuracy_3k: 0.8157 - accuracy_5k: 0.8830 - hamming_loss_k: 0.0040
Epoch 00014: val_loss improved from 0.01345 to 0.01344, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0118 - precision_1k: 0.5978 - precision_2k: 0.4625 - precision_3k: 0.3741 - precision_5k: 0.2711 - recall_1k: 0.3753 - recall_2k: 0.5466 - recall_3k: 0.6432 - recall_5k: 0.7505 - F1_1k: 0.4610 - F1_2k: 0.5010 - F1_3k: 0.4730 - F1_5k: 0.3983 - accuracy_1k: 0.5978 - accuracy_2k: 0.7443 - accuracy_3k: 0.8157 - accuracy_5k: 0.8830 - hamming_loss_k: 0.0040 - val_loss: 0.0134 - val_precision_1k: 0.5562 - val_precision_2k: 0.4304 - val_precision_3k: 0.3473 - val_precision_5k: 0.2506 - val_recall_1k: 0.3431 - val_recall_2k: 0.4995 - val_recall_3k: 0.5888 - val_recall_5k: 0.6899 - val_F1_1k: 0.4242 - val_F1_2k: 0.4623 - val_F1_3k: 0.4367 - val_F1_5k: 0.3675 - val_accuracy_1k: 0.5562 - val_accuracy_2k: 0.6971 - val_accuracy_3k: 0.7639 - val_accuracy_5k: 0.8372 - val_hamming_loss_k: 0.0042
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.0116 - precision_1k: 0.6099 - precision_2k: 0.4758 - precision_3k: 0.3828 - precision_5k: 0.2745 - recall_1k: 0.3815 - recall_2k: 0.5607 - recall_3k: 0.6566 - recall_5k: 0.7586 - F1_1k: 0.4693 - F1_2k: 0.5147 - F1_3k: 0.4836 - F1_5k: 0.4031 - accuracy_1k: 0.6099 - accuracy_2k: 0.7591 - accuracy_3k: 0.8284 - accuracy_5k: 0.8900 - hamming_loss_k: 0.0040
Epoch 00015: val_loss improved from 0.01344 to 0.01343, saving model to logs/ltqdko-lab-0604-133135/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0116 - precision_1k: 0.6099 - precision_2k: 0.4758 - precision_3k: 0.3828 - precision_5k: 0.2745 - recall_1k: 0.3815 - recall_2k: 0.5607 - recall_3k: 0.6566 - recall_5k: 0.7586 - F1_1k: 0.4693 - F1_2k: 0.5147 - F1_3k: 0.4836 - F1_5k: 0.4031 - accuracy_1k: 0.6099 - accuracy_2k: 0.7591 - accuracy_3k: 0.8284 - accuracy_5k: 0.8900 - hamming_loss_k: 0.0040 - val_loss: 0.0134 - val_precision_1k: 0.5524 - val_precision_2k: 0.4286 - val_precision_3k: 0.3443 - val_precision_5k: 0.2534 - val_recall_1k: 0.3429 - val_recall_2k: 0.5046 - val_recall_3k: 0.5888 - val_recall_5k: 0.6991 - val_F1_1k: 0.4231 - val_F1_2k: 0.4634 - val_F1_3k: 0.4344 - val_F1_5k: 0.3718 - val_accuracy_1k: 0.5524 - val_accuracy_2k: 0.7000 - val_accuracy_3k: 0.7677 - val_accuracy_5k: 0.8422 - val_hamming_loss_k: 0.0042
Epoch 00015: early stopping
176/176 [==============================] - 7s 40ms/step - loss: 0.0121 - precision_1k: 0.5886 - precision_2k: 0.4586 - precision_3k: 0.3667 - precision_5k: 0.2649 - recall_1k: 0.3685 - recall_2k: 0.5447 - recall_3k: 0.6343 - recall_5k: 0.7398 - F1_1k: 0.4519 - F1_2k: 0.4968 - F1_3k: 0.4638 - F1_5k: 0.3894 - accuracy_1k: 0.5886 - accuracy_2k: 0.7420 - accuracy_3k: 0.8095 - accuracy_5k: 0.8744 - hamming_loss_k: 0.0040
Best model result:  [0.012114888057112694, 0.5885579586029053, 0.45859548449516296, 0.36674928665161133, 0.26487278938293457, 0.36849719285964966, 0.5446760654449463, 0.6342592835426331, 0.7397794723510742, 0.4519336521625519, 0.4967605769634247, 0.4638136923313141, 0.38942983746528625, 0.5885579586029053, 0.7419675588607788, 0.809536874294281, 0.8744494915008545, 0.004028967581689358]
13498
3375
5625
Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_6 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_10 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_6[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_10[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_11 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_4 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_11[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_4[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_6 (Dense)                (None, 1024)         2098176     ['tf.concat_2[0][0]']            
                                                                                                  
 dense_7 (Dense)                (None, 15)           15375       ['dense_6[0][0]']                
                                                                                                  
 tf.nn.softmax_2 (TFOpLambda)   (None, 15)           0           ['dense_7[0][0]']                
                                                                                                  
 tf.expand_dims_4 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_2[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_4[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_7 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_12 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_7[0][0]']              
                                                                                                  
 dense_8 (Dense)                (None, 15, 150)      22650       ['lambda_12[0][0]']              
                                                                                                  
 tf.math.reduce_mean_5 (TFOpLam  (None, 150)         0           ['dense_8[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_5 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_5[0][0]']  
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_2 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_5[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 permute_8 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_2[0][0]']     
                                                                                                  
 lambda_13 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_8[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_13[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_14 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_2[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_14[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.0904 - precision_1k: 0.0599 - precision_2k: 0.0483 - precision_3k: 0.0423 - precision_5k: 0.0342 - recall_1k: 0.0301 - recall_2k: 0.0485 - recall_3k: 0.0648 - recall_5k: 0.0876 - F1_1k: 0.0399 - F1_2k: 0.0482 - F1_3k: 0.0510 - F1_5k: 0.0491 - accuracy_1k: 0.0599 - accuracy_2k: 0.0942 - accuracy_3k: 0.1202 - accuracy_5k: 0.1574 - hamming_loss_k: 0.0065
Epoch 00001: val_loss improved from inf to 0.02748, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 12s 387ms/step - loss: 0.0904 - precision_1k: 0.0599 - precision_2k: 0.0483 - precision_3k: 0.0423 - precision_5k: 0.0342 - recall_1k: 0.0301 - recall_2k: 0.0485 - recall_3k: 0.0648 - recall_5k: 0.0876 - F1_1k: 0.0399 - F1_2k: 0.0482 - F1_3k: 0.0510 - F1_5k: 0.0491 - accuracy_1k: 0.0599 - accuracy_2k: 0.0942 - accuracy_3k: 0.1202 - accuracy_5k: 0.1574 - hamming_loss_k: 0.0065 - val_loss: 0.0275 - val_precision_1k: 0.1166 - val_precision_2k: 0.0877 - val_precision_3k: 0.0752 - val_precision_5k: 0.0572 - val_recall_1k: 0.0526 - val_recall_2k: 0.0844 - val_recall_3k: 0.1151 - val_recall_5k: 0.1434 - val_F1_1k: 0.0725 - val_F1_2k: 0.0860 - val_F1_3k: 0.0908 - val_F1_5k: 0.0817 - val_accuracy_1k: 0.1166 - val_accuracy_2k: 0.1726 - val_accuracy_3k: 0.2157 - val_accuracy_5k: 0.2503 - val_hamming_loss_k: 0.0063
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.0251 - precision_1k: 0.1451 - precision_2k: 0.1154 - precision_3k: 0.1025 - precision_5k: 0.0839 - recall_1k: 0.0693 - recall_2k: 0.1116 - recall_3k: 0.1514 - recall_5k: 0.2081 - F1_1k: 0.0937 - F1_2k: 0.1134 - F1_3k: 0.1221 - F1_5k: 0.1195 - accuracy_1k: 0.1451 - accuracy_2k: 0.2129 - accuracy_3k: 0.2684 - accuracy_5k: 0.3351 - hamming_loss_k: 0.0061
Epoch 00002: val_loss improved from 0.02748 to 0.02407, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 393ms/step - loss: 0.0251 - precision_1k: 0.1451 - precision_2k: 0.1154 - precision_3k: 0.1025 - precision_5k: 0.0839 - recall_1k: 0.0693 - recall_2k: 0.1116 - recall_3k: 0.1514 - recall_5k: 0.2081 - F1_1k: 0.0937 - F1_2k: 0.1134 - F1_3k: 0.1221 - F1_5k: 0.1195 - accuracy_1k: 0.1451 - accuracy_2k: 0.2129 - accuracy_3k: 0.2684 - accuracy_5k: 0.3351 - hamming_loss_k: 0.0061 - val_loss: 0.0241 - val_precision_1k: 0.1522 - val_precision_2k: 0.1199 - val_precision_3k: 0.1199 - val_precision_5k: 0.1026 - val_recall_1k: 0.0702 - val_recall_2k: 0.1160 - val_recall_3k: 0.1778 - val_recall_5k: 0.2588 - val_F1_1k: 0.0961 - val_F1_2k: 0.1179 - val_F1_3k: 0.1431 - val_F1_5k: 0.1469 - val_accuracy_1k: 0.1522 - val_accuracy_2k: 0.2241 - val_accuracy_3k: 0.3088 - val_accuracy_5k: 0.4062 - val_hamming_loss_k: 0.0061
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.0226 - precision_1k: 0.2126 - precision_2k: 0.1788 - precision_3k: 0.1511 - precision_5k: 0.1189 - recall_1k: 0.1092 - recall_2k: 0.1802 - recall_3k: 0.2279 - recall_5k: 0.2984 - F1_1k: 0.1442 - F1_2k: 0.1794 - F1_3k: 0.1817 - F1_5k: 0.1700 - accuracy_1k: 0.2126 - accuracy_2k: 0.3066 - accuracy_3k: 0.3633 - accuracy_5k: 0.4379 - hamming_loss_k: 0.0058
Epoch 00003: val_loss improved from 0.02407 to 0.02143, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 396ms/step - loss: 0.0226 - precision_1k: 0.2126 - precision_2k: 0.1788 - precision_3k: 0.1511 - precision_5k: 0.1189 - recall_1k: 0.1092 - recall_2k: 0.1802 - recall_3k: 0.2279 - recall_5k: 0.2984 - F1_1k: 0.1442 - F1_2k: 0.1794 - F1_3k: 0.1817 - F1_5k: 0.1700 - accuracy_1k: 0.2126 - accuracy_2k: 0.3066 - accuracy_3k: 0.3633 - accuracy_5k: 0.4379 - hamming_loss_k: 0.0058 - val_loss: 0.0214 - val_precision_1k: 0.2892 - val_precision_2k: 0.2285 - val_precision_3k: 0.1935 - val_precision_5k: 0.1511 - val_recall_1k: 0.1602 - val_recall_2k: 0.2442 - val_recall_3k: 0.3034 - val_recall_5k: 0.3909 - val_F1_1k: 0.2060 - val_F1_2k: 0.2359 - val_F1_3k: 0.2361 - val_F1_5k: 0.2178 - val_accuracy_1k: 0.2892 - val_accuracy_2k: 0.3997 - val_accuracy_3k: 0.4601 - val_accuracy_5k: 0.5386 - val_hamming_loss_k: 0.0055
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.0196 - precision_1k: 0.3182 - precision_2k: 0.2587 - precision_3k: 0.2151 - precision_5k: 0.1685 - recall_1k: 0.1804 - recall_2k: 0.2853 - recall_3k: 0.3489 - recall_5k: 0.4468 - F1_1k: 0.2301 - F1_2k: 0.2713 - F1_3k: 0.2661 - F1_5k: 0.2447 - accuracy_1k: 0.3182 - accuracy_2k: 0.4418 - accuracy_3k: 0.5081 - accuracy_5k: 0.5952 - hamming_loss_k: 0.0053
Epoch 00004: val_loss improved from 0.02143 to 0.01860, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0196 - precision_1k: 0.3182 - precision_2k: 0.2587 - precision_3k: 0.2151 - precision_5k: 0.1685 - recall_1k: 0.1804 - recall_2k: 0.2853 - recall_3k: 0.3489 - recall_5k: 0.4468 - F1_1k: 0.2301 - F1_2k: 0.2713 - F1_3k: 0.2661 - F1_5k: 0.2447 - accuracy_1k: 0.3182 - accuracy_2k: 0.4418 - accuracy_3k: 0.5081 - accuracy_5k: 0.5952 - hamming_loss_k: 0.0053 - val_loss: 0.0186 - val_precision_1k: 0.3658 - val_precision_2k: 0.2877 - val_precision_3k: 0.2400 - val_precision_5k: 0.1863 - val_recall_1k: 0.2090 - val_recall_2k: 0.3208 - val_recall_3k: 0.3933 - val_recall_5k: 0.4968 - val_F1_1k: 0.2658 - val_F1_2k: 0.3031 - val_F1_3k: 0.2978 - val_F1_5k: 0.2707 - val_accuracy_1k: 0.3658 - val_accuracy_2k: 0.4945 - val_accuracy_3k: 0.5731 - val_accuracy_5k: 0.6604 - val_hamming_loss_k: 0.0051
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.0172 - precision_1k: 0.3903 - precision_2k: 0.3068 - precision_3k: 0.2555 - precision_5k: 0.1950 - recall_1k: 0.2300 - recall_2k: 0.3469 - recall_3k: 0.4254 - recall_5k: 0.5287 - F1_1k: 0.2893 - F1_2k: 0.3255 - F1_3k: 0.3192 - F1_5k: 0.2849 - accuracy_1k: 0.3903 - accuracy_2k: 0.5166 - accuracy_3k: 0.5937 - accuracy_5k: 0.6829 - hamming_loss_k: 0.0050
Epoch 00005: val_loss improved from 0.01860 to 0.01684, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0172 - precision_1k: 0.3903 - precision_2k: 0.3068 - precision_3k: 0.2555 - precision_5k: 0.1950 - recall_1k: 0.2300 - recall_2k: 0.3469 - recall_3k: 0.4254 - recall_5k: 0.5287 - F1_1k: 0.2893 - F1_2k: 0.3255 - F1_3k: 0.3192 - F1_5k: 0.2849 - accuracy_1k: 0.3903 - accuracy_2k: 0.5166 - accuracy_3k: 0.5937 - accuracy_5k: 0.6829 - hamming_loss_k: 0.0050 - val_loss: 0.0168 - val_precision_1k: 0.4181 - val_precision_2k: 0.3288 - val_precision_3k: 0.2693 - val_precision_5k: 0.2047 - val_recall_1k: 0.2457 - val_recall_2k: 0.3676 - val_recall_3k: 0.4438 - val_recall_5k: 0.5494 - val_F1_1k: 0.3093 - val_F1_2k: 0.3469 - val_F1_3k: 0.3350 - val_F1_5k: 0.2981 - val_accuracy_1k: 0.4181 - val_accuracy_2k: 0.5482 - val_accuracy_3k: 0.6248 - val_accuracy_5k: 0.7156 - val_hamming_loss_k: 0.0049
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.0158 - precision_1k: 0.4330 - precision_2k: 0.3439 - precision_3k: 0.2820 - precision_5k: 0.2122 - recall_1k: 0.2602 - recall_2k: 0.3938 - recall_3k: 0.4741 - recall_5k: 0.5800 - F1_1k: 0.3250 - F1_2k: 0.3671 - F1_3k: 0.3536 - F1_5k: 0.3107 - accuracy_1k: 0.4330 - accuracy_2k: 0.5706 - accuracy_3k: 0.6457 - accuracy_5k: 0.7337 - hamming_loss_k: 0.0048
Epoch 00006: val_loss improved from 0.01684 to 0.01581, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0158 - precision_1k: 0.4330 - precision_2k: 0.3439 - precision_3k: 0.2820 - precision_5k: 0.2122 - recall_1k: 0.2602 - recall_2k: 0.3938 - recall_3k: 0.4741 - recall_5k: 0.5800 - F1_1k: 0.3250 - F1_2k: 0.3671 - F1_3k: 0.3536 - F1_5k: 0.3107 - accuracy_1k: 0.4330 - accuracy_2k: 0.5706 - accuracy_3k: 0.6457 - accuracy_5k: 0.7337 - hamming_loss_k: 0.0048 - val_loss: 0.0158 - val_precision_1k: 0.4471 - val_precision_2k: 0.3514 - val_precision_3k: 0.2886 - val_precision_5k: 0.2163 - val_recall_1k: 0.2674 - val_recall_2k: 0.4022 - val_recall_3k: 0.4845 - val_recall_5k: 0.5856 - val_F1_1k: 0.3344 - val_F1_2k: 0.3748 - val_F1_3k: 0.3614 - val_F1_5k: 0.3158 - val_accuracy_1k: 0.4471 - val_accuracy_2k: 0.5849 - val_accuracy_3k: 0.6612 - val_accuracy_5k: 0.7408 - val_hamming_loss_k: 0.0047
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.0149 - precision_1k: 0.4701 - precision_2k: 0.3734 - precision_3k: 0.3056 - precision_5k: 0.2260 - recall_1k: 0.2872 - recall_2k: 0.4346 - recall_3k: 0.5194 - recall_5k: 0.6228 - F1_1k: 0.3564 - F1_2k: 0.4016 - F1_3k: 0.3847 - F1_5k: 0.3316 - accuracy_1k: 0.4701 - accuracy_2k: 0.6188 - accuracy_3k: 0.6951 - accuracy_5k: 0.7751 - hamming_loss_k: 0.0046
Epoch 00007: val_loss improved from 0.01581 to 0.01528, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 400ms/step - loss: 0.0149 - precision_1k: 0.4701 - precision_2k: 0.3734 - precision_3k: 0.3056 - precision_5k: 0.2260 - recall_1k: 0.2872 - recall_2k: 0.4346 - recall_3k: 0.5194 - recall_5k: 0.6228 - F1_1k: 0.3564 - F1_2k: 0.4016 - F1_3k: 0.3847 - F1_5k: 0.3316 - accuracy_1k: 0.4701 - accuracy_2k: 0.6188 - accuracy_3k: 0.6951 - accuracy_5k: 0.7751 - hamming_loss_k: 0.0046 - val_loss: 0.0153 - val_precision_1k: 0.4615 - val_precision_2k: 0.3638 - val_precision_3k: 0.3031 - val_precision_5k: 0.2251 - val_recall_1k: 0.2702 - val_recall_2k: 0.4125 - val_recall_3k: 0.5057 - val_recall_5k: 0.6135 - val_F1_1k: 0.3407 - val_F1_2k: 0.3864 - val_F1_3k: 0.3788 - val_F1_5k: 0.3292 - val_accuracy_1k: 0.4615 - val_accuracy_2k: 0.6028 - val_accuracy_3k: 0.6825 - val_accuracy_5k: 0.7693 - val_hamming_loss_k: 0.0047
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.0142 - precision_1k: 0.4943 - precision_2k: 0.3918 - precision_3k: 0.3202 - precision_5k: 0.2359 - recall_1k: 0.3024 - recall_2k: 0.4554 - recall_3k: 0.5439 - recall_5k: 0.6502 - F1_1k: 0.3752 - F1_2k: 0.4211 - F1_3k: 0.4031 - F1_5k: 0.3462 - accuracy_1k: 0.4943 - accuracy_2k: 0.6442 - accuracy_3k: 0.7194 - accuracy_5k: 0.7973 - hamming_loss_k: 0.0045
Epoch 00008: val_loss improved from 0.01528 to 0.01481, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0142 - precision_1k: 0.4943 - precision_2k: 0.3918 - precision_3k: 0.3202 - precision_5k: 0.2359 - recall_1k: 0.3024 - recall_2k: 0.4554 - recall_3k: 0.5439 - recall_5k: 0.6502 - F1_1k: 0.3752 - F1_2k: 0.4211 - F1_3k: 0.4031 - F1_5k: 0.3462 - accuracy_1k: 0.4943 - accuracy_2k: 0.6442 - accuracy_3k: 0.7194 - accuracy_5k: 0.7973 - hamming_loss_k: 0.0045 - val_loss: 0.0148 - val_precision_1k: 0.4769 - val_precision_2k: 0.3786 - val_precision_3k: 0.3146 - val_precision_5k: 0.2338 - val_recall_1k: 0.2856 - val_recall_2k: 0.4335 - val_recall_3k: 0.5292 - val_recall_5k: 0.6349 - val_F1_1k: 0.3571 - val_F1_2k: 0.4040 - val_F1_3k: 0.3944 - val_F1_5k: 0.3416 - val_accuracy_1k: 0.4769 - val_accuracy_2k: 0.6225 - val_accuracy_3k: 0.7028 - val_accuracy_5k: 0.7840 - val_hamming_loss_k: 0.0046
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.0137 - precision_1k: 0.5148 - precision_2k: 0.4070 - precision_3k: 0.3326 - precision_5k: 0.2436 - recall_1k: 0.3163 - recall_2k: 0.4735 - recall_3k: 0.5648 - recall_5k: 0.6715 - F1_1k: 0.3918 - F1_2k: 0.4377 - F1_3k: 0.4186 - F1_5k: 0.3575 - accuracy_1k: 0.5148 - accuracy_2k: 0.6641 - accuracy_3k: 0.7399 - accuracy_5k: 0.8162 - hamming_loss_k: 0.0044
Epoch 00009: val_loss improved from 0.01481 to 0.01446, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0137 - precision_1k: 0.5148 - precision_2k: 0.4070 - precision_3k: 0.3326 - precision_5k: 0.2436 - recall_1k: 0.3163 - recall_2k: 0.4735 - recall_3k: 0.5648 - recall_5k: 0.6715 - F1_1k: 0.3918 - F1_2k: 0.4377 - F1_3k: 0.4186 - F1_5k: 0.3575 - accuracy_1k: 0.5148 - accuracy_2k: 0.6641 - accuracy_3k: 0.7399 - accuracy_5k: 0.8162 - hamming_loss_k: 0.0044 - val_loss: 0.0145 - val_precision_1k: 0.4993 - val_precision_2k: 0.3925 - val_precision_3k: 0.3243 - val_precision_5k: 0.2372 - val_recall_1k: 0.3014 - val_recall_2k: 0.4540 - val_recall_3k: 0.5476 - val_recall_5k: 0.6498 - val_F1_1k: 0.3757 - val_F1_2k: 0.4208 - val_F1_3k: 0.4070 - val_F1_5k: 0.3473 - val_accuracy_1k: 0.4993 - val_accuracy_2k: 0.6446 - val_accuracy_3k: 0.7184 - val_accuracy_5k: 0.7987 - val_hamming_loss_k: 0.0045
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.0132 - precision_1k: 0.5379 - precision_2k: 0.4218 - precision_3k: 0.3413 - precision_5k: 0.2496 - recall_1k: 0.3329 - recall_2k: 0.4930 - recall_3k: 0.5822 - recall_5k: 0.6896 - F1_1k: 0.4112 - F1_2k: 0.4546 - F1_3k: 0.4302 - F1_5k: 0.3665 - accuracy_1k: 0.5379 - accuracy_2k: 0.6874 - accuracy_3k: 0.7576 - accuracy_5k: 0.8330 - hamming_loss_k: 0.0043
Epoch 00010: val_loss improved from 0.01446 to 0.01422, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0132 - precision_1k: 0.5379 - precision_2k: 0.4218 - precision_3k: 0.3413 - precision_5k: 0.2496 - recall_1k: 0.3329 - recall_2k: 0.4930 - recall_3k: 0.5822 - recall_5k: 0.6896 - F1_1k: 0.4112 - F1_2k: 0.4546 - F1_3k: 0.4302 - F1_5k: 0.3665 - accuracy_1k: 0.5379 - accuracy_2k: 0.6874 - accuracy_3k: 0.7576 - accuracy_5k: 0.8330 - hamming_loss_k: 0.0043 - val_loss: 0.0142 - val_precision_1k: 0.4996 - val_precision_2k: 0.4016 - val_precision_3k: 0.3307 - val_precision_5k: 0.2408 - val_recall_1k: 0.2996 - val_recall_2k: 0.4610 - val_recall_3k: 0.5572 - val_recall_5k: 0.6586 - val_F1_1k: 0.3743 - val_F1_2k: 0.4290 - val_F1_3k: 0.4148 - val_F1_5k: 0.3524 - val_accuracy_1k: 0.4996 - val_accuracy_2k: 0.6522 - val_accuracy_3k: 0.7342 - val_accuracy_5k: 0.8106 - val_hamming_loss_k: 0.0045
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.0128 - precision_1k: 0.5492 - precision_2k: 0.4330 - precision_3k: 0.3515 - precision_5k: 0.2551 - recall_1k: 0.3405 - recall_2k: 0.5060 - recall_3k: 0.5998 - recall_5k: 0.7047 - F1_1k: 0.4203 - F1_2k: 0.4666 - F1_3k: 0.4432 - F1_5k: 0.3746 - accuracy_1k: 0.5492 - accuracy_2k: 0.6999 - accuracy_3k: 0.7717 - accuracy_5k: 0.8458 - hamming_loss_k: 0.0042
Epoch 00011: val_loss improved from 0.01422 to 0.01398, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0128 - precision_1k: 0.5492 - precision_2k: 0.4330 - precision_3k: 0.3515 - precision_5k: 0.2551 - recall_1k: 0.3405 - recall_2k: 0.5060 - recall_3k: 0.5998 - recall_5k: 0.7047 - F1_1k: 0.4203 - F1_2k: 0.4666 - F1_3k: 0.4432 - F1_5k: 0.3746 - accuracy_1k: 0.5492 - accuracy_2k: 0.6999 - accuracy_3k: 0.7717 - accuracy_5k: 0.8458 - hamming_loss_k: 0.0042 - val_loss: 0.0140 - val_precision_1k: 0.5181 - val_precision_2k: 0.4081 - val_precision_3k: 0.3349 - val_precision_5k: 0.2454 - val_recall_1k: 0.3123 - val_recall_2k: 0.4689 - val_recall_3k: 0.5630 - val_recall_5k: 0.6702 - val_F1_1k: 0.3895 - val_F1_2k: 0.4362 - val_F1_3k: 0.4198 - val_F1_5k: 0.3590 - val_accuracy_1k: 0.5181 - val_accuracy_2k: 0.6592 - val_accuracy_3k: 0.7371 - val_accuracy_5k: 0.8160 - val_hamming_loss_k: 0.0044
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.0125 - precision_1k: 0.5678 - precision_2k: 0.4442 - precision_3k: 0.3602 - precision_5k: 0.2605 - recall_1k: 0.3516 - recall_2k: 0.5191 - recall_3k: 0.6144 - recall_5k: 0.7204 - F1_1k: 0.4342 - F1_2k: 0.4787 - F1_3k: 0.4541 - F1_5k: 0.3825 - accuracy_1k: 0.5678 - accuracy_2k: 0.7165 - accuracy_3k: 0.7874 - accuracy_5k: 0.8582 - hamming_loss_k: 0.0041
Epoch 00012: val_loss improved from 0.01398 to 0.01380, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 405ms/step - loss: 0.0125 - precision_1k: 0.5678 - precision_2k: 0.4442 - precision_3k: 0.3602 - precision_5k: 0.2605 - recall_1k: 0.3516 - recall_2k: 0.5191 - recall_3k: 0.6144 - recall_5k: 0.7204 - F1_1k: 0.4342 - F1_2k: 0.4787 - F1_3k: 0.4541 - F1_5k: 0.3825 - accuracy_1k: 0.5678 - accuracy_2k: 0.7165 - accuracy_3k: 0.7874 - accuracy_5k: 0.8582 - hamming_loss_k: 0.0041 - val_loss: 0.0138 - val_precision_1k: 0.5267 - val_precision_2k: 0.4132 - val_precision_3k: 0.3347 - val_precision_5k: 0.2464 - val_recall_1k: 0.3217 - val_recall_2k: 0.4782 - val_recall_3k: 0.5685 - val_recall_5k: 0.6789 - val_F1_1k: 0.3992 - val_F1_2k: 0.4431 - val_F1_3k: 0.4211 - val_F1_5k: 0.3613 - val_accuracy_1k: 0.5267 - val_accuracy_2k: 0.6679 - val_accuracy_3k: 0.7437 - val_accuracy_5k: 0.8281 - val_hamming_loss_k: 0.0044
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.0122 - precision_1k: 0.5862 - precision_2k: 0.4552 - precision_3k: 0.3670 - precision_5k: 0.2651 - recall_1k: 0.3672 - recall_2k: 0.5360 - recall_3k: 0.6291 - recall_5k: 0.7335 - F1_1k: 0.4515 - F1_2k: 0.4922 - F1_3k: 0.4635 - F1_5k: 0.3895 - accuracy_1k: 0.5862 - accuracy_2k: 0.7322 - accuracy_3k: 0.8023 - accuracy_5k: 0.8696 - hamming_loss_k: 0.0041
Epoch 00013: val_loss improved from 0.01380 to 0.01378, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0122 - precision_1k: 0.5862 - precision_2k: 0.4552 - precision_3k: 0.3670 - precision_5k: 0.2651 - recall_1k: 0.3672 - recall_2k: 0.5360 - recall_3k: 0.6291 - recall_5k: 0.7335 - F1_1k: 0.4515 - F1_2k: 0.4922 - F1_3k: 0.4635 - F1_5k: 0.3895 - accuracy_1k: 0.5862 - accuracy_2k: 0.7322 - accuracy_3k: 0.8023 - accuracy_5k: 0.8696 - hamming_loss_k: 0.0041 - val_loss: 0.0138 - val_precision_1k: 0.5284 - val_precision_2k: 0.4142 - val_precision_3k: 0.3408 - val_precision_5k: 0.2500 - val_recall_1k: 0.3210 - val_recall_2k: 0.4802 - val_recall_3k: 0.5787 - val_recall_5k: 0.6869 - val_F1_1k: 0.3992 - val_F1_2k: 0.4445 - val_F1_3k: 0.4287 - val_F1_5k: 0.3664 - val_accuracy_1k: 0.5284 - val_accuracy_2k: 0.6756 - val_accuracy_3k: 0.7542 - val_accuracy_5k: 0.8332 - val_hamming_loss_k: 0.0044
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.0120 - precision_1k: 0.5888 - precision_2k: 0.4617 - precision_3k: 0.3745 - precision_5k: 0.2694 - recall_1k: 0.3682 - recall_2k: 0.5421 - recall_3k: 0.6402 - recall_5k: 0.7460 - F1_1k: 0.4530 - F1_2k: 0.4986 - F1_3k: 0.4725 - F1_5k: 0.3958 - accuracy_1k: 0.5888 - accuracy_2k: 0.7406 - accuracy_3k: 0.8127 - accuracy_5k: 0.8787 - hamming_loss_k: 0.0040
Epoch 00014: val_loss improved from 0.01378 to 0.01358, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0120 - precision_1k: 0.5888 - precision_2k: 0.4617 - precision_3k: 0.3745 - precision_5k: 0.2694 - recall_1k: 0.3682 - recall_2k: 0.5421 - recall_3k: 0.6402 - recall_5k: 0.7460 - F1_1k: 0.4530 - F1_2k: 0.4986 - F1_3k: 0.4725 - F1_5k: 0.3958 - accuracy_1k: 0.5888 - accuracy_2k: 0.7406 - accuracy_3k: 0.8127 - accuracy_5k: 0.8787 - hamming_loss_k: 0.0040 - val_loss: 0.0136 - val_precision_1k: 0.5540 - val_precision_2k: 0.4256 - val_precision_3k: 0.3446 - val_precision_5k: 0.2511 - val_recall_1k: 0.3412 - val_recall_2k: 0.4960 - val_recall_3k: 0.5870 - val_recall_5k: 0.6936 - val_F1_1k: 0.4221 - val_F1_2k: 0.4579 - val_F1_3k: 0.4341 - val_F1_5k: 0.3686 - val_accuracy_1k: 0.5540 - val_accuracy_2k: 0.6958 - val_accuracy_3k: 0.7696 - val_accuracy_5k: 0.8443 - val_hamming_loss_k: 0.0042
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.0117 - precision_1k: 0.6068 - precision_2k: 0.4726 - precision_3k: 0.3814 - precision_5k: 0.2727 - recall_1k: 0.3806 - recall_2k: 0.5570 - recall_3k: 0.6536 - recall_5k: 0.7554 - F1_1k: 0.4677 - F1_2k: 0.5112 - F1_3k: 0.4816 - F1_5k: 0.4007 - accuracy_1k: 0.6068 - accuracy_2k: 0.7548 - accuracy_3k: 0.8219 - accuracy_5k: 0.8849 - hamming_loss_k: 0.0040
Epoch 00015: val_loss improved from 0.01358 to 0.01342, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0117 - precision_1k: 0.6068 - precision_2k: 0.4726 - precision_3k: 0.3814 - precision_5k: 0.2727 - recall_1k: 0.3806 - recall_2k: 0.5570 - recall_3k: 0.6536 - recall_5k: 0.7554 - F1_1k: 0.4677 - F1_2k: 0.5112 - F1_3k: 0.4816 - F1_5k: 0.4007 - accuracy_1k: 0.6068 - accuracy_2k: 0.7548 - accuracy_3k: 0.8219 - accuracy_5k: 0.8849 - hamming_loss_k: 0.0040 - val_loss: 0.0134 - val_precision_1k: 0.5368 - val_precision_2k: 0.4267 - val_precision_3k: 0.3494 - val_precision_5k: 0.2538 - val_recall_1k: 0.3306 - val_recall_2k: 0.4994 - val_recall_3k: 0.5949 - val_recall_5k: 0.6987 - val_F1_1k: 0.4089 - val_F1_2k: 0.4599 - val_F1_3k: 0.4400 - val_F1_5k: 0.3722 - val_accuracy_1k: 0.5368 - val_accuracy_2k: 0.6970 - val_accuracy_3k: 0.7741 - val_accuracy_5k: 0.8489 - val_hamming_loss_k: 0.0043
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.0113 - precision_1k: 0.6236 - precision_2k: 0.4856 - precision_3k: 0.3904 - precision_5k: 0.2778 - recall_1k: 0.3928 - recall_2k: 0.5739 - recall_3k: 0.6700 - recall_5k: 0.7715 - F1_1k: 0.4819 - F1_2k: 0.5259 - F1_3k: 0.4932 - F1_5k: 0.4084 - accuracy_1k: 0.6236 - accuracy_2k: 0.7731 - accuracy_3k: 0.8356 - accuracy_5k: 0.8987 - hamming_loss_k: 0.0039
Epoch 00016: val_loss improved from 0.01342 to 0.01335, saving model to logs/ynukky-lab-0604-133426/model/checkpoint_lab.h5
27/27 [==============================] - 11s 399ms/step - loss: 0.0113 - precision_1k: 0.6236 - precision_2k: 0.4856 - precision_3k: 0.3904 - precision_5k: 0.2778 - recall_1k: 0.3928 - recall_2k: 0.5739 - recall_3k: 0.6700 - recall_5k: 0.7715 - F1_1k: 0.4819 - F1_2k: 0.5259 - F1_3k: 0.4932 - F1_5k: 0.4084 - accuracy_1k: 0.6236 - accuracy_2k: 0.7731 - accuracy_3k: 0.8356 - accuracy_5k: 0.8987 - hamming_loss_k: 0.0039 - val_loss: 0.0133 - val_precision_1k: 0.5556 - val_precision_2k: 0.4333 - val_precision_3k: 0.3537 - val_precision_5k: 0.2562 - val_recall_1k: 0.3434 - val_recall_2k: 0.5037 - val_recall_3k: 0.5988 - val_recall_5k: 0.7026 - val_F1_1k: 0.4243 - val_F1_2k: 0.4656 - val_F1_3k: 0.4445 - val_F1_5k: 0.3753 - val_accuracy_1k: 0.5556 - val_accuracy_2k: 0.7006 - val_accuracy_3k: 0.7745 - val_accuracy_5k: 0.8472 - val_hamming_loss_k: 0.0042
Epoch 17/150
27/27 [==============================] - ETA: 0s - loss: 0.0110 - precision_1k: 0.6425 - precision_2k: 0.4957 - precision_3k: 0.3981 - precision_5k: 0.2826 - recall_1k: 0.4041 - recall_2k: 0.5848 - recall_3k: 0.6822 - recall_5k: 0.7809 - F1_1k: 0.4960 - F1_2k: 0.5365 - F1_3k: 0.5027 - F1_5k: 0.4150 - accuracy_1k: 0.6425 - accuracy_2k: 0.7860 - accuracy_3k: 0.8494 - accuracy_5k: 0.9041 - hamming_loss_k: 0.0038
Epoch 00017: val_loss did not improve from 0.01335
27/27 [==============================] - 10s 364ms/step - loss: 0.0110 - precision_1k: 0.6425 - precision_2k: 0.4957 - precision_3k: 0.3981 - precision_5k: 0.2826 - recall_1k: 0.4041 - recall_2k: 0.5848 - recall_3k: 0.6822 - recall_5k: 0.7809 - F1_1k: 0.4960 - F1_2k: 0.5365 - F1_3k: 0.5027 - F1_5k: 0.4150 - accuracy_1k: 0.6425 - accuracy_2k: 0.7860 - accuracy_3k: 0.8494 - accuracy_5k: 0.9041 - hamming_loss_k: 0.0038 - val_loss: 0.0134 - val_precision_1k: 0.5435 - val_precision_2k: 0.4276 - val_precision_3k: 0.3509 - val_precision_5k: 0.2561 - val_recall_1k: 0.3374 - val_recall_2k: 0.4976 - val_recall_3k: 0.5965 - val_recall_5k: 0.7043 - val_F1_1k: 0.4161 - val_F1_2k: 0.4597 - val_F1_3k: 0.4417 - val_F1_5k: 0.3754 - val_accuracy_1k: 0.5435 - val_accuracy_2k: 0.6932 - val_accuracy_3k: 0.7713 - val_accuracy_5k: 0.8525 - val_hamming_loss_k: 0.0043
Epoch 00017: early stopping
176/176 [==============================] - 8s 40ms/step - loss: 0.0116 - precision_1k: 0.6237 - precision_2k: 0.4762 - precision_3k: 0.3830 - precision_5k: 0.2735 - recall_1k: 0.3939 - recall_2k: 0.5628 - recall_3k: 0.6578 - recall_5k: 0.7596 - F1_1k: 0.4816 - F1_2k: 0.5147 - F1_3k: 0.4831 - F1_5k: 0.4016 - accuracy_1k: 0.6237 - accuracy_2k: 0.7567 - accuracy_3k: 0.8212 - accuracy_5k: 0.8866 - hamming_loss_k: 0.0039
Best model result:  [0.011579174548387527, 0.6236657500267029, 0.4761587083339691, 0.3830391764640808, 0.27353474497795105, 0.393881231546402, 0.5627664923667908, 0.657813549041748, 0.7596233487129211, 0.481608122587204, 0.5146691799163818, 0.4831426739692688, 0.4016045331954956, 0.6236657500267029, 0.756704568862915, 0.8212015628814697, 0.8866450786590576, 0.00386453396640718]
13499
3374
5625
Model: "model_3"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 permute_9 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_15 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_9[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_15[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_16 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_6 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_16[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_3 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_6[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_9 (Dense)                (None, 1024)         2098176     ['tf.concat_3[0][0]']            
                                                                                                  
 dense_10 (Dense)               (None, 15)           15375       ['dense_9[0][0]']                
                                                                                                  
 tf.nn.softmax_3 (TFOpLambda)   (None, 15)           0           ['dense_10[0][0]']               
                                                                                                  
 tf.expand_dims_6 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_3[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_6[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_10 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_17 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_10[0][0]']             
                                                                                                  
 dense_11 (Dense)               (None, 15, 150)      22650       ['lambda_17[0][0]']              
                                                                                                  
 tf.math.reduce_mean_7 (TFOpLam  (None, 150)         0           ['dense_11[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_7 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_7[0][0]']  
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_3 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_7[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_10[0][
                                                                 0]']                             
                                                                                                  
 permute_11 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_3[0][0]']     
                                                                                                  
 lambda_18 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_11[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_18[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_19 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_3[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_19[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Epoch 1/150
/home/dzq/k12/atmk_system-master/MathByte/models/evaluation_metrics.py:260: RuntimeWarning: invalid value encountered in true_divide
  return (2 * p_k * r_k) / (p_k + r_k)
27/27 [==============================] - ETA: 0s - loss: 0.0847 - precision_1k: 0.0594 - precision_2k: 0.0448 - precision_3k: 0.0397 - precision_5k: 0.0345 - recall_1k: 0.0272 - recall_2k: 0.0421 - recall_3k: 0.0559 - recall_5k: 0.0826 - F1_1k: nan - F1_2k: nan - F1_3k: 0.0464 - F1_5k: 0.0486 - accuracy_1k: 0.0594 - accuracy_2k: 0.0852 - accuracy_3k: 0.1103 - accuracy_5k: 0.1532 - hamming_loss_k: 0.0065
Epoch 00001: val_loss improved from inf to 0.02619, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 12s 398ms/step - loss: 0.0847 - precision_1k: 0.0594 - precision_2k: 0.0448 - precision_3k: 0.0397 - precision_5k: 0.0345 - recall_1k: 0.0272 - recall_2k: 0.0421 - recall_3k: 0.0559 - recall_5k: 0.0826 - F1_1k: nan - F1_2k: nan - F1_3k: 0.0464 - F1_5k: 0.0486 - accuracy_1k: 0.0594 - accuracy_2k: 0.0852 - accuracy_3k: 0.1103 - accuracy_5k: 0.1532 - hamming_loss_k: 0.0065 - val_loss: 0.0262 - val_precision_1k: 0.1227 - val_precision_2k: 0.0838 - val_precision_3k: 0.0723 - val_precision_5k: 0.0615 - val_recall_1k: 0.0562 - val_recall_2k: 0.0767 - val_recall_3k: 0.1036 - val_recall_5k: 0.1501 - val_F1_1k: 0.0769 - val_F1_2k: 0.0799 - val_F1_3k: 0.0850 - val_F1_5k: 0.0872 - val_accuracy_1k: 0.1227 - val_accuracy_2k: 0.1476 - val_accuracy_3k: 0.1965 - val_accuracy_5k: 0.2685 - val_hamming_loss_k: 0.0062
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.0245 - precision_1k: 0.1639 - precision_2k: 0.1372 - precision_3k: 0.1137 - precision_5k: 0.0929 - recall_1k: 0.0796 - recall_2k: 0.1352 - recall_3k: 0.1686 - recall_5k: 0.2310 - F1_1k: 0.1070 - F1_2k: 0.1361 - F1_3k: 0.1358 - F1_5k: 0.1324 - accuracy_1k: 0.1639 - accuracy_2k: 0.2345 - accuracy_3k: 0.2780 - accuracy_5k: 0.3592 - hamming_loss_k: 0.0060
Epoch 00002: val_loss improved from 0.02619 to 0.02332, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0245 - precision_1k: 0.1639 - precision_2k: 0.1372 - precision_3k: 0.1137 - precision_5k: 0.0929 - recall_1k: 0.0796 - recall_2k: 0.1352 - recall_3k: 0.1686 - recall_5k: 0.2310 - F1_1k: 0.1070 - F1_2k: 0.1361 - F1_3k: 0.1358 - F1_5k: 0.1324 - accuracy_1k: 0.1639 - accuracy_2k: 0.2345 - accuracy_3k: 0.2780 - accuracy_5k: 0.3592 - hamming_loss_k: 0.0060 - val_loss: 0.0233 - val_precision_1k: 0.1818 - val_precision_2k: 0.1591 - val_precision_3k: 0.1359 - val_precision_5k: 0.1085 - val_recall_1k: 0.0913 - val_recall_2k: 0.1573 - val_recall_3k: 0.1988 - val_recall_5k: 0.2668 - val_F1_1k: 0.1214 - val_F1_2k: 0.1580 - val_F1_3k: 0.1613 - val_F1_5k: 0.1542 - val_accuracy_1k: 0.1818 - val_accuracy_2k: 0.2654 - val_accuracy_3k: 0.3226 - val_accuracy_5k: 0.4002 - val_hamming_loss_k: 0.0060
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.0217 - precision_1k: 0.2570 - precision_2k: 0.2105 - precision_3k: 0.1756 - precision_5k: 0.1387 - recall_1k: 0.1389 - recall_2k: 0.2218 - recall_3k: 0.2756 - recall_5k: 0.3597 - F1_1k: 0.1802 - F1_2k: 0.2158 - F1_3k: 0.2144 - F1_5k: 0.2001 - accuracy_1k: 0.2570 - accuracy_2k: 0.3551 - accuracy_3k: 0.4160 - accuracy_5k: 0.4996 - hamming_loss_k: 0.0056
Epoch 00003: val_loss improved from 0.02332 to 0.02039, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0217 - precision_1k: 0.2570 - precision_2k: 0.2105 - precision_3k: 0.1756 - precision_5k: 0.1387 - recall_1k: 0.1389 - recall_2k: 0.2218 - recall_3k: 0.2756 - recall_5k: 0.3597 - F1_1k: 0.1802 - F1_2k: 0.2158 - F1_3k: 0.2144 - F1_5k: 0.2001 - accuracy_1k: 0.2570 - accuracy_2k: 0.3551 - accuracy_3k: 0.4160 - accuracy_5k: 0.4996 - hamming_loss_k: 0.0056 - val_loss: 0.0204 - val_precision_1k: 0.2924 - val_precision_2k: 0.2319 - val_precision_3k: 0.1978 - val_precision_5k: 0.1578 - val_recall_1k: 0.1662 - val_recall_2k: 0.2536 - val_recall_3k: 0.3188 - val_recall_5k: 0.4134 - val_F1_1k: 0.2118 - val_F1_2k: 0.2421 - val_F1_3k: 0.2439 - val_F1_5k: 0.2283 - val_accuracy_1k: 0.2924 - val_accuracy_2k: 0.3960 - val_accuracy_3k: 0.4719 - val_accuracy_5k: 0.5600 - val_hamming_loss_k: 0.0054
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.0188 - precision_1k: 0.3415 - precision_2k: 0.2695 - precision_3k: 0.2262 - precision_5k: 0.1770 - recall_1k: 0.1978 - recall_2k: 0.3007 - recall_3k: 0.3703 - recall_5k: 0.4711 - F1_1k: 0.2504 - F1_2k: 0.2841 - F1_3k: 0.2807 - F1_5k: 0.2573 - accuracy_1k: 0.3415 - accuracy_2k: 0.4622 - accuracy_3k: 0.5328 - accuracy_5k: 0.6217 - hamming_loss_k: 0.0052
Epoch 00004: val_loss improved from 0.02039 to 0.01801, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 396ms/step - loss: 0.0188 - precision_1k: 0.3415 - precision_2k: 0.2695 - precision_3k: 0.2262 - precision_5k: 0.1770 - recall_1k: 0.1978 - recall_2k: 0.3007 - recall_3k: 0.3703 - recall_5k: 0.4711 - F1_1k: 0.2504 - F1_2k: 0.2841 - F1_3k: 0.2807 - F1_5k: 0.2573 - accuracy_1k: 0.3415 - accuracy_2k: 0.4622 - accuracy_3k: 0.5328 - accuracy_5k: 0.6217 - hamming_loss_k: 0.0052 - val_loss: 0.0180 - val_precision_1k: 0.3685 - val_precision_2k: 0.2878 - val_precision_3k: 0.2436 - val_precision_5k: 0.1838 - val_recall_1k: 0.2177 - val_recall_2k: 0.3297 - val_recall_3k: 0.4079 - val_recall_5k: 0.4968 - val_F1_1k: 0.2735 - val_F1_2k: 0.3071 - val_F1_3k: 0.3048 - val_F1_5k: 0.2682 - val_accuracy_1k: 0.3685 - val_accuracy_2k: 0.4899 - val_accuracy_3k: 0.5689 - val_accuracy_5k: 0.6502 - val_hamming_loss_k: 0.0051
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.0169 - precision_1k: 0.4019 - precision_2k: 0.3196 - precision_3k: 0.2649 - precision_5k: 0.2007 - recall_1k: 0.2379 - recall_2k: 0.3614 - recall_3k: 0.4407 - recall_5k: 0.5433 - F1_1k: 0.2988 - F1_2k: 0.3391 - F1_3k: 0.3308 - F1_5k: 0.2930 - accuracy_1k: 0.4019 - accuracy_2k: 0.5364 - accuracy_3k: 0.6109 - accuracy_5k: 0.6957 - hamming_loss_k: 0.0049
Epoch 00005: val_loss improved from 0.01801 to 0.01662, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0169 - precision_1k: 0.4019 - precision_2k: 0.3196 - precision_3k: 0.2649 - precision_5k: 0.2007 - recall_1k: 0.2379 - recall_2k: 0.3614 - recall_3k: 0.4407 - recall_5k: 0.5433 - F1_1k: 0.2988 - F1_2k: 0.3391 - F1_3k: 0.3308 - F1_5k: 0.2930 - accuracy_1k: 0.4019 - accuracy_2k: 0.5364 - accuracy_3k: 0.6109 - accuracy_5k: 0.6957 - hamming_loss_k: 0.0049 - val_loss: 0.0166 - val_precision_1k: 0.4069 - val_precision_2k: 0.3235 - val_precision_3k: 0.2695 - val_precision_5k: 0.2032 - val_recall_1k: 0.2427 - val_recall_2k: 0.3746 - val_recall_3k: 0.4553 - val_recall_5k: 0.5522 - val_F1_1k: 0.3039 - val_F1_2k: 0.3470 - val_F1_3k: 0.3384 - val_F1_5k: 0.2969 - val_accuracy_1k: 0.4069 - val_accuracy_2k: 0.5514 - val_accuracy_3k: 0.6257 - val_accuracy_5k: 0.7098 - val_hamming_loss_k: 0.0049
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.0157 - precision_1k: 0.4433 - precision_2k: 0.3487 - precision_3k: 0.2884 - precision_5k: 0.2148 - recall_1k: 0.2665 - recall_2k: 0.3998 - recall_3k: 0.4863 - recall_5k: 0.5887 - F1_1k: 0.3329 - F1_2k: 0.3725 - F1_3k: 0.3620 - F1_5k: 0.3147 - accuracy_1k: 0.4433 - accuracy_2k: 0.5802 - accuracy_3k: 0.6609 - accuracy_5k: 0.7414 - hamming_loss_k: 0.0047
Epoch 00006: val_loss improved from 0.01662 to 0.01576, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0157 - precision_1k: 0.4433 - precision_2k: 0.3487 - precision_3k: 0.2884 - precision_5k: 0.2148 - recall_1k: 0.2665 - recall_2k: 0.3998 - recall_3k: 0.4863 - recall_5k: 0.5887 - F1_1k: 0.3329 - F1_2k: 0.3725 - F1_3k: 0.3620 - F1_5k: 0.3147 - accuracy_1k: 0.4433 - accuracy_2k: 0.5802 - accuracy_3k: 0.6609 - accuracy_5k: 0.7414 - hamming_loss_k: 0.0047 - val_loss: 0.0158 - val_precision_1k: 0.4418 - val_precision_2k: 0.3360 - val_precision_3k: 0.2811 - val_precision_5k: 0.2129 - val_recall_1k: 0.2700 - val_recall_2k: 0.3860 - val_recall_3k: 0.4736 - val_recall_5k: 0.5825 - val_F1_1k: 0.3349 - val_F1_2k: 0.3591 - val_F1_3k: 0.3526 - val_F1_5k: 0.3116 - val_accuracy_1k: 0.4418 - val_accuracy_2k: 0.5635 - val_accuracy_3k: 0.6447 - val_accuracy_5k: 0.7411 - val_hamming_loss_k: 0.0047
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.0149 - precision_1k: 0.4720 - precision_2k: 0.3696 - precision_3k: 0.3033 - precision_5k: 0.2258 - recall_1k: 0.2863 - recall_2k: 0.4257 - recall_3k: 0.5132 - recall_5k: 0.6212 - F1_1k: 0.3563 - F1_2k: 0.3956 - F1_3k: 0.3813 - F1_5k: 0.3311 - accuracy_1k: 0.4720 - accuracy_2k: 0.6117 - accuracy_3k: 0.6882 - accuracy_5k: 0.7730 - hamming_loss_k: 0.0046 ETA: 3s - loss: 0.0150 - precision_1k: 0.4617 - precision_2k: 0.3643 - precision_3k: 0.3022 - precision_5k: 0.2250 - recall_1k: 0.2795 - recall_2k: 0.4191 - recall_3k: 0.5115 - recall_5k: 0.6180 - F1_1k: 0.3481 - F1_2k: 0.3898 - F1_3k: 0.3799 - F1_5k: 0.3298 - accuracy_1k: 0.4617 - accuracy_2k: 0.6049 - accuracy_3k: 0.6863 - accuracy_5k: 0.7709 - 
Epoch 00007: val_loss improved from 0.01576 to 0.01528, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0149 - precision_1k: 0.4720 - precision_2k: 0.3696 - precision_3k: 0.3033 - precision_5k: 0.2258 - recall_1k: 0.2863 - recall_2k: 0.4257 - recall_3k: 0.5132 - recall_5k: 0.6212 - F1_1k: 0.3563 - F1_2k: 0.3956 - F1_3k: 0.3813 - F1_5k: 0.3311 - accuracy_1k: 0.4720 - accuracy_2k: 0.6117 - accuracy_3k: 0.6882 - accuracy_5k: 0.7730 - hamming_loss_k: 0.0046 - val_loss: 0.0153 - val_precision_1k: 0.4722 - val_precision_2k: 0.3616 - val_precision_3k: 0.2959 - val_precision_5k: 0.2203 - val_recall_1k: 0.2884 - val_recall_2k: 0.4191 - val_recall_3k: 0.5020 - val_recall_5k: 0.6070 - val_F1_1k: 0.3579 - val_F1_2k: 0.3880 - val_F1_3k: 0.3721 - val_F1_5k: 0.3231 - val_accuracy_1k: 0.4722 - val_accuracy_2k: 0.6062 - val_accuracy_3k: 0.6798 - val_accuracy_5k: 0.7648 - val_hamming_loss_k: 0.0046
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.0143 - precision_1k: 0.5032 - precision_2k: 0.3932 - precision_3k: 0.3206 - precision_5k: 0.2351 - recall_1k: 0.3076 - recall_2k: 0.4546 - recall_3k: 0.5436 - recall_5k: 0.6476 - F1_1k: 0.3817 - F1_2k: 0.4216 - F1_3k: 0.4033 - F1_5k: 0.3449 - accuracy_1k: 0.5032 - accuracy_2k: 0.6432 - accuracy_3k: 0.7170 - accuracy_5k: 0.7974 - hamming_loss_k: 0.0045
Epoch 00008: val_loss improved from 0.01528 to 0.01491, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0143 - precision_1k: 0.5032 - precision_2k: 0.3932 - precision_3k: 0.3206 - precision_5k: 0.2351 - recall_1k: 0.3076 - recall_2k: 0.4546 - recall_3k: 0.5436 - recall_5k: 0.6476 - F1_1k: 0.3817 - F1_2k: 0.4216 - F1_3k: 0.4033 - F1_5k: 0.3449 - accuracy_1k: 0.5032 - accuracy_2k: 0.6432 - accuracy_3k: 0.7170 - accuracy_5k: 0.7974 - hamming_loss_k: 0.0045 - val_loss: 0.0149 - val_precision_1k: 0.4791 - val_precision_2k: 0.3746 - val_precision_3k: 0.3056 - val_precision_5k: 0.2249 - val_recall_1k: 0.2976 - val_recall_2k: 0.4358 - val_recall_3k: 0.5174 - val_recall_5k: 0.6149 - val_F1_1k: 0.3670 - val_F1_2k: 0.4027 - val_F1_3k: 0.3840 - val_F1_5k: 0.3292 - val_accuracy_1k: 0.4791 - val_accuracy_2k: 0.6175 - val_accuracy_3k: 0.6902 - val_accuracy_5k: 0.7642 - val_hamming_loss_k: 0.0046
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.0137 - precision_1k: 0.5216 - precision_2k: 0.4099 - precision_3k: 0.3337 - precision_5k: 0.2438 - recall_1k: 0.3208 - recall_2k: 0.4776 - recall_3k: 0.5670 - recall_5k: 0.6716 - F1_1k: 0.3972 - F1_2k: 0.4411 - F1_3k: 0.4201 - F1_5k: 0.3577 - accuracy_1k: 0.5216 - accuracy_2k: 0.6733 - accuracy_3k: 0.7433 - accuracy_5k: 0.8187 - hamming_loss_k: 0.0044
Epoch 00009: val_loss improved from 0.01491 to 0.01439, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0137 - precision_1k: 0.5216 - precision_2k: 0.4099 - precision_3k: 0.3337 - precision_5k: 0.2438 - recall_1k: 0.3208 - recall_2k: 0.4776 - recall_3k: 0.5670 - recall_5k: 0.6716 - F1_1k: 0.3972 - F1_2k: 0.4411 - F1_3k: 0.4201 - F1_5k: 0.3577 - accuracy_1k: 0.5216 - accuracy_2k: 0.6733 - accuracy_3k: 0.7433 - accuracy_5k: 0.8187 - hamming_loss_k: 0.0044 - val_loss: 0.0144 - val_precision_1k: 0.4947 - val_precision_2k: 0.3851 - val_precision_3k: 0.3149 - val_precision_5k: 0.2320 - val_recall_1k: 0.3047 - val_recall_2k: 0.4461 - val_recall_3k: 0.5353 - val_recall_5k: 0.6401 - val_F1_1k: 0.3768 - val_F1_2k: 0.4132 - val_F1_3k: 0.3963 - val_F1_5k: 0.3404 - val_accuracy_1k: 0.4947 - val_accuracy_2k: 0.6398 - val_accuracy_3k: 0.7154 - val_accuracy_5k: 0.7982 - val_hamming_loss_k: 0.0045
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.0132 - precision_1k: 0.5463 - precision_2k: 0.4247 - precision_3k: 0.3465 - precision_5k: 0.2521 - recall_1k: 0.3373 - recall_2k: 0.4962 - recall_3k: 0.5911 - recall_5k: 0.6963 - F1_1k: 0.4170 - F1_2k: 0.4576 - F1_3k: 0.4369 - F1_5k: 0.3701 - accuracy_1k: 0.5463 - accuracy_2k: 0.6912 - accuracy_3k: 0.7644 - accuracy_5k: 0.8392 - hamming_loss_k: 0.0043
Epoch 00010: val_loss improved from 0.01439 to 0.01413, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 400ms/step - loss: 0.0132 - precision_1k: 0.5463 - precision_2k: 0.4247 - precision_3k: 0.3465 - precision_5k: 0.2521 - recall_1k: 0.3373 - recall_2k: 0.4962 - recall_3k: 0.5911 - recall_5k: 0.6963 - F1_1k: 0.4170 - F1_2k: 0.4576 - F1_3k: 0.4369 - F1_5k: 0.3701 - accuracy_1k: 0.5463 - accuracy_2k: 0.6912 - accuracy_3k: 0.7644 - accuracy_5k: 0.8392 - hamming_loss_k: 0.0043 - val_loss: 0.0141 - val_precision_1k: 0.5088 - val_precision_2k: 0.3978 - val_precision_3k: 0.3208 - val_precision_5k: 0.2357 - val_recall_1k: 0.3182 - val_recall_2k: 0.4671 - val_recall_3k: 0.5486 - val_recall_5k: 0.6528 - val_F1_1k: 0.3913 - val_F1_2k: 0.4294 - val_F1_3k: 0.4047 - val_F1_5k: 0.3462 - val_accuracy_1k: 0.5088 - val_accuracy_2k: 0.6568 - val_accuracy_3k: 0.7263 - val_accuracy_5k: 0.8040 - val_hamming_loss_k: 0.0044
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.0130 - precision_1k: 0.5496 - precision_2k: 0.4272 - precision_3k: 0.3483 - precision_5k: 0.2532 - recall_1k: 0.3410 - recall_2k: 0.4997 - recall_3k: 0.5951 - recall_5k: 0.6994 - F1_1k: 0.4208 - F1_2k: 0.4605 - F1_3k: 0.4393 - F1_5k: 0.3717 - accuracy_1k: 0.5496 - accuracy_2k: 0.6945 - accuracy_3k: 0.7695 - accuracy_5k: 0.8445 - hamming_loss_k: 0.0042
Epoch 00011: val_loss improved from 0.01413 to 0.01397, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0130 - precision_1k: 0.5496 - precision_2k: 0.4272 - precision_3k: 0.3483 - precision_5k: 0.2532 - recall_1k: 0.3410 - recall_2k: 0.4997 - recall_3k: 0.5951 - recall_5k: 0.6994 - F1_1k: 0.4208 - F1_2k: 0.4605 - F1_3k: 0.4393 - F1_5k: 0.3717 - accuracy_1k: 0.5496 - accuracy_2k: 0.6945 - accuracy_3k: 0.7695 - accuracy_5k: 0.8445 - hamming_loss_k: 0.0042 - val_loss: 0.0140 - val_precision_1k: 0.5182 - val_precision_2k: 0.4053 - val_precision_3k: 0.3297 - val_precision_5k: 0.2406 - val_recall_1k: 0.3206 - val_recall_2k: 0.4720 - val_recall_3k: 0.5603 - val_recall_5k: 0.6648 - val_F1_1k: 0.3958 - val_F1_2k: 0.4359 - val_F1_3k: 0.4150 - val_F1_5k: 0.3532 - val_accuracy_1k: 0.5182 - val_accuracy_2k: 0.6629 - val_accuracy_3k: 0.7378 - val_accuracy_5k: 0.8117 - val_hamming_loss_k: 0.0044
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.0126 - precision_1k: 0.5653 - precision_2k: 0.4441 - precision_3k: 0.3603 - precision_5k: 0.2601 - recall_1k: 0.3502 - recall_2k: 0.5183 - recall_3k: 0.6141 - recall_5k: 0.7178 - F1_1k: 0.4324 - F1_2k: 0.4783 - F1_3k: 0.4540 - F1_5k: 0.3818 - accuracy_1k: 0.5653 - accuracy_2k: 0.7165 - accuracy_3k: 0.7881 - accuracy_5k: 0.8569 - hamming_loss_k: 0.0042
Epoch 00012: val_loss improved from 0.01397 to 0.01373, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 400ms/step - loss: 0.0126 - precision_1k: 0.5653 - precision_2k: 0.4441 - precision_3k: 0.3603 - precision_5k: 0.2601 - recall_1k: 0.3502 - recall_2k: 0.5183 - recall_3k: 0.6141 - recall_5k: 0.7178 - F1_1k: 0.4324 - F1_2k: 0.4783 - F1_3k: 0.4540 - F1_5k: 0.3818 - accuracy_1k: 0.5653 - accuracy_2k: 0.7165 - accuracy_3k: 0.7881 - accuracy_5k: 0.8569 - hamming_loss_k: 0.0042 - val_loss: 0.0137 - val_precision_1k: 0.5281 - val_precision_2k: 0.4068 - val_precision_3k: 0.3349 - val_precision_5k: 0.2440 - val_recall_1k: 0.3282 - val_recall_2k: 0.4745 - val_recall_3k: 0.5716 - val_recall_5k: 0.6748 - val_F1_1k: 0.4046 - val_F1_2k: 0.4379 - val_F1_3k: 0.4221 - val_F1_5k: 0.3583 - val_accuracy_1k: 0.5281 - val_accuracy_2k: 0.6654 - val_accuracy_3k: 0.7472 - val_accuracy_5k: 0.8217 - val_hamming_loss_k: 0.0043
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.0123 - precision_1k: 0.5836 - precision_2k: 0.4538 - precision_3k: 0.3660 - precision_5k: 0.2648 - recall_1k: 0.3641 - recall_2k: 0.5322 - recall_3k: 0.6260 - recall_5k: 0.7305 - F1_1k: 0.4484 - F1_2k: 0.4898 - F1_3k: 0.4619 - F1_5k: 0.3886 - accuracy_1k: 0.5836 - accuracy_2k: 0.7292 - accuracy_3k: 0.7982 - accuracy_5k: 0.8668 - hamming_loss_k: 0.0041
Epoch 00013: val_loss improved from 0.01373 to 0.01366, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0123 - precision_1k: 0.5836 - precision_2k: 0.4538 - precision_3k: 0.3660 - precision_5k: 0.2648 - recall_1k: 0.3641 - recall_2k: 0.5322 - recall_3k: 0.6260 - recall_5k: 0.7305 - F1_1k: 0.4484 - F1_2k: 0.4898 - F1_3k: 0.4619 - F1_5k: 0.3886 - accuracy_1k: 0.5836 - accuracy_2k: 0.7292 - accuracy_3k: 0.7982 - accuracy_5k: 0.8668 - hamming_loss_k: 0.0041 - val_loss: 0.0137 - val_precision_1k: 0.5263 - val_precision_2k: 0.4129 - val_precision_3k: 0.3376 - val_precision_5k: 0.2486 - val_recall_1k: 0.3291 - val_recall_2k: 0.4834 - val_recall_3k: 0.5755 - val_recall_5k: 0.6853 - val_F1_1k: 0.4047 - val_F1_2k: 0.4451 - val_F1_3k: 0.4253 - val_F1_5k: 0.3646 - val_accuracy_1k: 0.5263 - val_accuracy_2k: 0.6775 - val_accuracy_3k: 0.7547 - val_accuracy_5k: 0.8320 - val_hamming_loss_k: 0.0043
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.0120 - precision_1k: 0.6013 - precision_2k: 0.4659 - precision_3k: 0.3757 - precision_5k: 0.2696 - recall_1k: 0.3746 - recall_2k: 0.5472 - recall_3k: 0.6431 - recall_5k: 0.7437 - F1_1k: 0.4616 - F1_2k: 0.5032 - F1_3k: 0.4743 - F1_5k: 0.3957 - accuracy_1k: 0.6013 - accuracy_2k: 0.7460 - accuracy_3k: 0.8154 - accuracy_5k: 0.8780 - hamming_loss_k: 0.0040
Epoch 00014: val_loss improved from 0.01366 to 0.01333, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0120 - precision_1k: 0.6013 - precision_2k: 0.4659 - precision_3k: 0.3757 - precision_5k: 0.2696 - recall_1k: 0.3746 - recall_2k: 0.5472 - recall_3k: 0.6431 - recall_5k: 0.7437 - F1_1k: 0.4616 - F1_2k: 0.5032 - F1_3k: 0.4743 - F1_5k: 0.3957 - accuracy_1k: 0.6013 - accuracy_2k: 0.7460 - accuracy_3k: 0.8154 - accuracy_5k: 0.8780 - hamming_loss_k: 0.0040 - val_loss: 0.0133 - val_precision_1k: 0.5626 - val_precision_2k: 0.4313 - val_precision_3k: 0.3432 - val_precision_5k: 0.2498 - val_recall_1k: 0.3512 - val_recall_2k: 0.5061 - val_recall_3k: 0.5876 - val_recall_5k: 0.6941 - val_F1_1k: 0.4323 - val_F1_2k: 0.4655 - val_F1_3k: 0.4331 - val_F1_5k: 0.3672 - val_accuracy_1k: 0.5626 - val_accuracy_2k: 0.7058 - val_accuracy_3k: 0.7679 - val_accuracy_5k: 0.8378 - val_hamming_loss_k: 0.0042
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.0116 - precision_1k: 0.6144 - precision_2k: 0.4782 - precision_3k: 0.3830 - precision_5k: 0.2745 - recall_1k: 0.3864 - recall_2k: 0.5639 - recall_3k: 0.6565 - recall_5k: 0.7598 - F1_1k: 0.4743 - F1_2k: 0.5175 - F1_3k: 0.4838 - F1_5k: 0.4033 - accuracy_1k: 0.6144 - accuracy_2k: 0.7634 - accuracy_3k: 0.8257 - accuracy_5k: 0.8884 - hamming_loss_k: 0.0039
Epoch 00015: val_loss did not improve from 0.01333
27/27 [==============================] - 10s 364ms/step - loss: 0.0116 - precision_1k: 0.6144 - precision_2k: 0.4782 - precision_3k: 0.3830 - precision_5k: 0.2745 - recall_1k: 0.3864 - recall_2k: 0.5639 - recall_3k: 0.6565 - recall_5k: 0.7598 - F1_1k: 0.4743 - F1_2k: 0.5175 - F1_3k: 0.4838 - F1_5k: 0.4033 - accuracy_1k: 0.6144 - accuracy_2k: 0.7634 - accuracy_3k: 0.8257 - accuracy_5k: 0.8884 - hamming_loss_k: 0.0039 - val_loss: 0.0134 - val_precision_1k: 0.5384 - val_precision_2k: 0.4239 - val_precision_3k: 0.3438 - val_precision_5k: 0.2499 - val_recall_1k: 0.3374 - val_recall_2k: 0.4977 - val_recall_3k: 0.5872 - val_recall_5k: 0.6893 - val_F1_1k: 0.4146 - val_F1_2k: 0.4576 - val_F1_3k: 0.4335 - val_F1_5k: 0.3667 - val_accuracy_1k: 0.5384 - val_accuracy_2k: 0.6969 - val_accuracy_3k: 0.7678 - val_accuracy_5k: 0.8350 - val_hamming_loss_k: 0.0043
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.0114 - precision_1k: 0.6258 - precision_2k: 0.4855 - precision_3k: 0.3907 - precision_5k: 0.2790 - recall_1k: 0.3919 - recall_2k: 0.5693 - recall_3k: 0.6657 - recall_5k: 0.7688 - F1_1k: 0.4819 - F1_2k: 0.5240 - F1_3k: 0.4924 - F1_5k: 0.4094 - accuracy_1k: 0.6258 - accuracy_2k: 0.7693 - accuracy_3k: 0.8361 - accuracy_5k: 0.8988 - hamming_loss_k: 0.0039
Epoch 00016: val_loss improved from 0.01333 to 0.01327, saving model to logs/iwprfa-lab-0604-133738/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0114 - precision_1k: 0.6258 - precision_2k: 0.4855 - precision_3k: 0.3907 - precision_5k: 0.2790 - recall_1k: 0.3919 - recall_2k: 0.5693 - recall_3k: 0.6657 - recall_5k: 0.7688 - F1_1k: 0.4819 - F1_2k: 0.5240 - F1_3k: 0.4924 - F1_5k: 0.4094 - accuracy_1k: 0.6258 - accuracy_2k: 0.7693 - accuracy_3k: 0.8361 - accuracy_5k: 0.8988 - hamming_loss_k: 0.0039 - val_loss: 0.0133 - val_precision_1k: 0.5543 - val_precision_2k: 0.4297 - val_precision_3k: 0.3508 - val_precision_5k: 0.2523 - val_recall_1k: 0.3476 - val_recall_2k: 0.5044 - val_recall_3k: 0.6008 - val_recall_5k: 0.6975 - val_F1_1k: 0.4270 - val_F1_2k: 0.4638 - val_F1_3k: 0.4427 - val_F1_5k: 0.3704 - val_accuracy_1k: 0.5543 - val_accuracy_2k: 0.7000 - val_accuracy_3k: 0.7783 - val_accuracy_5k: 0.8425 - val_hamming_loss_k: 0.0042
Epoch 00016: early stopping
176/176 [==============================] - 8s 41ms/step - loss: 0.0115 - precision_1k: 0.6167 - precision_2k: 0.4795 - precision_3k: 0.3855 - precision_5k: 0.2764 - recall_1k: 0.3880 - recall_2k: 0.5656 - recall_3k: 0.6635 - recall_5k: 0.7660 - F1_1k: 0.4751 - F1_2k: 0.5178 - F1_3k: 0.4867 - F1_5k: 0.4056 - accuracy_1k: 0.6167 - accuracy_2k: 0.7635 - accuracy_3k: 0.8358 - accuracy_5k: 0.8914 - hamming_loss_k: 0.0039 2s - loss: 0.0115 - precision_1k: 0.6196 - precision_2k: 0.4781 - precision_3k: 0.3826 - precision_5k: 0.2742 - recall_1k: 0.3880 - recall_2k: 0.5654 - recall_3k: 0.6609 - recall_5k: 0.7637 - F1_1k: 0.4757 - F1_2k: 0.5169 - F1_3k: 0.4836 - F1_5k: 0.4029 - accuracy_1k: 0.6196 - accuracy_2k: 0.7625 - accu
Best model result:  [0.011524638161063194, 0.6167407631874084, 0.47946593165397644, 0.3854653239250183, 0.27641651034355164, 0.3880425691604614, 0.5655773282051086, 0.663539707660675, 0.7660099267959595, 0.4750547707080841, 0.5178152918815613, 0.48668524622917175, 0.4055831730365753, 0.6167407631874084, 0.7635005712509155, 0.8358138799667358, 0.8914387226104736, 0.003896967973560095]
13499
3374
5625
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_12 (S  (None, 15, 300)     0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_12[0][
                                                                 0]']                             
                                                                                                  
 permute_12 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_20 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_12[0][0]']             
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_20[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_21 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_8 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_21[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_4 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_8[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_12 (Dense)               (None, 1024)         2098176     ['tf.concat_4[0][0]']            
                                                                                                  
 dense_13 (Dense)               (None, 15)           15375       ['dense_12[0][0]']               
                                                                                                  
 tf.nn.softmax_4 (TFOpLambda)   (None, 15)           0           ['dense_13[0][0]']               
                                                                                                  
 tf.expand_dims_8 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_4[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_8[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_13 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_22 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_13[0][0]']             
                                                                                                  
 dense_14 (Dense)               (None, 15, 150)      22650       ['lambda_22[0][0]']              
                                                                                                  
 tf.math.reduce_mean_9 (TFOpLam  (None, 150)         0           ['dense_14[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_9 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_9[0][0]']  
                                                                                                  
 tf.__operators__.getitem_13 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_4 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_9[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_13[0][
                                                                 0]']                             
                                                                                                  
 permute_14 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_4[0][0]']     
                                                                                                  
 lambda_23 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_14[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_23[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_24 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_4[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_24[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.0888 - precision_1k: 0.0782 - precision_2k: 0.0586 - precision_3k: 0.0478 - precision_5k: 0.0372 - recall_1k: 0.0377 - recall_2k: 0.0572 - recall_3k: 0.0706 - recall_5k: 0.0921 - F1_1k: 0.0508 - F1_2k: 0.0578 - F1_3k: 0.0569 - F1_5k: 0.0529 - accuracy_1k: 0.0782 - accuracy_2k: 0.1131 - accuracy_3k: 0.1352 - accuracy_5k: 0.1683 - hamming_loss_k: 0.0065
Epoch 00001: val_loss improved from inf to 0.02580, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 12s 386ms/step - loss: 0.0888 - precision_1k: 0.0782 - precision_2k: 0.0586 - precision_3k: 0.0478 - precision_5k: 0.0372 - recall_1k: 0.0377 - recall_2k: 0.0572 - recall_3k: 0.0706 - recall_5k: 0.0921 - F1_1k: 0.0508 - F1_2k: 0.0578 - F1_3k: 0.0569 - F1_5k: 0.0529 - accuracy_1k: 0.0782 - accuracy_2k: 0.1131 - accuracy_3k: 0.1352 - accuracy_5k: 0.1683 - hamming_loss_k: 0.0065 - val_loss: 0.0258 - val_precision_1k: 0.1232 - val_precision_2k: 0.1082 - val_precision_3k: 0.0911 - val_precision_5k: 0.0682 - val_recall_1k: 0.0613 - val_recall_2k: 0.1040 - val_recall_3k: 0.1301 - val_recall_5k: 0.1641 - val_F1_1k: 0.0817 - val_F1_2k: 0.1060 - val_F1_3k: 0.1071 - val_F1_5k: 0.0963 - val_accuracy_1k: 0.1232 - val_accuracy_2k: 0.1800 - val_accuracy_3k: 0.2242 - val_accuracy_5k: 0.2815 - val_hamming_loss_k: 0.0062
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.0244 - precision_1k: 0.1578 - precision_2k: 0.1355 - precision_3k: 0.1163 - precision_5k: 0.0918 - recall_1k: 0.0779 - recall_2k: 0.1356 - recall_3k: 0.1737 - recall_5k: 0.2303 - F1_1k: 0.1042 - F1_2k: 0.1355 - F1_3k: 0.1392 - F1_5k: 0.1312 - accuracy_1k: 0.1578 - accuracy_2k: 0.2399 - accuracy_3k: 0.2871 - accuracy_5k: 0.3587 - hamming_loss_k: 0.0061
Epoch 00002: val_loss improved from 0.02580 to 0.02279, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0244 - precision_1k: 0.1578 - precision_2k: 0.1355 - precision_3k: 0.1163 - precision_5k: 0.0918 - recall_1k: 0.0779 - recall_2k: 0.1356 - recall_3k: 0.1737 - recall_5k: 0.2303 - F1_1k: 0.1042 - F1_2k: 0.1355 - F1_3k: 0.1392 - F1_5k: 0.1312 - accuracy_1k: 0.1578 - accuracy_2k: 0.2399 - accuracy_3k: 0.2871 - accuracy_5k: 0.3587 - hamming_loss_k: 0.0061 - val_loss: 0.0228 - val_precision_1k: 0.1852 - val_precision_2k: 0.1626 - val_precision_3k: 0.1422 - val_precision_5k: 0.1183 - val_recall_1k: 0.0946 - val_recall_2k: 0.1638 - val_recall_3k: 0.2114 - val_recall_5k: 0.2971 - val_F1_1k: 0.1252 - val_F1_2k: 0.1631 - val_F1_3k: 0.1699 - val_F1_5k: 0.1692 - val_accuracy_1k: 0.1852 - val_accuracy_2k: 0.2885 - val_accuracy_3k: 0.3406 - val_accuracy_5k: 0.4309 - val_hamming_loss_k: 0.0059
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.0213 - precision_1k: 0.2653 - precision_2k: 0.2119 - precision_3k: 0.1785 - precision_5k: 0.1423 - recall_1k: 0.1459 - recall_2k: 0.2271 - recall_3k: 0.2824 - recall_5k: 0.3696 - F1_1k: 0.1881 - F1_2k: 0.2191 - F1_3k: 0.2186 - F1_5k: 0.2054 - accuracy_1k: 0.2653 - accuracy_2k: 0.3639 - accuracy_3k: 0.4263 - accuracy_5k: 0.5145 - hamming_loss_k: 0.0056 ETA: 6s - loss: 0.0223 - precision_1k: 0.2165 - precision_2k: 0.1749 - precision_3k: 0.1477 - precision_5k: 0.1229 - recall_1k: 0.1115 - recall_2k: 0.1789 - recall_3k: 0.2255 - recall_5k: 0.3137 - F1_1k: 0.1471 - F1_2k: 0.1768 - F1_3k: 0.1784 - F1_5k: 0.1766 - accuracy_1k: 0.2165 - accuracy_2k: 0.3016 - accuracy_3k: 0.3541 - accu
Epoch 00003: val_loss improved from 0.02279 to 0.01970, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0213 - precision_1k: 0.2653 - precision_2k: 0.2119 - precision_3k: 0.1785 - precision_5k: 0.1423 - recall_1k: 0.1459 - recall_2k: 0.2271 - recall_3k: 0.2824 - recall_5k: 0.3696 - F1_1k: 0.1881 - F1_2k: 0.2191 - F1_3k: 0.2186 - F1_5k: 0.2054 - accuracy_1k: 0.2653 - accuracy_2k: 0.3639 - accuracy_3k: 0.4263 - accuracy_5k: 0.5145 - hamming_loss_k: 0.0056 - val_loss: 0.0197 - val_precision_1k: 0.3195 - val_precision_2k: 0.2533 - val_precision_3k: 0.2101 - val_precision_5k: 0.1612 - val_recall_1k: 0.1817 - val_recall_2k: 0.2770 - val_recall_3k: 0.3396 - val_recall_5k: 0.4287 - val_F1_1k: 0.2316 - val_F1_2k: 0.2645 - val_F1_3k: 0.2595 - val_F1_5k: 0.2342 - val_accuracy_1k: 0.3195 - val_accuracy_2k: 0.4324 - val_accuracy_3k: 0.4971 - val_accuracy_5k: 0.5717 - val_hamming_loss_k: 0.0053
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.0184 - precision_1k: 0.3521 - precision_2k: 0.2781 - precision_3k: 0.2346 - precision_5k: 0.1830 - recall_1k: 0.2043 - recall_2k: 0.3116 - recall_3k: 0.3860 - recall_5k: 0.4883 - F1_1k: 0.2585 - F1_2k: 0.2939 - F1_3k: 0.2918 - F1_5k: 0.2662 - accuracy_1k: 0.3521 - accuracy_2k: 0.4747 - accuracy_3k: 0.5491 - accuracy_5k: 0.6423 - hamming_loss_k: 0.0052
Epoch 00004: val_loss improved from 0.01970 to 0.01742, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 400ms/step - loss: 0.0184 - precision_1k: 0.3521 - precision_2k: 0.2781 - precision_3k: 0.2346 - precision_5k: 0.1830 - recall_1k: 0.2043 - recall_2k: 0.3116 - recall_3k: 0.3860 - recall_5k: 0.4883 - F1_1k: 0.2585 - F1_2k: 0.2939 - F1_3k: 0.2918 - F1_5k: 0.2662 - accuracy_1k: 0.3521 - accuracy_2k: 0.4747 - accuracy_3k: 0.5491 - accuracy_5k: 0.6423 - hamming_loss_k: 0.0052 - val_loss: 0.0174 - val_precision_1k: 0.3739 - val_precision_2k: 0.2932 - val_precision_3k: 0.2488 - val_precision_5k: 0.1916 - val_recall_1k: 0.2251 - val_recall_2k: 0.3363 - val_recall_3k: 0.4165 - val_recall_5k: 0.5231 - val_F1_1k: 0.2809 - val_F1_2k: 0.3131 - val_F1_3k: 0.3114 - val_F1_5k: 0.2803 - val_accuracy_1k: 0.3739 - val_accuracy_2k: 0.5014 - val_accuracy_3k: 0.5815 - val_accuracy_5k: 0.6732 - val_hamming_loss_k: 0.0050
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.0166 - precision_1k: 0.4060 - precision_2k: 0.3242 - precision_3k: 0.2696 - precision_5k: 0.2047 - recall_1k: 0.2364 - recall_2k: 0.3640 - recall_3k: 0.4460 - recall_5k: 0.5532 - F1_1k: 0.2987 - F1_2k: 0.3429 - F1_3k: 0.3360 - F1_5k: 0.2988 - accuracy_1k: 0.4060 - accuracy_2k: 0.5412 - accuracy_3k: 0.6201 - accuracy_5k: 0.7074 - hamming_loss_k: 0.0049
Epoch 00005: val_loss improved from 0.01742 to 0.01612, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0166 - precision_1k: 0.4060 - precision_2k: 0.3242 - precision_3k: 0.2696 - precision_5k: 0.2047 - recall_1k: 0.2364 - recall_2k: 0.3640 - recall_3k: 0.4460 - recall_5k: 0.5532 - F1_1k: 0.2987 - F1_2k: 0.3429 - F1_3k: 0.3360 - F1_5k: 0.2988 - accuracy_1k: 0.4060 - accuracy_2k: 0.5412 - accuracy_3k: 0.6201 - accuracy_5k: 0.7074 - hamming_loss_k: 0.0049 - val_loss: 0.0161 - val_precision_1k: 0.4131 - val_precision_2k: 0.3300 - val_precision_3k: 0.2773 - val_precision_5k: 0.2080 - val_recall_1k: 0.2520 - val_recall_2k: 0.3837 - val_recall_3k: 0.4720 - val_recall_5k: 0.5757 - val_F1_1k: 0.3129 - val_F1_2k: 0.3546 - val_F1_3k: 0.3492 - val_F1_5k: 0.3055 - val_accuracy_1k: 0.4131 - val_accuracy_2k: 0.5607 - val_accuracy_3k: 0.6410 - val_accuracy_5k: 0.7251 - val_hamming_loss_k: 0.0048
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.0156 - precision_1k: 0.4420 - precision_2k: 0.3506 - precision_3k: 0.2898 - precision_5k: 0.2182 - recall_1k: 0.2638 - recall_2k: 0.3993 - recall_3k: 0.4839 - recall_5k: 0.5913 - F1_1k: 0.3303 - F1_2k: 0.3733 - F1_3k: 0.3625 - F1_5k: 0.3187 - accuracy_1k: 0.4420 - accuracy_2k: 0.5828 - accuracy_3k: 0.6595 - accuracy_5k: 0.7435 - hamming_loss_k: 0.0048
Epoch 00006: val_loss improved from 0.01612 to 0.01551, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0156 - precision_1k: 0.4420 - precision_2k: 0.3506 - precision_3k: 0.2898 - precision_5k: 0.2182 - recall_1k: 0.2638 - recall_2k: 0.3993 - recall_3k: 0.4839 - recall_5k: 0.5913 - F1_1k: 0.3303 - F1_2k: 0.3733 - F1_3k: 0.3625 - F1_5k: 0.3187 - accuracy_1k: 0.4420 - accuracy_2k: 0.5828 - accuracy_3k: 0.6595 - accuracy_5k: 0.7435 - hamming_loss_k: 0.0048 - val_loss: 0.0155 - val_precision_1k: 0.4378 - val_precision_2k: 0.3449 - val_precision_3k: 0.2874 - val_precision_5k: 0.2117 - val_recall_1k: 0.2644 - val_recall_2k: 0.3965 - val_recall_3k: 0.4860 - val_recall_5k: 0.5857 - val_F1_1k: 0.3295 - val_F1_2k: 0.3687 - val_F1_3k: 0.3610 - val_F1_5k: 0.3109 - val_accuracy_1k: 0.4378 - val_accuracy_2k: 0.5667 - val_accuracy_3k: 0.6482 - val_accuracy_5k: 0.7278 - val_hamming_loss_k: 0.0047
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.0148 - precision_1k: 0.4721 - precision_2k: 0.3731 - precision_3k: 0.3058 - precision_5k: 0.2270 - recall_1k: 0.2839 - recall_2k: 0.4277 - recall_3k: 0.5141 - recall_5k: 0.6196 - F1_1k: 0.3545 - F1_2k: 0.3984 - F1_3k: 0.3834 - F1_5k: 0.3323 - accuracy_1k: 0.4721 - accuracy_2k: 0.6125 - accuracy_3k: 0.6880 - accuracy_5k: 0.7715 - hamming_loss_k: 0.0046
Epoch 00007: val_loss improved from 0.01551 to 0.01485, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 398ms/step - loss: 0.0148 - precision_1k: 0.4721 - precision_2k: 0.3731 - precision_3k: 0.3058 - precision_5k: 0.2270 - recall_1k: 0.2839 - recall_2k: 0.4277 - recall_3k: 0.5141 - recall_5k: 0.6196 - F1_1k: 0.3545 - F1_2k: 0.3984 - F1_3k: 0.3834 - F1_5k: 0.3323 - accuracy_1k: 0.4721 - accuracy_2k: 0.6125 - accuracy_3k: 0.6880 - accuracy_5k: 0.7715 - hamming_loss_k: 0.0046 - val_loss: 0.0148 - val_precision_1k: 0.4736 - val_precision_2k: 0.3646 - val_precision_3k: 0.3022 - val_precision_5k: 0.2256 - val_recall_1k: 0.2884 - val_recall_2k: 0.4220 - val_recall_3k: 0.5159 - val_recall_5k: 0.6245 - val_F1_1k: 0.3584 - val_F1_2k: 0.3911 - val_F1_3k: 0.3810 - val_F1_5k: 0.3313 - val_accuracy_1k: 0.4736 - val_accuracy_2k: 0.6084 - val_accuracy_3k: 0.6877 - val_accuracy_5k: 0.7679 - val_hamming_loss_k: 0.0045
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.0143 - precision_1k: 0.4972 - precision_2k: 0.3882 - precision_3k: 0.3179 - precision_5k: 0.2350 - recall_1k: 0.3023 - recall_2k: 0.4462 - recall_3k: 0.5352 - recall_5k: 0.6430 - F1_1k: 0.3759 - F1_2k: 0.4151 - F1_3k: 0.3988 - F1_5k: 0.3442 - accuracy_1k: 0.4972 - accuracy_2k: 0.6360 - accuracy_3k: 0.7130 - accuracy_5k: 0.7957 - hamming_loss_k: 0.0045
Epoch 00008: val_loss improved from 0.01485 to 0.01447, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 403ms/step - loss: 0.0143 - precision_1k: 0.4972 - precision_2k: 0.3882 - precision_3k: 0.3179 - precision_5k: 0.2350 - recall_1k: 0.3023 - recall_2k: 0.4462 - recall_3k: 0.5352 - recall_5k: 0.6430 - F1_1k: 0.3759 - F1_2k: 0.4151 - F1_3k: 0.3988 - F1_5k: 0.3442 - accuracy_1k: 0.4972 - accuracy_2k: 0.6360 - accuracy_3k: 0.7130 - accuracy_5k: 0.7957 - hamming_loss_k: 0.0045 - val_loss: 0.0145 - val_precision_1k: 0.4905 - val_precision_2k: 0.3802 - val_precision_3k: 0.3098 - val_precision_5k: 0.2274 - val_recall_1k: 0.3003 - val_recall_2k: 0.4435 - val_recall_3k: 0.5306 - val_recall_5k: 0.6361 - val_F1_1k: 0.3724 - val_F1_2k: 0.4093 - val_F1_3k: 0.3910 - val_F1_5k: 0.3349 - val_accuracy_1k: 0.4905 - val_accuracy_2k: 0.6271 - val_accuracy_3k: 0.6967 - val_accuracy_5k: 0.7744 - val_hamming_loss_k: 0.0045
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.0136 - precision_1k: 0.5269 - precision_2k: 0.4109 - precision_3k: 0.3335 - precision_5k: 0.2444 - recall_1k: 0.3213 - recall_2k: 0.4748 - recall_3k: 0.5626 - recall_5k: 0.6687 - F1_1k: 0.3991 - F1_2k: 0.4405 - F1_3k: 0.4187 - F1_5k: 0.3580 - accuracy_1k: 0.5269 - accuracy_2k: 0.6680 - accuracy_3k: 0.7386 - accuracy_5k: 0.8159 - hamming_loss_k: 0.0044 ETA: 1s - loss: 0.0137 - precision_1k: 0.5269 - precision_2k: 0.4108 - precision_3k: 0.3326 - precision_5k: 0.2437 - recall_1k: 0.3201 - recall_2k: 0.4729 - recall_3k: 0.5603 - recall_5k: 0.6648 - F1_1k: 0.3982 - F1_2k: 0.4396 - F1_3k: 0.4174 - F1_5k: 0.3566 - accuracy_1k: 0.5269 - accuracy_2k: 0.6653 - accuracy_3k: 0.7362 - accuracy_5k: 0.8127 - hamming_lo
Epoch 00009: val_loss improved from 0.01447 to 0.01393, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0136 - precision_1k: 0.5269 - precision_2k: 0.4109 - precision_3k: 0.3335 - precision_5k: 0.2444 - recall_1k: 0.3213 - recall_2k: 0.4748 - recall_3k: 0.5626 - recall_5k: 0.6687 - F1_1k: 0.3991 - F1_2k: 0.4405 - F1_3k: 0.4187 - F1_5k: 0.3580 - accuracy_1k: 0.5269 - accuracy_2k: 0.6680 - accuracy_3k: 0.7386 - accuracy_5k: 0.8159 - hamming_loss_k: 0.0044 - val_loss: 0.0139 - val_precision_1k: 0.5233 - val_precision_2k: 0.4043 - val_precision_3k: 0.3262 - val_precision_5k: 0.2386 - val_recall_1k: 0.3231 - val_recall_2k: 0.4772 - val_recall_3k: 0.5622 - val_recall_5k: 0.6691 - val_F1_1k: 0.3994 - val_F1_2k: 0.4376 - val_F1_3k: 0.4126 - val_F1_5k: 0.3516 - val_accuracy_1k: 0.5233 - val_accuracy_2k: 0.6629 - val_accuracy_3k: 0.7299 - val_accuracy_5k: 0.8073 - val_hamming_loss_k: 0.0043
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.0133 - precision_1k: 0.5371 - precision_2k: 0.4177 - precision_3k: 0.3406 - precision_5k: 0.2502 - recall_1k: 0.3290 - recall_2k: 0.4854 - recall_3k: 0.5790 - recall_5k: 0.6875 - F1_1k: 0.4080 - F1_2k: 0.4489 - F1_3k: 0.4288 - F1_5k: 0.3668 - accuracy_1k: 0.5371 - accuracy_2k: 0.6817 - accuracy_3k: 0.7567 - accuracy_5k: 0.8330 - hamming_loss_k: 0.0043 ETA: 7s - loss: 0.0133 - precision_1k: 0.5469 - precision_2k: 0.4248 - precision_3k: 0.3472 - precision_5k: 0.2541 - recall_1k: 0.3319 - recall_2k: 0.4918 - recall_3k: 0.5893 - recall_5k: 0.6914 - F1_1k: 0.4131 - F1_2k: 0.4558 - F1_3k: 0.4369 - F1_5k: 0.3716 - accuracy_1k: 0.5469 - accuracy_2k: 0.6880 - accuracy_3k: 0.7666 
Epoch 00010: val_loss improved from 0.01393 to 0.01383, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 400ms/step - loss: 0.0133 - precision_1k: 0.5371 - precision_2k: 0.4177 - precision_3k: 0.3406 - precision_5k: 0.2502 - recall_1k: 0.3290 - recall_2k: 0.4854 - recall_3k: 0.5790 - recall_5k: 0.6875 - F1_1k: 0.4080 - F1_2k: 0.4489 - F1_3k: 0.4288 - F1_5k: 0.3668 - accuracy_1k: 0.5371 - accuracy_2k: 0.6817 - accuracy_3k: 0.7567 - accuracy_5k: 0.8330 - hamming_loss_k: 0.0043 - val_loss: 0.0138 - val_precision_1k: 0.5142 - val_precision_2k: 0.3983 - val_precision_3k: 0.3248 - val_precision_5k: 0.2393 - val_recall_1k: 0.3183 - val_recall_2k: 0.4688 - val_recall_3k: 0.5601 - val_recall_5k: 0.6683 - val_F1_1k: 0.3931 - val_F1_2k: 0.4306 - val_F1_3k: 0.4110 - val_F1_5k: 0.3523 - val_accuracy_1k: 0.5142 - val_accuracy_2k: 0.6552 - val_accuracy_3k: 0.7278 - val_accuracy_5k: 0.8063 - val_hamming_loss_k: 0.0043
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.0129 - precision_1k: 0.5518 - precision_2k: 0.4331 - precision_3k: 0.3512 - precision_5k: 0.2551 - recall_1k: 0.3396 - recall_2k: 0.5049 - recall_3k: 0.5957 - recall_5k: 0.7014 - F1_1k: 0.4204 - F1_2k: 0.4662 - F1_3k: 0.4418 - F1_5k: 0.3741 - accuracy_1k: 0.5518 - accuracy_2k: 0.7004 - accuracy_3k: 0.7702 - accuracy_5k: 0.8427 - hamming_loss_k: 0.0042
Epoch 00011: val_loss improved from 0.01383 to 0.01374, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 401ms/step - loss: 0.0129 - precision_1k: 0.5518 - precision_2k: 0.4331 - precision_3k: 0.3512 - precision_5k: 0.2551 - recall_1k: 0.3396 - recall_2k: 0.5049 - recall_3k: 0.5957 - recall_5k: 0.7014 - F1_1k: 0.4204 - F1_2k: 0.4662 - F1_3k: 0.4418 - F1_5k: 0.3741 - accuracy_1k: 0.5518 - accuracy_2k: 0.7004 - accuracy_3k: 0.7702 - accuracy_5k: 0.8427 - hamming_loss_k: 0.0042 - val_loss: 0.0137 - val_precision_1k: 0.5222 - val_precision_2k: 0.4051 - val_precision_3k: 0.3323 - val_precision_5k: 0.2417 - val_recall_1k: 0.3225 - val_recall_2k: 0.4777 - val_recall_3k: 0.5744 - val_recall_5k: 0.6742 - val_F1_1k: 0.3986 - val_F1_2k: 0.4382 - val_F1_3k: 0.4209 - val_F1_5k: 0.3557 - val_accuracy_1k: 0.5222 - val_accuracy_2k: 0.6666 - val_accuracy_3k: 0.7434 - val_accuracy_5k: 0.8124 - val_hamming_loss_k: 0.0043
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.0125 - precision_1k: 0.5705 - precision_2k: 0.4459 - precision_3k: 0.3619 - precision_5k: 0.2617 - recall_1k: 0.3534 - recall_2k: 0.5208 - recall_3k: 0.6159 - recall_5k: 0.7212 - F1_1k: 0.4362 - F1_2k: 0.4804 - F1_3k: 0.4559 - F1_5k: 0.3840 - accuracy_1k: 0.5705 - accuracy_2k: 0.7198 - accuracy_3k: 0.7918 - accuracy_5k: 0.8606 - hamming_loss_k: 0.0042
Epoch 00012: val_loss improved from 0.01374 to 0.01342, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 404ms/step - loss: 0.0125 - precision_1k: 0.5705 - precision_2k: 0.4459 - precision_3k: 0.3619 - precision_5k: 0.2617 - recall_1k: 0.3534 - recall_2k: 0.5208 - recall_3k: 0.6159 - recall_5k: 0.7212 - F1_1k: 0.4362 - F1_2k: 0.4804 - F1_3k: 0.4559 - F1_5k: 0.3840 - accuracy_1k: 0.5705 - accuracy_2k: 0.7198 - accuracy_3k: 0.7918 - accuracy_5k: 0.8606 - hamming_loss_k: 0.0042 - val_loss: 0.0134 - val_precision_1k: 0.5398 - val_precision_2k: 0.4198 - val_precision_3k: 0.3392 - val_precision_5k: 0.2456 - val_recall_1k: 0.3353 - val_recall_2k: 0.4957 - val_recall_3k: 0.5835 - val_recall_5k: 0.6868 - val_F1_1k: 0.4136 - val_F1_2k: 0.4545 - val_F1_3k: 0.4288 - val_F1_5k: 0.3617 - val_accuracy_1k: 0.5398 - val_accuracy_2k: 0.6880 - val_accuracy_3k: 0.7539 - val_accuracy_5k: 0.8258 - val_hamming_loss_k: 0.0042
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.0122 - precision_1k: 0.5832 - precision_2k: 0.4549 - precision_3k: 0.3685 - precision_5k: 0.2664 - recall_1k: 0.3623 - recall_2k: 0.5320 - recall_3k: 0.6266 - recall_5k: 0.7314 - F1_1k: 0.4468 - F1_2k: 0.4904 - F1_3k: 0.4640 - F1_5k: 0.3905 - accuracy_1k: 0.5832 - accuracy_2k: 0.7301 - accuracy_3k: 0.7993 - accuracy_5k: 0.8691 - hamming_loss_k: 0.0041
Epoch 00013: val_loss improved from 0.01342 to 0.01321, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0122 - precision_1k: 0.5832 - precision_2k: 0.4549 - precision_3k: 0.3685 - precision_5k: 0.2664 - recall_1k: 0.3623 - recall_2k: 0.5320 - recall_3k: 0.6266 - recall_5k: 0.7314 - F1_1k: 0.4468 - F1_2k: 0.4904 - F1_3k: 0.4640 - F1_5k: 0.3905 - accuracy_1k: 0.5832 - accuracy_2k: 0.7301 - accuracy_3k: 0.7993 - accuracy_5k: 0.8691 - hamming_loss_k: 0.0041 - val_loss: 0.0132 - val_precision_1k: 0.5361 - val_precision_2k: 0.4263 - val_precision_3k: 0.3458 - val_precision_5k: 0.2509 - val_recall_1k: 0.3329 - val_recall_2k: 0.5011 - val_recall_3k: 0.5953 - val_recall_5k: 0.7030 - val_F1_1k: 0.4106 - val_F1_2k: 0.4605 - val_F1_3k: 0.4374 - val_F1_5k: 0.3697 - val_accuracy_1k: 0.5361 - val_accuracy_2k: 0.6927 - val_accuracy_3k: 0.7635 - val_accuracy_5k: 0.8384 - val_hamming_loss_k: 0.0042
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.0119 - precision_1k: 0.6009 - precision_2k: 0.4659 - precision_3k: 0.3755 - precision_5k: 0.2703 - recall_1k: 0.3739 - recall_2k: 0.5461 - recall_3k: 0.6416 - recall_5k: 0.7449 - F1_1k: 0.4609 - F1_2k: 0.5027 - F1_3k: 0.4737 - F1_5k: 0.3966 - accuracy_1k: 0.6009 - accuracy_2k: 0.7475 - accuracy_3k: 0.8165 - accuracy_5k: 0.8802 - hamming_loss_k: 0.0040
Epoch 00014: val_loss improved from 0.01321 to 0.01309, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0119 - precision_1k: 0.6009 - precision_2k: 0.4659 - precision_3k: 0.3755 - precision_5k: 0.2703 - recall_1k: 0.3739 - recall_2k: 0.5461 - recall_3k: 0.6416 - recall_5k: 0.7449 - F1_1k: 0.4609 - F1_2k: 0.5027 - F1_3k: 0.4737 - F1_5k: 0.3966 - accuracy_1k: 0.6009 - accuracy_2k: 0.7475 - accuracy_3k: 0.8165 - accuracy_5k: 0.8802 - hamming_loss_k: 0.0040 - val_loss: 0.0131 - val_precision_1k: 0.5453 - val_precision_2k: 0.4282 - val_precision_3k: 0.3482 - val_precision_5k: 0.2532 - val_recall_1k: 0.3358 - val_recall_2k: 0.5053 - val_recall_3k: 0.6003 - val_recall_5k: 0.7090 - val_F1_1k: 0.4156 - val_F1_2k: 0.4634 - val_F1_3k: 0.4405 - val_F1_5k: 0.3730 - val_accuracy_1k: 0.5453 - val_accuracy_2k: 0.7052 - val_accuracy_3k: 0.7713 - val_accuracy_5k: 0.8442 - val_hamming_loss_k: 0.0042
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.0117 - precision_1k: 0.6065 - precision_2k: 0.4733 - precision_3k: 0.3829 - precision_5k: 0.2753 - recall_1k: 0.3794 - recall_2k: 0.5566 - recall_3k: 0.6529 - recall_5k: 0.7569 - F1_1k: 0.4668 - F1_2k: 0.5115 - F1_3k: 0.4827 - F1_5k: 0.4037 - accuracy_1k: 0.6065 - accuracy_2k: 0.7588 - accuracy_3k: 0.8270 - accuracy_5k: 0.8897 - hamming_loss_k: 0.0040
Epoch 00015: val_loss improved from 0.01309 to 0.01304, saving model to logs/ivzcrh-lab-0604-134040/model/checkpoint_lab.h5
27/27 [==============================] - 11s 402ms/step - loss: 0.0117 - precision_1k: 0.6065 - precision_2k: 0.4733 - precision_3k: 0.3829 - precision_5k: 0.2753 - recall_1k: 0.3794 - recall_2k: 0.5566 - recall_3k: 0.6529 - recall_5k: 0.7569 - F1_1k: 0.4668 - F1_2k: 0.5115 - F1_3k: 0.4827 - F1_5k: 0.4037 - accuracy_1k: 0.6065 - accuracy_2k: 0.7588 - accuracy_3k: 0.8270 - accuracy_5k: 0.8897 - hamming_loss_k: 0.0040 - val_loss: 0.0130 - val_precision_1k: 0.5660 - val_precision_2k: 0.4312 - val_precision_3k: 0.3497 - val_precision_5k: 0.2528 - val_recall_1k: 0.3529 - val_recall_2k: 0.5110 - val_recall_3k: 0.6062 - val_recall_5k: 0.7113 - val_F1_1k: 0.4347 - val_F1_2k: 0.4676 - val_F1_3k: 0.4434 - val_F1_5k: 0.3729 - val_accuracy_1k: 0.5660 - val_accuracy_2k: 0.7033 - val_accuracy_3k: 0.7749 - val_accuracy_5k: 0.8472 - val_hamming_loss_k: 0.0041
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.0114 - precision_1k: 0.6229 - precision_2k: 0.4819 - precision_3k: 0.3880 - precision_5k: 0.2774 - recall_1k: 0.3917 - recall_2k: 0.5674 - recall_3k: 0.6642 - recall_5k: 0.7646 - F1_1k: 0.4809 - F1_2k: 0.5210 - F1_3k: 0.4897 - F1_5k: 0.4071 - accuracy_1k: 0.6229 - accuracy_2k: 0.7665 - accuracy_3k: 0.8355 - accuracy_5k: 0.8958 - hamming_loss_k: 0.0039
Epoch 00016: val_loss did not improve from 0.01304
27/27 [==============================] - 10s 366ms/step - loss: 0.0114 - precision_1k: 0.6229 - precision_2k: 0.4819 - precision_3k: 0.3880 - precision_5k: 0.2774 - recall_1k: 0.3917 - recall_2k: 0.5674 - recall_3k: 0.6642 - recall_5k: 0.7646 - F1_1k: 0.4809 - F1_2k: 0.5210 - F1_3k: 0.4897 - F1_5k: 0.4071 - accuracy_1k: 0.6229 - accuracy_2k: 0.7665 - accuracy_3k: 0.8355 - accuracy_5k: 0.8958 - hamming_loss_k: 0.0039 - val_loss: 0.0131 - val_precision_1k: 0.5573 - val_precision_2k: 0.4281 - val_precision_3k: 0.3466 - val_precision_5k: 0.2518 - val_recall_1k: 0.3488 - val_recall_2k: 0.5106 - val_recall_3k: 0.6003 - val_recall_5k: 0.7050 - val_F1_1k: 0.4290 - val_F1_2k: 0.4656 - val_F1_3k: 0.4393 - val_F1_5k: 0.3709 - val_accuracy_1k: 0.5573 - val_accuracy_2k: 0.7019 - val_accuracy_3k: 0.7655 - val_accuracy_5k: 0.8367 - val_hamming_loss_k: 0.0041
Epoch 00016: early stopping
176/176 [==============================] - 8s 40ms/step - loss: 0.0116 - precision_1k: 0.6092 - precision_2k: 0.4731 - precision_3k: 0.3777 - precision_5k: 0.2733 - recall_1k: 0.3832 - recall_2k: 0.5587 - recall_3k: 0.6515 - recall_5k: 0.7613 - F1_1k: 0.4691 - F1_2k: 0.5111 - F1_3k: 0.4772 - F1_5k: 0.4016 - accuracy_1k: 0.6092 - accuracy_2k: 0.7540 - accuracy_3k: 0.8224 - accuracy_5k: 0.8908 - hamming_loss_k: 0.0039 5s - loss: 0.0117 - precision_1k: 0.6083 - precision_2k: 0.4712 - precision_3k: 0.3757 - precision_5k: 0.2696 - recall_1k: 0.3831 - recall_2k: 0.5581 - recall_3k: 0.6512 - recall_5k: 0.7540 - F1_1k: 0.4689 - F1_2k: 0.5099 - F1_3k: 0.4
Best model result:  [0.011622129008173943, 0.6092411875724792, 0.4730982482433319, 0.37774085998535156, 0.27334892749786377, 0.3832193613052368, 0.5587393045425415, 0.6515370011329651, 0.7613034844398499, 0.469137966632843, 0.5111369490623474, 0.47720786929130554, 0.4015606939792633, 0.6092411875724792, 0.7540109157562256, 0.8223739862442017, 0.8908277750015259, 0.003932130057364702]
fold_result:  [[0.01199701614677906, 0.5961385369300842, 0.4629022479057312, 0.3733745515346527, 0.26976144313812256, 0.37446871399879456, 0.5487017631530762, 0.6429423689842224, 0.7514296174049377, 0.4588415026664734, 0.5010805130004883, 0.4714859127998352, 0.3963235020637512, 0.5961385369300842, 0.7448624968528748, 0.8120739459991455, 0.8798190951347351, 0.003993440885096788], [0.012114888057112694, 0.5885579586029053, 0.45859548449516296, 0.36674928665161133, 0.26487278938293457, 0.36849719285964966, 0.5446760654449463, 0.6342592835426331, 0.7397794723510742, 0.4519336521625519, 0.4967605769634247, 0.4638136923313141, 0.38942983746528625, 0.5885579586029053, 0.7419675588607788, 0.809536874294281, 0.8744494915008545, 0.004028967581689358], [0.011579174548387527, 0.6236657500267029, 0.4761587083339691, 0.3830391764640808, 0.27353474497795105, 0.393881231546402, 0.5627664923667908, 0.657813549041748, 0.7596233487129211, 0.481608122587204, 0.5146691799163818, 0.4831426739692688, 0.4016045331954956, 0.6236657500267029, 0.756704568862915, 0.8212015628814697, 0.8866450786590576, 0.00386453396640718], [0.011524638161063194, 0.6167407631874084, 0.47946593165397644, 0.3854653239250183, 0.27641651034355164, 0.3880425691604614, 0.5655773282051086, 0.663539707660675, 0.7660099267959595, 0.4750547707080841, 0.5178152918815613, 0.48668524622917175, 0.4055831730365753, 0.6167407631874084, 0.7635005712509155, 0.8358138799667358, 0.8914387226104736, 0.003896967973560095], [0.011622129008173943, 0.6092411875724792, 0.4730982482433319, 0.37774085998535156, 0.27334892749786377, 0.3832193613052368, 0.5587393045425415, 0.6515370011329651, 0.7613034844398499, 0.469137966632843, 0.5111369490623474, 0.47720786929130554, 0.4015606939792633, 0.6092411875724792, 0.7540109157562256, 0.8223739862442017, 0.8908277750015259, 0.003932130057364702]]
average_result:  [0.011767569184303283, 0.606868839263916, 0.4700441241264343, 0.37727383971214296, 0.2715868830680847, 0.38162181377410886, 0.5560921907424927, 0.6500183820724488, 0.7556291699409485, 0.46731520295143125, 0.5082925021648407, 0.47646707892417905, 0.39890034794807433, 0.606868839263916, 0.7522092223167419, 0.8202000498771668, 0.8846360325813294, 0.003943208092823625]
2024-06-04 13:43:41,543 : INFO : =======End=======
