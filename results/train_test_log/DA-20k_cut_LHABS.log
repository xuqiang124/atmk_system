/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/home/dzq/anaconda3/envs/k121/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.7.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2024-06-04 12:16:52,395 : INFO : Loading config...
2024-06-04 12:16:52,396 : INFO : {'cache_file_h5py': '../file_data/a30/math_data_cut.h5', 'cache_file_pickle': '../file_data/a32/vocab_label.pkl', 'embeddings': '../file_data/a32/embeddings.pkl', 'maxlen': 150, 'emb_size': 300, 'epochs': 150, 'batch_size': 512, 'alpha': 4, 'hidden_size': 512, 'num_classes_list': [15, 427], 'l_patience': 2, 'b_patience': 10}
2024-06-04 12:16:52,396 : INFO : Loading data...
2024-06-04 12:16:52,479 : INFO : Loading embeddings...
2024-06-04 12:16:52,549 : INFO : model name labs
TOTAL: 22498 TRAIN: [[ 126    3 1315 ...    0    0    0]
 [  44    3   17 ...    0    0    0]
 [ 216    3   11 ...    0    0    0]
 ...
 [ 130    3   78 ...    0    0    0]
 [ 238    3  134 ...    0    0    0]
 [  59    3   78 ...    0    0    0]] 16873 TEST: [[ 105    3 1490 ...   38    0    0]
 [ 238    3 2235 ...    0    0    0]
 [ 172    3  134 ...    0    0    0]
 ...
 [ 161    3 2144 ...    0    0    0]
 [ 161    3 4753 ...    0    0    0]
 [ 189    3   17 ...    0    0    0]] 5625
2024-06-04 12:16:52,582 : INFO : =====Start final=====
13498
3375
5625
2024-06-04 12:16:53.008306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.030875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.030983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.031214: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-04 12:16:53.033170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.033267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.033324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.357443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.357570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.357636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-06-04 12:16:53.357706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22102 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 15, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean (TFOpLambd  (None, 1024)        0           ['BiLSTM[0][0]']                 
 a)                                                                                               
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat (TFOpLambda)         (None, 2048)         0           ['tf.math.reduce_mean[0][0]',    
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense (Dense)                  (None, 1024)         2098176     ['tf.concat[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 15)           15375       ['dense[0][0]']                  
                                                                                                  
 tf.nn.softmax (TFOpLambda)     (None, 15)           0           ['dense_1[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 15, 1)        0           ['tf.nn.softmax[0][0]']          
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims[0][0]']         
 (Dense)                                                                                          
                                                                                                  
 permute_1 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15, 150)      22650       ['lambda_2[0][0]']               
                                                                                                  
 tf.math.reduce_mean_1 (TFOpLam  (None, 150)         0           ['dense_2[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_1[0][0]']  
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply (TFOpLambda)  (None, 150, 1024)    0           ['BiLSTM[0][0]',                 
                                                                  'tf.expand_dims_1[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply[0][0]']       
                                                                                                  
 lambda_3 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_3[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_4 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply[0][0]']       
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_4[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 15, 300)     0           ['label_emb[0][0]']              
 ingOpLambda)                                                                                     
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 permute (Permute)              (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda (Lambda)                (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute[0][0]']                
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda[0][0]']                 
 Dense)                                                                                           
                                                                                                  
 lambda_1 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean (TFOpLambd  (None, 1024)        0           ['BiLSTM[0][0]']                 
 a)                                                                                               
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_1[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat (TFOpLambda)         (None, 2048)         0           ['tf.math.reduce_mean[0][0]',    
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense (Dense)                  (None, 1024)         2098176     ['tf.concat[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 15)           15375       ['dense[0][0]']                  
                                                                                                  
 tf.nn.softmax (TFOpLambda)     (None, 15)           0           ['dense_1[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 15, 1)        0           ['tf.nn.softmax[0][0]']          
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims[0][0]']         
 (Dense)                                                                                          
                                                                                                  
 permute_1 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_2 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15, 150)      22650       ['lambda_2[0][0]']               
                                                                                                  
 tf.math.reduce_mean_1 (TFOpLam  (None, 150)         0           ['dense_2[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_1[0][0]']  
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply (TFOpLambda)  (None, 150, 1024)    0           ['BiLSTM[0][0]',                 
                                                                  'tf.expand_dims_1[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_1[0][0
                                                                 ]']                              
                                                                                                  
 permute_2 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply[0][0]']       
                                                                                                  
 lambda_3 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_2[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_3[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_4 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply[0][0]']       
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_4[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_2[0][0
                                                                 ]']                              
                                                                                                  
 dot (Dot)                      (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot[0][0]']                    
                                                                                                  
 concatenate (Concatenate)      (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/150
2024-06-04 12:16:56.845537: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2024-06-04 12:16:56.879722: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8906
27/27 [==============================] - ETA: 0s - loss: 0.4878 - lcm_precision_1k: 0.2360 - lcm_precision_2k: 0.1932 - lcm_precision_3k: 0.1708 - lcm_precision_5k: 0.1407 - lcm_recall_1k: 0.1283 - lcm_recall_2k: 0.2098 - lcm_recall_3k: 0.2769 - lcm_recall_5k: 0.3748 - lcm_f1_1k: 0.1660 - lcm_f1_2k: 0.2010 - lcm_f1_3k: 0.2112 - lcm_f1_5k: 0.2046 - lcm_accuracy_1k: 0.2360 - lcm_accuracy_2k: 0.3394 - lcm_accuracy_3k: 0.4164 - lcm_accuracy_5k: 0.5148 - lcm_hamming_loss_k: 0.0057
Epoch 00001: val_loss improved from inf to 0.41076, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 14s 415ms/step - loss: 0.4878 - lcm_precision_1k: 0.2360 - lcm_precision_2k: 0.1932 - lcm_precision_3k: 0.1708 - lcm_precision_5k: 0.1407 - lcm_recall_1k: 0.1283 - lcm_recall_2k: 0.2098 - lcm_recall_3k: 0.2769 - lcm_recall_5k: 0.3748 - lcm_f1_1k: 0.1660 - lcm_f1_2k: 0.2010 - lcm_f1_3k: 0.2112 - lcm_f1_5k: 0.2046 - lcm_accuracy_1k: 0.2360 - lcm_accuracy_2k: 0.3394 - lcm_accuracy_3k: 0.4164 - lcm_accuracy_5k: 0.5148 - lcm_hamming_loss_k: 0.0057 - val_loss: 0.4108 - val_lcm_precision_1k: 0.3380 - val_lcm_precision_2k: 0.2794 - val_lcm_precision_3k: 0.2376 - val_lcm_precision_5k: 0.1829 - val_lcm_recall_1k: 0.2051 - val_lcm_recall_2k: 0.3273 - val_lcm_recall_3k: 0.4090 - val_lcm_recall_5k: 0.5119 - val_lcm_f1_1k: 0.2552 - val_lcm_f1_2k: 0.3013 - val_lcm_f1_3k: 0.3005 - val_lcm_f1_5k: 0.2694 - val_lcm_accuracy_1k: 0.3380 - val_lcm_accuracy_2k: 0.4982 - val_lcm_accuracy_3k: 0.5861 - val_lcm_accuracy_5k: 0.6756 - val_lcm_hamming_loss_k: 0.0052
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.3590 - lcm_precision_1k: 0.4170 - lcm_precision_2k: 0.3422 - lcm_precision_3k: 0.2895 - lcm_precision_5k: 0.2211 - lcm_recall_1k: 0.2493 - lcm_recall_2k: 0.3910 - lcm_recall_3k: 0.4866 - lcm_recall_5k: 0.6015 - lcm_f1_1k: 0.3120 - lcm_f1_2k: 0.3649 - lcm_f1_3k: 0.3630 - lcm_f1_5k: 0.3233 - lcm_accuracy_1k: 0.4170 - lcm_accuracy_2k: 0.5735 - lcm_accuracy_3k: 0.6618 - lcm_accuracy_5k: 0.7515 - lcm_hamming_loss_k: 0.0049
Epoch 00002: val_loss improved from 0.41076 to 0.33796, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 440ms/step - loss: 0.3590 - lcm_precision_1k: 0.4170 - lcm_precision_2k: 0.3422 - lcm_precision_3k: 0.2895 - lcm_precision_5k: 0.2211 - lcm_recall_1k: 0.2493 - lcm_recall_2k: 0.3910 - lcm_recall_3k: 0.4866 - lcm_recall_5k: 0.6015 - lcm_f1_1k: 0.3120 - lcm_f1_2k: 0.3649 - lcm_f1_3k: 0.3630 - lcm_f1_5k: 0.3233 - lcm_accuracy_1k: 0.4170 - lcm_accuracy_2k: 0.5735 - lcm_accuracy_3k: 0.6618 - lcm_accuracy_5k: 0.7515 - lcm_hamming_loss_k: 0.0049 - val_loss: 0.3380 - val_lcm_precision_1k: 0.4486 - val_lcm_precision_2k: 0.3571 - val_lcm_precision_3k: 0.3005 - val_lcm_precision_5k: 0.2271 - val_lcm_recall_1k: 0.2761 - val_lcm_recall_2k: 0.4174 - val_lcm_recall_3k: 0.5176 - val_lcm_recall_5k: 0.6331 - val_lcm_f1_1k: 0.3417 - val_lcm_f1_2k: 0.3848 - val_lcm_f1_3k: 0.3801 - val_lcm_f1_5k: 0.3341 - val_lcm_accuracy_1k: 0.4486 - val_lcm_accuracy_2k: 0.6035 - val_lcm_accuracy_3k: 0.6940 - val_lcm_accuracy_5k: 0.7810 - val_lcm_hamming_loss_k: 0.0046
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.3160 - lcm_precision_1k: 0.5036 - lcm_precision_2k: 0.4027 - lcm_precision_3k: 0.3336 - lcm_precision_5k: 0.2469 - lcm_recall_1k: 0.3046 - lcm_recall_2k: 0.4630 - lcm_recall_3k: 0.5632 - lcm_recall_5k: 0.6771 - lcm_f1_1k: 0.3795 - lcm_f1_2k: 0.4307 - lcm_f1_3k: 0.4190 - lcm_f1_5k: 0.3618 - lcm_accuracy_1k: 0.5036 - lcm_accuracy_2k: 0.6565 - lcm_accuracy_3k: 0.7405 - lcm_accuracy_5k: 0.8224 - lcm_hamming_loss_k: 0.0045
Epoch 00003: val_loss improved from 0.33796 to 0.32029, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3160 - lcm_precision_1k: 0.5036 - lcm_precision_2k: 0.4027 - lcm_precision_3k: 0.3336 - lcm_precision_5k: 0.2469 - lcm_recall_1k: 0.3046 - lcm_recall_2k: 0.4630 - lcm_recall_3k: 0.5632 - lcm_recall_5k: 0.6771 - lcm_f1_1k: 0.3795 - lcm_f1_2k: 0.4307 - lcm_f1_3k: 0.4190 - lcm_f1_5k: 0.3618 - lcm_accuracy_1k: 0.5036 - lcm_accuracy_2k: 0.6565 - lcm_accuracy_3k: 0.7405 - lcm_accuracy_5k: 0.8224 - lcm_hamming_loss_k: 0.0045 - val_loss: 0.3203 - val_lcm_precision_1k: 0.5032 - val_lcm_precision_2k: 0.3945 - val_lcm_precision_3k: 0.3261 - val_lcm_precision_5k: 0.2382 - val_lcm_recall_1k: 0.3128 - val_lcm_recall_2k: 0.4672 - val_lcm_recall_3k: 0.5624 - val_lcm_recall_5k: 0.6668 - val_lcm_f1_1k: 0.3857 - val_lcm_f1_2k: 0.4276 - val_lcm_f1_3k: 0.4127 - val_lcm_f1_5k: 0.3509 - val_lcm_accuracy_1k: 0.5032 - val_lcm_accuracy_2k: 0.6543 - val_lcm_accuracy_3k: 0.7335 - val_lcm_accuracy_5k: 0.8116 - val_lcm_hamming_loss_k: 0.0044
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.2960 - lcm_precision_1k: 0.5447 - lcm_precision_2k: 0.4345 - lcm_precision_3k: 0.3565 - lcm_precision_5k: 0.2600 - lcm_recall_1k: 0.3337 - lcm_recall_2k: 0.5044 - lcm_recall_3k: 0.6056 - lcm_recall_5k: 0.7141 - lcm_f1_1k: 0.4137 - lcm_f1_2k: 0.4667 - lcm_f1_3k: 0.4487 - lcm_f1_5k: 0.3811 - lcm_accuracy_1k: 0.5447 - lcm_accuracy_2k: 0.7017 - lcm_accuracy_3k: 0.7788 - lcm_accuracy_5k: 0.8523 - lcm_hamming_loss_k: 0.0043
Epoch 00004: val_loss improved from 0.32029 to 0.30568, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2960 - lcm_precision_1k: 0.5447 - lcm_precision_2k: 0.4345 - lcm_precision_3k: 0.3565 - lcm_precision_5k: 0.2600 - lcm_recall_1k: 0.3337 - lcm_recall_2k: 0.5044 - lcm_recall_3k: 0.6056 - lcm_recall_5k: 0.7141 - lcm_f1_1k: 0.4137 - lcm_f1_2k: 0.4667 - lcm_f1_3k: 0.4487 - lcm_f1_5k: 0.3811 - lcm_accuracy_1k: 0.5447 - lcm_accuracy_2k: 0.7017 - lcm_accuracy_3k: 0.7788 - lcm_accuracy_5k: 0.8523 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3057 - val_lcm_precision_1k: 0.5141 - val_lcm_precision_2k: 0.4048 - val_lcm_precision_3k: 0.3386 - val_lcm_precision_5k: 0.2497 - val_lcm_recall_1k: 0.3222 - val_lcm_recall_2k: 0.4805 - val_lcm_recall_3k: 0.5893 - val_lcm_recall_5k: 0.6988 - val_lcm_f1_1k: 0.3961 - val_lcm_f1_2k: 0.4393 - val_lcm_f1_3k: 0.4299 - val_lcm_f1_5k: 0.3678 - val_lcm_accuracy_1k: 0.5141 - val_lcm_accuracy_2k: 0.6683 - val_lcm_accuracy_3k: 0.7614 - val_lcm_accuracy_5k: 0.8390 - val_lcm_hamming_loss_k: 0.0043
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.2812 - lcm_precision_1k: 0.5751 - lcm_precision_2k: 0.4580 - lcm_precision_3k: 0.3744 - lcm_precision_5k: 0.2716 - lcm_recall_1k: 0.3559 - lcm_recall_2k: 0.5343 - lcm_recall_3k: 0.6375 - lcm_recall_5k: 0.7459 - lcm_f1_1k: 0.4396 - lcm_f1_2k: 0.4932 - lcm_f1_3k: 0.4717 - lcm_f1_5k: 0.3981 - lcm_accuracy_1k: 0.5751 - lcm_accuracy_2k: 0.7321 - lcm_accuracy_3k: 0.8105 - lcm_accuracy_5k: 0.8777 - lcm_hamming_loss_k: 0.0041
Epoch 00005: val_loss improved from 0.30568 to 0.29561, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 444ms/step - loss: 0.2812 - lcm_precision_1k: 0.5751 - lcm_precision_2k: 0.4580 - lcm_precision_3k: 0.3744 - lcm_precision_5k: 0.2716 - lcm_recall_1k: 0.3559 - lcm_recall_2k: 0.5343 - lcm_recall_3k: 0.6375 - lcm_recall_5k: 0.7459 - lcm_f1_1k: 0.4396 - lcm_f1_2k: 0.4932 - lcm_f1_3k: 0.4717 - lcm_f1_5k: 0.3981 - lcm_accuracy_1k: 0.5751 - lcm_accuracy_2k: 0.7321 - lcm_accuracy_3k: 0.8105 - lcm_accuracy_5k: 0.8777 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.2956 - val_lcm_precision_1k: 0.5491 - val_lcm_precision_2k: 0.4308 - val_lcm_precision_3k: 0.3544 - val_lcm_precision_5k: 0.2542 - val_lcm_recall_1k: 0.3453 - val_lcm_recall_2k: 0.5109 - val_lcm_recall_3k: 0.6145 - val_lcm_recall_5k: 0.7134 - val_lcm_f1_1k: 0.4239 - val_lcm_f1_2k: 0.4674 - val_lcm_f1_3k: 0.4494 - val_lcm_f1_5k: 0.3748 - val_lcm_accuracy_1k: 0.5491 - val_lcm_accuracy_2k: 0.7022 - val_lcm_accuracy_3k: 0.7847 - val_lcm_accuracy_5k: 0.8480 - val_lcm_hamming_loss_k: 0.0042
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.2710 - lcm_precision_1k: 0.6051 - lcm_precision_2k: 0.4794 - lcm_precision_3k: 0.3883 - lcm_precision_5k: 0.2790 - lcm_recall_1k: 0.3776 - lcm_recall_2k: 0.5615 - lcm_recall_3k: 0.6623 - lcm_recall_5k: 0.7671 - lcm_f1_1k: 0.4650 - lcm_f1_2k: 0.5172 - lcm_f1_3k: 0.4895 - lcm_f1_5k: 0.4092 - lcm_accuracy_1k: 0.6051 - lcm_accuracy_2k: 0.7631 - lcm_accuracy_3k: 0.8335 - lcm_accuracy_5k: 0.8950 - lcm_hamming_loss_k: 0.0040
Epoch 00006: val_loss improved from 0.29561 to 0.29023, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 443ms/step - loss: 0.2710 - lcm_precision_1k: 0.6051 - lcm_precision_2k: 0.4794 - lcm_precision_3k: 0.3883 - lcm_precision_5k: 0.2790 - lcm_recall_1k: 0.3776 - lcm_recall_2k: 0.5615 - lcm_recall_3k: 0.6623 - lcm_recall_5k: 0.7671 - lcm_f1_1k: 0.4650 - lcm_f1_2k: 0.5172 - lcm_f1_3k: 0.4895 - lcm_f1_5k: 0.4092 - lcm_accuracy_1k: 0.6051 - lcm_accuracy_2k: 0.7631 - lcm_accuracy_3k: 0.8335 - lcm_accuracy_5k: 0.8950 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2902 - val_lcm_precision_1k: 0.5576 - val_lcm_precision_2k: 0.4474 - val_lcm_precision_3k: 0.3602 - val_lcm_precision_5k: 0.2577 - val_lcm_recall_1k: 0.3485 - val_lcm_recall_2k: 0.5293 - val_lcm_recall_3k: 0.6218 - val_lcm_recall_5k: 0.7224 - val_lcm_f1_1k: 0.4289 - val_lcm_f1_2k: 0.4847 - val_lcm_f1_3k: 0.4560 - val_lcm_f1_5k: 0.3798 - val_lcm_accuracy_1k: 0.5576 - val_lcm_accuracy_2k: 0.7199 - val_lcm_accuracy_3k: 0.7928 - val_lcm_accuracy_5k: 0.8574 - val_lcm_hamming_loss_k: 0.0041
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.2615 - lcm_precision_1k: 0.6211 - lcm_precision_2k: 0.4903 - lcm_precision_3k: 0.3982 - lcm_precision_5k: 0.2856 - lcm_recall_1k: 0.3867 - lcm_recall_2k: 0.5749 - lcm_recall_3k: 0.6787 - lcm_recall_5k: 0.7846 - lcm_f1_1k: 0.4765 - lcm_f1_2k: 0.5291 - lcm_f1_3k: 0.5019 - lcm_f1_5k: 0.4187 - lcm_accuracy_1k: 0.6211 - lcm_accuracy_2k: 0.7759 - lcm_accuracy_3k: 0.8468 - lcm_accuracy_5k: 0.9078 - lcm_hamming_loss_k: 0.0039
Epoch 00007: val_loss improved from 0.29023 to 0.28723, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2615 - lcm_precision_1k: 0.6211 - lcm_precision_2k: 0.4903 - lcm_precision_3k: 0.3982 - lcm_precision_5k: 0.2856 - lcm_recall_1k: 0.3867 - lcm_recall_2k: 0.5749 - lcm_recall_3k: 0.6787 - lcm_recall_5k: 0.7846 - lcm_f1_1k: 0.4765 - lcm_f1_2k: 0.5291 - lcm_f1_3k: 0.5019 - lcm_f1_5k: 0.4187 - lcm_accuracy_1k: 0.6211 - lcm_accuracy_2k: 0.7759 - lcm_accuracy_3k: 0.8468 - lcm_accuracy_5k: 0.9078 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2872 - val_lcm_precision_1k: 0.5610 - val_lcm_precision_2k: 0.4470 - val_lcm_precision_3k: 0.3641 - val_lcm_precision_5k: 0.2600 - val_lcm_recall_1k: 0.3539 - val_lcm_recall_2k: 0.5337 - val_lcm_recall_3k: 0.6298 - val_lcm_recall_5k: 0.7298 - val_lcm_f1_1k: 0.4339 - val_lcm_f1_2k: 0.4863 - val_lcm_f1_3k: 0.4613 - val_lcm_f1_5k: 0.3833 - val_lcm_accuracy_1k: 0.5610 - val_lcm_accuracy_2k: 0.7248 - val_lcm_accuracy_3k: 0.8003 - val_lcm_accuracy_5k: 0.8659 - val_lcm_hamming_loss_k: 0.0041
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.2533 - lcm_precision_1k: 0.6344 - lcm_precision_2k: 0.5008 - lcm_precision_3k: 0.4058 - lcm_precision_5k: 0.2906 - lcm_recall_1k: 0.3980 - lcm_recall_2k: 0.5879 - lcm_recall_3k: 0.6912 - lcm_recall_5k: 0.7977 - lcm_f1_1k: 0.4891 - lcm_f1_2k: 0.5408 - lcm_f1_3k: 0.5113 - lcm_f1_5k: 0.4259 - lcm_accuracy_1k: 0.6344 - lcm_accuracy_2k: 0.7903 - lcm_accuracy_3k: 0.8585 - lcm_accuracy_5k: 0.9152 - lcm_hamming_loss_k: 0.0039 ETA: 2s - loss: 0.2528 - lcm_precision_1k: 0.6357 - lcm_precision_2k: 0.5005 - lcm_precision_3k: 0.4066 - lcm_precision_5k: 0.2912 - lcm_recall_1k: 0.3992 - lcm_recall_2k: 0.5880 - lcm_recall_3k: 0.6931 - lcm_recall_5k: 0.7995 - lcm_f1_1k: 0.4904 - lcm_f1_2k: 0.5407 - lcm_f1_3k: 0.5125 - lcm_f1_5k: 0.4269 - lcm_accuracy_1k: 0.6357 - lcm_accuracy_2k: 0.7918 - lcm_accuracy_3k: 0.8629 - lcm_accuracy_5k: 0.9178 - lcm_hammin
Epoch 00008: val_loss improved from 0.28723 to 0.28352, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2533 - lcm_precision_1k: 0.6344 - lcm_precision_2k: 0.5008 - lcm_precision_3k: 0.4058 - lcm_precision_5k: 0.2906 - lcm_recall_1k: 0.3980 - lcm_recall_2k: 0.5879 - lcm_recall_3k: 0.6912 - lcm_recall_5k: 0.7977 - lcm_f1_1k: 0.4891 - lcm_f1_2k: 0.5408 - lcm_f1_3k: 0.5113 - lcm_f1_5k: 0.4259 - lcm_accuracy_1k: 0.6344 - lcm_accuracy_2k: 0.7903 - lcm_accuracy_3k: 0.8585 - lcm_accuracy_5k: 0.9152 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2835 - val_lcm_precision_1k: 0.5777 - val_lcm_precision_2k: 0.4572 - val_lcm_precision_3k: 0.3665 - val_lcm_precision_5k: 0.2617 - val_lcm_recall_1k: 0.3658 - val_lcm_recall_2k: 0.5455 - val_lcm_recall_3k: 0.6375 - val_lcm_recall_5k: 0.7332 - val_lcm_f1_1k: 0.4478 - val_lcm_f1_2k: 0.4973 - val_lcm_f1_3k: 0.4653 - val_lcm_f1_5k: 0.3856 - val_lcm_accuracy_1k: 0.5777 - val_lcm_accuracy_2k: 0.7370 - val_lcm_accuracy_3k: 0.8033 - val_lcm_accuracy_5k: 0.8664 - val_lcm_hamming_loss_k: 0.0040
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.2459 - lcm_precision_1k: 0.6523 - lcm_precision_2k: 0.5153 - lcm_precision_3k: 0.4158 - lcm_precision_5k: 0.2959 - lcm_recall_1k: 0.4112 - lcm_recall_2k: 0.6054 - lcm_recall_3k: 0.7095 - lcm_recall_5k: 0.8117 - lcm_f1_1k: 0.5044 - lcm_f1_2k: 0.5566 - lcm_f1_3k: 0.5242 - lcm_f1_5k: 0.4336 - lcm_accuracy_1k: 0.6523 - lcm_accuracy_2k: 0.8040 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9248 - lcm_hamming_loss_k: 0.0038
Epoch 00009: val_loss improved from 0.28352 to 0.27695, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2459 - lcm_precision_1k: 0.6523 - lcm_precision_2k: 0.5153 - lcm_precision_3k: 0.4158 - lcm_precision_5k: 0.2959 - lcm_recall_1k: 0.4112 - lcm_recall_2k: 0.6054 - lcm_recall_3k: 0.7095 - lcm_recall_5k: 0.8117 - lcm_f1_1k: 0.5044 - lcm_f1_2k: 0.5566 - lcm_f1_3k: 0.5242 - lcm_f1_5k: 0.4336 - lcm_accuracy_1k: 0.6523 - lcm_accuracy_2k: 0.8040 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9248 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2770 - val_lcm_precision_1k: 0.5913 - val_lcm_precision_2k: 0.4647 - val_lcm_precision_3k: 0.3738 - val_lcm_precision_5k: 0.2647 - val_lcm_recall_1k: 0.3716 - val_lcm_recall_2k: 0.5567 - val_lcm_recall_3k: 0.6481 - val_lcm_recall_5k: 0.7455 - val_lcm_f1_1k: 0.4563 - val_lcm_f1_2k: 0.5064 - val_lcm_f1_3k: 0.4740 - val_lcm_f1_5k: 0.3905 - val_lcm_accuracy_1k: 0.5913 - val_lcm_accuracy_2k: 0.7512 - val_lcm_accuracy_3k: 0.8163 - val_lcm_accuracy_5k: 0.8776 - val_lcm_hamming_loss_k: 0.0040
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.2387 - lcm_precision_1k: 0.6692 - lcm_precision_2k: 0.5261 - lcm_precision_3k: 0.4229 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4212 - lcm_recall_2k: 0.6182 - lcm_recall_3k: 0.7197 - lcm_recall_5k: 0.8254 - lcm_f1_1k: 0.5169 - lcm_f1_2k: 0.5683 - lcm_f1_3k: 0.5326 - lcm_f1_5k: 0.4420 - lcm_accuracy_1k: 0.6692 - lcm_accuracy_2k: 0.8189 - lcm_accuracy_3k: 0.8822 - lcm_accuracy_5k: 0.9356 - lcm_hamming_loss_k: 0.0037
Epoch 00010: val_loss improved from 0.27695 to 0.27322, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 439ms/step - loss: 0.2387 - lcm_precision_1k: 0.6692 - lcm_precision_2k: 0.5261 - lcm_precision_3k: 0.4229 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4212 - lcm_recall_2k: 0.6182 - lcm_recall_3k: 0.7197 - lcm_recall_5k: 0.8254 - lcm_f1_1k: 0.5169 - lcm_f1_2k: 0.5683 - lcm_f1_3k: 0.5326 - lcm_f1_5k: 0.4420 - lcm_accuracy_1k: 0.6692 - lcm_accuracy_2k: 0.8189 - lcm_accuracy_3k: 0.8822 - lcm_accuracy_5k: 0.9356 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2732 - val_lcm_precision_1k: 0.5850 - val_lcm_precision_2k: 0.4704 - val_lcm_precision_3k: 0.3796 - val_lcm_precision_5k: 0.2695 - val_lcm_recall_1k: 0.3706 - val_lcm_recall_2k: 0.5615 - val_lcm_recall_3k: 0.6585 - val_lcm_recall_5k: 0.7560 - val_lcm_f1_1k: 0.4536 - val_lcm_f1_2k: 0.5118 - val_lcm_f1_3k: 0.4814 - val_lcm_f1_5k: 0.3973 - val_lcm_accuracy_1k: 0.5850 - val_lcm_accuracy_2k: 0.7506 - val_lcm_accuracy_3k: 0.8257 - val_lcm_accuracy_5k: 0.8824 - val_lcm_hamming_loss_k: 0.0040
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.2327 - lcm_precision_1k: 0.6840 - lcm_precision_2k: 0.5355 - lcm_precision_3k: 0.4313 - lcm_precision_5k: 0.3040 - lcm_recall_1k: 0.4334 - lcm_recall_2k: 0.6305 - lcm_recall_3k: 0.7344 - lcm_recall_5k: 0.8333 - lcm_f1_1k: 0.5305 - lcm_f1_2k: 0.5791 - lcm_f1_3k: 0.5434 - lcm_f1_5k: 0.4454 - lcm_accuracy_1k: 0.6840 - lcm_accuracy_2k: 0.8307 - lcm_accuracy_3k: 0.8928 - lcm_accuracy_5k: 0.9397 - lcm_hamming_loss_k: 0.0036 ETA: 4s - loss: 0.2305 - lcm_precision_1k: 0.6875 - lcm_precision_2k: 0.5376 - lcm_precision_3k: 0.4335 - lcm_precision_5k: 0.3054 - lcm_recall_1k: 0.4371 - lcm_recall_2k: 0.6362 - lcm_recall_3k: 0.7412 - lcm_recall_5k: 0.8412 - lcm_f1_1k: 0.5344 - lcm_f1_2k: 0.5827 - lcm_f1_3k: 0.5470 - lcm_f1_5k: 0.4481 - lcm_accuracy_1k: 0.6875 - lcm_accuracy_2k: 0.8331 - lcm_accuracy_3k: 0.8963 - lcm_accuracy_5k: 0.9446 
Epoch 00011: val_loss improved from 0.27322 to 0.27000, saving model to logs/kkbllm-labs-0604-121652/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2327 - lcm_precision_1k: 0.6840 - lcm_precision_2k: 0.5355 - lcm_precision_3k: 0.4313 - lcm_precision_5k: 0.3040 - lcm_recall_1k: 0.4334 - lcm_recall_2k: 0.6305 - lcm_recall_3k: 0.7344 - lcm_recall_5k: 0.8333 - lcm_f1_1k: 0.5305 - lcm_f1_2k: 0.5791 - lcm_f1_3k: 0.5434 - lcm_f1_5k: 0.4454 - lcm_accuracy_1k: 0.6840 - lcm_accuracy_2k: 0.8307 - lcm_accuracy_3k: 0.8928 - lcm_accuracy_5k: 0.9397 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2700 - val_lcm_precision_1k: 0.6008 - val_lcm_precision_2k: 0.4725 - val_lcm_precision_3k: 0.3790 - val_lcm_precision_5k: 0.2692 - val_lcm_recall_1k: 0.3784 - val_lcm_recall_2k: 0.5637 - val_lcm_recall_3k: 0.6562 - val_lcm_recall_5k: 0.7547 - val_lcm_f1_1k: 0.4642 - val_lcm_f1_2k: 0.5140 - val_lcm_f1_3k: 0.4804 - val_lcm_f1_5k: 0.3967 - val_lcm_accuracy_1k: 0.6008 - val_lcm_accuracy_2k: 0.7613 - val_lcm_accuracy_3k: 0.8234 - val_lcm_accuracy_5k: 0.8815 - val_lcm_hamming_loss_k: 0.0039
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.2255 - lcm_precision_1k: 0.6982 - lcm_precision_2k: 0.5470 - lcm_precision_3k: 0.4382 - lcm_precision_5k: 0.3082 - lcm_recall_1k: 0.4432 - lcm_recall_2k: 0.6448 - lcm_recall_3k: 0.7459 - lcm_recall_5k: 0.8441 - lcm_f1_1k: 0.5421 - lcm_f1_2k: 0.5918 - lcm_f1_3k: 0.5520 - lcm_f1_5k: 0.4515 - lcm_accuracy_1k: 0.6982 - lcm_accuracy_2k: 0.8434 - lcm_accuracy_3k: 0.8997 - lcm_accuracy_5k: 0.9446 - lcm_hamming_loss_k: 0.0036
Epoch 00012: val_loss did not improve from 0.27000
27/27 [==============================] - 10s 389ms/step - loss: 0.2255 - lcm_precision_1k: 0.6982 - lcm_precision_2k: 0.5470 - lcm_precision_3k: 0.4382 - lcm_precision_5k: 0.3082 - lcm_recall_1k: 0.4432 - lcm_recall_2k: 0.6448 - lcm_recall_3k: 0.7459 - lcm_recall_5k: 0.8441 - lcm_f1_1k: 0.5421 - lcm_f1_2k: 0.5918 - lcm_f1_3k: 0.5520 - lcm_f1_5k: 0.4515 - lcm_accuracy_1k: 0.6982 - lcm_accuracy_2k: 0.8434 - lcm_accuracy_3k: 0.8997 - lcm_accuracy_5k: 0.9446 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2707 - val_lcm_precision_1k: 0.6109 - val_lcm_precision_2k: 0.4757 - val_lcm_precision_3k: 0.3843 - val_lcm_precision_5k: 0.2707 - val_lcm_recall_1k: 0.3853 - val_lcm_recall_2k: 0.5681 - val_lcm_recall_3k: 0.6669 - val_lcm_recall_5k: 0.7597 - val_lcm_f1_1k: 0.4725 - val_lcm_f1_2k: 0.5176 - val_lcm_f1_3k: 0.4875 - val_lcm_f1_5k: 0.3991 - val_lcm_accuracy_1k: 0.6109 - val_lcm_accuracy_2k: 0.7648 - val_lcm_accuracy_3k: 0.8310 - val_lcm_accuracy_5k: 0.8877 - val_lcm_hamming_loss_k: 0.0039
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.2191 - lcm_precision_1k: 0.7091 - lcm_precision_2k: 0.5548 - lcm_precision_3k: 0.4451 - lcm_precision_5k: 0.3133 - lcm_recall_1k: 0.4495 - lcm_recall_2k: 0.6535 - lcm_recall_3k: 0.7563 - lcm_recall_5k: 0.8554 - lcm_f1_1k: 0.5501 - lcm_f1_2k: 0.6000 - lcm_f1_3k: 0.5604 - lcm_f1_5k: 0.4586 - lcm_accuracy_1k: 0.7091 - lcm_accuracy_2k: 0.8528 - lcm_accuracy_3k: 0.9083 - lcm_accuracy_5k: 0.9542 - lcm_hamming_loss_k: 0.0035
Epoch 00013: val_loss did not improve from 0.27000
27/27 [==============================] - 11s 391ms/step - loss: 0.2191 - lcm_precision_1k: 0.7091 - lcm_precision_2k: 0.5548 - lcm_precision_3k: 0.4451 - lcm_precision_5k: 0.3133 - lcm_recall_1k: 0.4495 - lcm_recall_2k: 0.6535 - lcm_recall_3k: 0.7563 - lcm_recall_5k: 0.8554 - lcm_f1_1k: 0.5501 - lcm_f1_2k: 0.6000 - lcm_f1_3k: 0.5604 - lcm_f1_5k: 0.4586 - lcm_accuracy_1k: 0.7091 - lcm_accuracy_2k: 0.8528 - lcm_accuracy_3k: 0.9083 - lcm_accuracy_5k: 0.9542 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2791 - val_lcm_precision_1k: 0.6157 - val_lcm_precision_2k: 0.4796 - val_lcm_precision_3k: 0.3846 - val_lcm_precision_5k: 0.2711 - val_lcm_recall_1k: 0.3900 - val_lcm_recall_2k: 0.5737 - val_lcm_recall_3k: 0.6706 - val_lcm_recall_5k: 0.7630 - val_lcm_f1_1k: 0.4774 - val_lcm_f1_2k: 0.5223 - val_lcm_f1_3k: 0.4887 - val_lcm_f1_5k: 0.4000 - val_lcm_accuracy_1k: 0.6157 - val_lcm_accuracy_2k: 0.7655 - val_lcm_accuracy_3k: 0.8378 - val_lcm_accuracy_5k: 0.8913 - val_lcm_hamming_loss_k: 0.0039
Epoch 00013: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2424 - lcm_precision_1k: 0.6644 - lcm_precision_2k: 0.5164 - lcm_precision_3k: 0.4108 - lcm_precision_5k: 0.2922 - lcm_recall_1k: 0.4202 - lcm_recall_2k: 0.6129 - lcm_recall_3k: 0.7046 - lcm_recall_5k: 0.8087 - lcm_f1_1k: 0.5136 - lcm_f1_2k: 0.5594 - lcm_f1_3k: 0.5181 - lcm_f1_5k: 0.4286 - lcm_accuracy_1k: 0.6644 - lcm_accuracy_2k: 0.8113 - lcm_accuracy_3k: 0.8658 - lcm_accuracy_5k: 0.9199 - lcm_hamming_loss_k: 0.0037 3s - loss: 0.2428 - lcm_precision_1k: 0.6691 - lcm_precision_2k: 0.5160 - lcm_precision_3k: 0.4091 - lcm_precision_5k: 0.2921 - lcm_recall_1k: 0.4246 - lcm_recall_2k: 0.6141 - lcm_recall_3k: 0.7068 - lcm_recall_5k: 0.8120 - lcm_f1_1k: 0.5182 - lcm_f1_2k: 0.5595 - lcm_f1_3k: 0.5172 - lcm_f1_5k: 0.4289 - lcm_accuracy_1k: 0.6691 - lcm_accuracy
Best model result:  [0.24239058792591095, 0.6643727421760559, 0.5163941979408264, 0.41083985567092896, 0.29219552874565125, 0.42024943232536316, 0.6129316687583923, 0.7045593857765198, 0.808739423751831, 0.5136027932167053, 0.5594011545181274, 0.5180511474609375, 0.4285776913166046, 0.6643727421760559, 0.8112583756446838, 0.8658292293548584, 0.9199478030204773, 0.003673854283988476]
13498
3375
5625
Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_5 (Lambda)              (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_5[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_6 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_2 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_6[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_2[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_3 (Dense)                (None, 1024)         2098176     ['tf.concat_1[0][0]']            
                                                                                                  
 dense_4 (Dense)                (None, 15)           15375       ['dense_3[0][0]']                
                                                                                                  
 tf.nn.softmax_1 (TFOpLambda)   (None, 15)           0           ['dense_4[0][0]']                
                                                                                                  
 tf.expand_dims_2 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_1[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_2[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_4 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_7 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_4[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 15, 150)      22650       ['lambda_7[0][0]']               
                                                                                                  
 tf.math.reduce_mean_3 (TFOpLam  (None, 150)         0           ['dense_5[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_3 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_3[0][0]']  
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_1 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_3[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_5 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_1[0][0]']     
                                                                                                  
 lambda_8 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_5[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_1[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_3"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_3[0][0
                                                                 ]']                              
                                                                                                  
 permute_3 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_5 (Lambda)              (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_3[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_5[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_6 (Lambda)              (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_2 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_6[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_2[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_3 (Dense)                (None, 1024)         2098176     ['tf.concat_1[0][0]']            
                                                                                                  
 dense_4 (Dense)                (None, 15)           15375       ['dense_3[0][0]']                
                                                                                                  
 tf.nn.softmax_1 (TFOpLambda)   (None, 15)           0           ['dense_4[0][0]']                
                                                                                                  
 tf.expand_dims_2 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_1[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_2[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_4 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_7 (Lambda)              (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_4[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 15, 150)      22650       ['lambda_7[0][0]']               
                                                                                                  
 tf.math.reduce_mean_3 (TFOpLam  (None, 150)         0           ['dense_5[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_3 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_3[0][0]']  
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_1 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_3[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_4[0][0
                                                                 ]']                              
                                                                                                  
 permute_5 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_1[0][0]']     
                                                                                                  
 lambda_8 (Lambda)              (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_5[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_8[0][0]']               
 Dense)                                                                                           
                                                                                                  
 lambda_9 (Lambda)              (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_1[0][0]']     
                                                                                                  
 tf.__operators__.getitem_5 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_9[0][0]']               
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_5[0][0
                                                                 ]']                              
                                                                                                  
 dot_1 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_1[0][0]']                  
                                                                                                  
 concatenate_1 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.4922 - lcm_precision_1k: 0.2393 - lcm_precision_2k: 0.1933 - lcm_precision_3k: 0.1665 - lcm_precision_5k: 0.1350 - lcm_recall_1k: 0.1332 - lcm_recall_2k: 0.2138 - lcm_recall_3k: 0.2731 - lcm_recall_5k: 0.3639 - lcm_f1_1k: 0.1711 - lcm_f1_2k: 0.2029 - lcm_f1_3k: 0.2068 - lcm_f1_5k: 0.1969 - lcm_accuracy_1k: 0.2393 - lcm_accuracy_2k: 0.3438 - lcm_accuracy_3k: 0.4127 - lcm_accuracy_5k: 0.5068 - lcm_hamming_loss_k: 0.0057
Epoch 00001: val_loss improved from inf to 0.40764, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 13s 416ms/step - loss: 0.4922 - lcm_precision_1k: 0.2393 - lcm_precision_2k: 0.1933 - lcm_precision_3k: 0.1665 - lcm_precision_5k: 0.1350 - lcm_recall_1k: 0.1332 - lcm_recall_2k: 0.2138 - lcm_recall_3k: 0.2731 - lcm_recall_5k: 0.3639 - lcm_f1_1k: 0.1711 - lcm_f1_2k: 0.2029 - lcm_f1_3k: 0.2068 - lcm_f1_5k: 0.1969 - lcm_accuracy_1k: 0.2393 - lcm_accuracy_2k: 0.3438 - lcm_accuracy_3k: 0.4127 - lcm_accuracy_5k: 0.5068 - lcm_hamming_loss_k: 0.0057 - val_loss: 0.4076 - val_lcm_precision_1k: 0.3485 - val_lcm_precision_2k: 0.2880 - val_lcm_precision_3k: 0.2432 - val_lcm_precision_5k: 0.1949 - val_lcm_recall_1k: 0.2042 - val_lcm_recall_2k: 0.3264 - val_lcm_recall_3k: 0.4050 - val_lcm_recall_5k: 0.5262 - val_lcm_f1_1k: 0.2574 - val_lcm_f1_2k: 0.3058 - val_lcm_f1_3k: 0.3037 - val_lcm_f1_5k: 0.2844 - val_lcm_accuracy_1k: 0.3485 - val_lcm_accuracy_2k: 0.5031 - val_lcm_accuracy_3k: 0.5841 - val_lcm_accuracy_5k: 0.6873 - val_lcm_hamming_loss_k: 0.0052
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.3609 - lcm_precision_1k: 0.4155 - lcm_precision_2k: 0.3368 - lcm_precision_3k: 0.2823 - lcm_precision_5k: 0.2164 - lcm_recall_1k: 0.2476 - lcm_recall_2k: 0.3862 - lcm_recall_3k: 0.4766 - lcm_recall_5k: 0.5916 - lcm_f1_1k: 0.3102 - lcm_f1_2k: 0.3597 - lcm_f1_3k: 0.3545 - lcm_f1_5k: 0.3169 - lcm_accuracy_1k: 0.4155 - lcm_accuracy_2k: 0.5661 - lcm_accuracy_3k: 0.6539 - lcm_accuracy_5k: 0.7440 - lcm_hamming_loss_k: 0.0049
Epoch 00002: val_loss improved from 0.40764 to 0.33791, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3609 - lcm_precision_1k: 0.4155 - lcm_precision_2k: 0.3368 - lcm_precision_3k: 0.2823 - lcm_precision_5k: 0.2164 - lcm_recall_1k: 0.2476 - lcm_recall_2k: 0.3862 - lcm_recall_3k: 0.4766 - lcm_recall_5k: 0.5916 - lcm_f1_1k: 0.3102 - lcm_f1_2k: 0.3597 - lcm_f1_3k: 0.3545 - lcm_f1_5k: 0.3169 - lcm_accuracy_1k: 0.4155 - lcm_accuracy_2k: 0.5661 - lcm_accuracy_3k: 0.6539 - lcm_accuracy_5k: 0.7440 - lcm_hamming_loss_k: 0.0049 - val_loss: 0.3379 - val_lcm_precision_1k: 0.4734 - val_lcm_precision_2k: 0.3765 - val_lcm_precision_3k: 0.3173 - val_lcm_precision_5k: 0.2351 - val_lcm_recall_1k: 0.2858 - val_lcm_recall_2k: 0.4320 - val_lcm_recall_3k: 0.5380 - val_lcm_recall_5k: 0.6419 - val_lcm_f1_1k: 0.3562 - val_lcm_f1_2k: 0.4021 - val_lcm_f1_3k: 0.3990 - val_lcm_f1_5k: 0.3441 - val_lcm_accuracy_1k: 0.4734 - val_lcm_accuracy_2k: 0.6260 - val_lcm_accuracy_3k: 0.7109 - val_lcm_accuracy_5k: 0.7885 - val_lcm_hamming_loss_k: 0.0046
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.3173 - lcm_precision_1k: 0.4938 - lcm_precision_2k: 0.3963 - lcm_precision_3k: 0.3306 - lcm_precision_5k: 0.2440 - lcm_recall_1k: 0.3008 - lcm_recall_2k: 0.4584 - lcm_recall_3k: 0.5610 - lcm_recall_5k: 0.6718 - lcm_f1_1k: 0.3738 - lcm_f1_2k: 0.4251 - lcm_f1_3k: 0.4160 - lcm_f1_5k: 0.3580 - lcm_accuracy_1k: 0.4938 - lcm_accuracy_2k: 0.6508 - lcm_accuracy_3k: 0.7374 - lcm_accuracy_5k: 0.8177 - lcm_hamming_loss_k: 0.0045
Epoch 00003: val_loss improved from 0.33791 to 0.31850, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3173 - lcm_precision_1k: 0.4938 - lcm_precision_2k: 0.3963 - lcm_precision_3k: 0.3306 - lcm_precision_5k: 0.2440 - lcm_recall_1k: 0.3008 - lcm_recall_2k: 0.4584 - lcm_recall_3k: 0.5610 - lcm_recall_5k: 0.6718 - lcm_f1_1k: 0.3738 - lcm_f1_2k: 0.4251 - lcm_f1_3k: 0.4160 - lcm_f1_5k: 0.3580 - lcm_accuracy_1k: 0.4938 - lcm_accuracy_2k: 0.6508 - lcm_accuracy_3k: 0.7374 - lcm_accuracy_5k: 0.8177 - lcm_hamming_loss_k: 0.0045 - val_loss: 0.3185 - val_lcm_precision_1k: 0.5204 - val_lcm_precision_2k: 0.4183 - val_lcm_precision_3k: 0.3404 - val_lcm_precision_5k: 0.2481 - val_lcm_recall_1k: 0.3129 - val_lcm_recall_2k: 0.4792 - val_lcm_recall_3k: 0.5749 - val_lcm_recall_5k: 0.6768 - val_lcm_f1_1k: 0.3906 - val_lcm_f1_2k: 0.4465 - val_lcm_f1_3k: 0.4274 - val_lcm_f1_5k: 0.3629 - val_lcm_accuracy_1k: 0.5204 - val_lcm_accuracy_2k: 0.6763 - val_lcm_accuracy_3k: 0.7518 - val_lcm_accuracy_5k: 0.8195 - val_lcm_hamming_loss_k: 0.0044
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.2969 - lcm_precision_1k: 0.5422 - lcm_precision_2k: 0.4299 - lcm_precision_3k: 0.3542 - lcm_precision_5k: 0.2581 - lcm_recall_1k: 0.3349 - lcm_recall_2k: 0.5012 - lcm_recall_3k: 0.6034 - lcm_recall_5k: 0.7111 - lcm_f1_1k: 0.4139 - lcm_f1_2k: 0.4627 - lcm_f1_3k: 0.4463 - lcm_f1_5k: 0.3787 - lcm_accuracy_1k: 0.5422 - lcm_accuracy_2k: 0.6968 - lcm_accuracy_3k: 0.7770 - lcm_accuracy_5k: 0.8522 - lcm_hamming_loss_k: 0.0043
Epoch 00004: val_loss improved from 0.31850 to 0.30466, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2969 - lcm_precision_1k: 0.5422 - lcm_precision_2k: 0.4299 - lcm_precision_3k: 0.3542 - lcm_precision_5k: 0.2581 - lcm_recall_1k: 0.3349 - lcm_recall_2k: 0.5012 - lcm_recall_3k: 0.6034 - lcm_recall_5k: 0.7111 - lcm_f1_1k: 0.4139 - lcm_f1_2k: 0.4627 - lcm_f1_3k: 0.4463 - lcm_f1_5k: 0.3787 - lcm_accuracy_1k: 0.5422 - lcm_accuracy_2k: 0.6968 - lcm_accuracy_3k: 0.7770 - lcm_accuracy_5k: 0.8522 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3047 - val_lcm_precision_1k: 0.5406 - val_lcm_precision_2k: 0.4274 - val_lcm_precision_3k: 0.3497 - val_lcm_precision_5k: 0.2549 - val_lcm_recall_1k: 0.3290 - val_lcm_recall_2k: 0.4968 - val_lcm_recall_3k: 0.5938 - val_lcm_recall_5k: 0.6984 - val_lcm_f1_1k: 0.4089 - val_lcm_f1_2k: 0.4593 - val_lcm_f1_3k: 0.4400 - val_lcm_f1_5k: 0.3733 - val_lcm_accuracy_1k: 0.5406 - val_lcm_accuracy_2k: 0.6948 - val_lcm_accuracy_3k: 0.7712 - val_lcm_accuracy_5k: 0.8407 - val_lcm_hamming_loss_k: 0.0043
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.2829 - lcm_precision_1k: 0.5693 - lcm_precision_2k: 0.4513 - lcm_precision_3k: 0.3695 - lcm_precision_5k: 0.2683 - lcm_recall_1k: 0.3554 - lcm_recall_2k: 0.5292 - lcm_recall_3k: 0.6311 - lcm_recall_5k: 0.7397 - lcm_f1_1k: 0.4375 - lcm_f1_2k: 0.4871 - lcm_f1_3k: 0.4661 - lcm_f1_5k: 0.3937 - lcm_accuracy_1k: 0.5693 - lcm_accuracy_2k: 0.7263 - lcm_accuracy_3k: 0.8040 - lcm_accuracy_5k: 0.8731 - lcm_hamming_loss_k: 0.0041 ETA: 4s - loss: 0.2855 - lcm_precision_1k: 0.5620 - lcm_precision_2k: 0.4452 - lcm_precision_3k: 0.3663 - lcm_precision_5k: 0.2670 - lcm_recall_1k: 0.3514 - lcm_recall_2k: 0.5245 - lcm_recall_3k: 0.6258 - lcm_recall_5k: 0.7370 - lcm_f1_1k: 0.4324 - lcm_f1_2k: 0.4815 - lcm_f1_3k: 0.4620 - lcm_f1_5k: 0.3920 - lcm_accuracy_1k: 0.5620 - lcm_accuracy_2k: 0.7207 - lcm_accuracy_3k: 0.8016 - lcm_accuracy_5k: 0.873
Epoch 00005: val_loss improved from 0.30466 to 0.29640, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 433ms/step - loss: 0.2829 - lcm_precision_1k: 0.5693 - lcm_precision_2k: 0.4513 - lcm_precision_3k: 0.3695 - lcm_precision_5k: 0.2683 - lcm_recall_1k: 0.3554 - lcm_recall_2k: 0.5292 - lcm_recall_3k: 0.6311 - lcm_recall_5k: 0.7397 - lcm_f1_1k: 0.4375 - lcm_f1_2k: 0.4871 - lcm_f1_3k: 0.4661 - lcm_f1_5k: 0.3937 - lcm_accuracy_1k: 0.5693 - lcm_accuracy_2k: 0.7263 - lcm_accuracy_3k: 0.8040 - lcm_accuracy_5k: 0.8731 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.2964 - val_lcm_precision_1k: 0.5580 - val_lcm_precision_2k: 0.4453 - val_lcm_precision_3k: 0.3598 - val_lcm_precision_5k: 0.2607 - val_lcm_recall_1k: 0.3415 - val_lcm_recall_2k: 0.5167 - val_lcm_recall_3k: 0.6099 - val_lcm_recall_5k: 0.7178 - val_lcm_f1_1k: 0.4236 - val_lcm_f1_2k: 0.4782 - val_lcm_f1_3k: 0.4525 - val_lcm_f1_5k: 0.3824 - val_lcm_accuracy_1k: 0.5580 - val_lcm_accuracy_2k: 0.7101 - val_lcm_accuracy_3k: 0.7787 - val_lcm_accuracy_5k: 0.8518 - val_lcm_hamming_loss_k: 0.0042
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.2735 - lcm_precision_1k: 0.5943 - lcm_precision_2k: 0.4692 - lcm_precision_3k: 0.3821 - lcm_precision_5k: 0.2756 - lcm_recall_1k: 0.3718 - lcm_recall_2k: 0.5514 - lcm_recall_3k: 0.6545 - lcm_recall_5k: 0.7618 - lcm_f1_1k: 0.4573 - lcm_f1_2k: 0.5070 - lcm_f1_3k: 0.4825 - lcm_f1_5k: 0.4047 - lcm_accuracy_1k: 0.5943 - lcm_accuracy_2k: 0.7512 - lcm_accuracy_3k: 0.8252 - lcm_accuracy_5k: 0.8897 - lcm_hamming_loss_k: 0.0040
Epoch 00006: val_loss improved from 0.29640 to 0.29156, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2735 - lcm_precision_1k: 0.5943 - lcm_precision_2k: 0.4692 - lcm_precision_3k: 0.3821 - lcm_precision_5k: 0.2756 - lcm_recall_1k: 0.3718 - lcm_recall_2k: 0.5514 - lcm_recall_3k: 0.6545 - lcm_recall_5k: 0.7618 - lcm_f1_1k: 0.4573 - lcm_f1_2k: 0.5070 - lcm_f1_3k: 0.4825 - lcm_f1_5k: 0.4047 - lcm_accuracy_1k: 0.5943 - lcm_accuracy_2k: 0.7512 - lcm_accuracy_3k: 0.8252 - lcm_accuracy_5k: 0.8897 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2916 - val_lcm_precision_1k: 0.5747 - val_lcm_precision_2k: 0.4567 - val_lcm_precision_3k: 0.3705 - val_lcm_precision_5k: 0.2660 - val_lcm_recall_1k: 0.3520 - val_lcm_recall_2k: 0.5313 - val_lcm_recall_3k: 0.6301 - val_lcm_recall_5k: 0.7307 - val_lcm_f1_1k: 0.4364 - val_lcm_f1_2k: 0.4910 - val_lcm_f1_3k: 0.4664 - val_lcm_f1_5k: 0.3899 - val_lcm_accuracy_1k: 0.5747 - val_lcm_accuracy_2k: 0.7302 - val_lcm_accuracy_3k: 0.8044 - val_lcm_accuracy_5k: 0.8667 - val_lcm_hamming_loss_k: 0.0041
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.2632 - lcm_precision_1k: 0.6130 - lcm_precision_2k: 0.4867 - lcm_precision_3k: 0.3938 - lcm_precision_5k: 0.2826 - lcm_recall_1k: 0.3829 - lcm_recall_2k: 0.5724 - lcm_recall_3k: 0.6732 - lcm_recall_5k: 0.7794 - lcm_f1_1k: 0.4713 - lcm_f1_2k: 0.5260 - lcm_f1_3k: 0.4969 - lcm_f1_5k: 0.4148 - lcm_accuracy_1k: 0.6130 - lcm_accuracy_2k: 0.7741 - lcm_accuracy_3k: 0.8444 - lcm_accuracy_5k: 0.9038 - lcm_hamming_loss_k: 0.0039
Epoch 00007: val_loss improved from 0.29156 to 0.28626, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2632 - lcm_precision_1k: 0.6130 - lcm_precision_2k: 0.4867 - lcm_precision_3k: 0.3938 - lcm_precision_5k: 0.2826 - lcm_recall_1k: 0.3829 - lcm_recall_2k: 0.5724 - lcm_recall_3k: 0.6732 - lcm_recall_5k: 0.7794 - lcm_f1_1k: 0.4713 - lcm_f1_2k: 0.5260 - lcm_f1_3k: 0.4969 - lcm_f1_5k: 0.4148 - lcm_accuracy_1k: 0.6130 - lcm_accuracy_2k: 0.7741 - lcm_accuracy_3k: 0.8444 - lcm_accuracy_5k: 0.9038 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2863 - val_lcm_precision_1k: 0.5865 - val_lcm_precision_2k: 0.4622 - val_lcm_precision_3k: 0.3745 - val_lcm_precision_5k: 0.2701 - val_lcm_recall_1k: 0.3596 - val_lcm_recall_2k: 0.5402 - val_lcm_recall_3k: 0.6386 - val_lcm_recall_5k: 0.7440 - val_lcm_f1_1k: 0.4457 - val_lcm_f1_2k: 0.4980 - val_lcm_f1_3k: 0.4720 - val_lcm_f1_5k: 0.3962 - val_lcm_accuracy_1k: 0.5865 - val_lcm_accuracy_2k: 0.7399 - val_lcm_accuracy_3k: 0.8123 - val_lcm_accuracy_5k: 0.8760 - val_lcm_hamming_loss_k: 0.0041
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.2543 - lcm_precision_1k: 0.6305 - lcm_precision_2k: 0.4985 - lcm_precision_3k: 0.4047 - lcm_precision_5k: 0.2888 - lcm_recall_1k: 0.3968 - lcm_recall_2k: 0.5901 - lcm_recall_3k: 0.6939 - lcm_recall_5k: 0.7985 - lcm_f1_1k: 0.4870 - lcm_f1_2k: 0.5403 - lcm_f1_3k: 0.5111 - lcm_f1_5k: 0.4241 - lcm_accuracy_1k: 0.6305 - lcm_accuracy_2k: 0.7890 - lcm_accuracy_3k: 0.8586 - lcm_accuracy_5k: 0.9173 - lcm_hamming_loss_k: 0.0038
Epoch 00008: val_loss improved from 0.28626 to 0.28084, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 434ms/step - loss: 0.2543 - lcm_precision_1k: 0.6305 - lcm_precision_2k: 0.4985 - lcm_precision_3k: 0.4047 - lcm_precision_5k: 0.2888 - lcm_recall_1k: 0.3968 - lcm_recall_2k: 0.5901 - lcm_recall_3k: 0.6939 - lcm_recall_5k: 0.7985 - lcm_f1_1k: 0.4870 - lcm_f1_2k: 0.5403 - lcm_f1_3k: 0.5111 - lcm_f1_5k: 0.4241 - lcm_accuracy_1k: 0.6305 - lcm_accuracy_2k: 0.7890 - lcm_accuracy_3k: 0.8586 - lcm_accuracy_5k: 0.9173 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2808 - val_lcm_precision_1k: 0.5911 - val_lcm_precision_2k: 0.4656 - val_lcm_precision_3k: 0.3799 - val_lcm_precision_5k: 0.2709 - val_lcm_recall_1k: 0.3635 - val_lcm_recall_2k: 0.5430 - val_lcm_recall_3k: 0.6469 - val_lcm_recall_5k: 0.7462 - val_lcm_f1_1k: 0.4501 - val_lcm_f1_2k: 0.5012 - val_lcm_f1_3k: 0.4785 - val_lcm_f1_5k: 0.3974 - val_lcm_accuracy_1k: 0.5911 - val_lcm_accuracy_2k: 0.7379 - val_lcm_accuracy_3k: 0.8151 - val_lcm_accuracy_5k: 0.8787 - val_lcm_hamming_loss_k: 0.0041
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.2481 - lcm_precision_1k: 0.6468 - lcm_precision_2k: 0.5085 - lcm_precision_3k: 0.4105 - lcm_precision_5k: 0.2937 - lcm_recall_1k: 0.4076 - lcm_recall_2k: 0.6008 - lcm_recall_3k: 0.7033 - lcm_recall_5k: 0.8103 - lcm_f1_1k: 0.5000 - lcm_f1_2k: 0.5508 - lcm_f1_3k: 0.5183 - lcm_f1_5k: 0.4311 - lcm_accuracy_1k: 0.6468 - lcm_accuracy_2k: 0.8021 - lcm_accuracy_3k: 0.8677 - lcm_accuracy_5k: 0.9234 - lcm_hamming_loss_k: 0.0038
Epoch 00009: val_loss improved from 0.28084 to 0.28042, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2481 - lcm_precision_1k: 0.6468 - lcm_precision_2k: 0.5085 - lcm_precision_3k: 0.4105 - lcm_precision_5k: 0.2937 - lcm_recall_1k: 0.4076 - lcm_recall_2k: 0.6008 - lcm_recall_3k: 0.7033 - lcm_recall_5k: 0.8103 - lcm_f1_1k: 0.5000 - lcm_f1_2k: 0.5508 - lcm_f1_3k: 0.5183 - lcm_f1_5k: 0.4311 - lcm_accuracy_1k: 0.6468 - lcm_accuracy_2k: 0.8021 - lcm_accuracy_3k: 0.8677 - lcm_accuracy_5k: 0.9234 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2804 - val_lcm_precision_1k: 0.5978 - val_lcm_precision_2k: 0.4703 - val_lcm_precision_3k: 0.3819 - val_lcm_precision_5k: 0.2736 - val_lcm_recall_1k: 0.3724 - val_lcm_recall_2k: 0.5538 - val_lcm_recall_3k: 0.6537 - val_lcm_recall_5k: 0.7550 - val_lcm_f1_1k: 0.4587 - val_lcm_f1_2k: 0.5085 - val_lcm_f1_3k: 0.4820 - val_lcm_f1_5k: 0.4015 - val_lcm_accuracy_1k: 0.5978 - val_lcm_accuracy_2k: 0.7525 - val_lcm_accuracy_3k: 0.8237 - val_lcm_accuracy_5k: 0.8868 - val_lcm_hamming_loss_k: 0.0040
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.2406 - lcm_precision_1k: 0.6636 - lcm_precision_2k: 0.5193 - lcm_precision_3k: 0.4187 - lcm_precision_5k: 0.2968 - lcm_recall_1k: 0.4192 - lcm_recall_2k: 0.6145 - lcm_recall_3k: 0.7175 - lcm_recall_5k: 0.8189 - lcm_f1_1k: 0.5137 - lcm_f1_2k: 0.5629 - lcm_f1_3k: 0.5288 - lcm_f1_5k: 0.4356 - lcm_accuracy_1k: 0.6636 - lcm_accuracy_2k: 0.8180 - lcm_accuracy_3k: 0.8789 - lcm_accuracy_5k: 0.9303 - lcm_hamming_loss_k: 0.0037
Epoch 00010: val_loss improved from 0.28042 to 0.27385, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2406 - lcm_precision_1k: 0.6636 - lcm_precision_2k: 0.5193 - lcm_precision_3k: 0.4187 - lcm_precision_5k: 0.2968 - lcm_recall_1k: 0.4192 - lcm_recall_2k: 0.6145 - lcm_recall_3k: 0.7175 - lcm_recall_5k: 0.8189 - lcm_f1_1k: 0.5137 - lcm_f1_2k: 0.5629 - lcm_f1_3k: 0.5288 - lcm_f1_5k: 0.4356 - lcm_accuracy_1k: 0.6636 - lcm_accuracy_2k: 0.8180 - lcm_accuracy_3k: 0.8789 - lcm_accuracy_5k: 0.9303 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2738 - val_lcm_precision_1k: 0.6130 - val_lcm_precision_2k: 0.4797 - val_lcm_precision_3k: 0.3855 - val_lcm_precision_5k: 0.2757 - val_lcm_recall_1k: 0.3812 - val_lcm_recall_2k: 0.5633 - val_lcm_recall_3k: 0.6584 - val_lcm_recall_5k: 0.7571 - val_lcm_f1_1k: 0.4699 - val_lcm_f1_2k: 0.5180 - val_lcm_f1_3k: 0.4861 - val_lcm_f1_5k: 0.4041 - val_lcm_accuracy_1k: 0.6130 - val_lcm_accuracy_2k: 0.7617 - val_lcm_accuracy_3k: 0.8310 - val_lcm_accuracy_5k: 0.8861 - val_lcm_hamming_loss_k: 0.0040
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.2334 - lcm_precision_1k: 0.6770 - lcm_precision_2k: 0.5291 - lcm_precision_3k: 0.4270 - lcm_precision_5k: 0.3022 - lcm_recall_1k: 0.4306 - lcm_recall_2k: 0.6247 - lcm_recall_3k: 0.7300 - lcm_recall_5k: 0.8328 - lcm_f1_1k: 0.5263 - lcm_f1_2k: 0.5729 - lcm_f1_3k: 0.5388 - lcm_f1_5k: 0.4434 - lcm_accuracy_1k: 0.6770 - lcm_accuracy_2k: 0.8268 - lcm_accuracy_3k: 0.8875 - lcm_accuracy_5k: 0.9384 - lcm_hamming_loss_k: 0.0036
Epoch 00011: val_loss improved from 0.27385 to 0.26989, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2334 - lcm_precision_1k: 0.6770 - lcm_precision_2k: 0.5291 - lcm_precision_3k: 0.4270 - lcm_precision_5k: 0.3022 - lcm_recall_1k: 0.4306 - lcm_recall_2k: 0.6247 - lcm_recall_3k: 0.7300 - lcm_recall_5k: 0.8328 - lcm_f1_1k: 0.5263 - lcm_f1_2k: 0.5729 - lcm_f1_3k: 0.5388 - lcm_f1_5k: 0.4434 - lcm_accuracy_1k: 0.6770 - lcm_accuracy_2k: 0.8268 - lcm_accuracy_3k: 0.8875 - lcm_accuracy_5k: 0.9384 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2699 - val_lcm_precision_1k: 0.6182 - val_lcm_precision_2k: 0.4925 - val_lcm_precision_3k: 0.3941 - val_lcm_precision_5k: 0.2770 - val_lcm_recall_1k: 0.3846 - val_lcm_recall_2k: 0.5768 - val_lcm_recall_3k: 0.6699 - val_lcm_recall_5k: 0.7598 - val_lcm_f1_1k: 0.4740 - val_lcm_f1_2k: 0.5311 - val_lcm_f1_3k: 0.4960 - val_lcm_f1_5k: 0.4059 - val_lcm_accuracy_1k: 0.6182 - val_lcm_accuracy_2k: 0.7756 - val_lcm_accuracy_3k: 0.8355 - val_lcm_accuracy_5k: 0.8859 - val_lcm_hamming_loss_k: 0.0039
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.2273 - lcm_precision_1k: 0.6921 - lcm_precision_2k: 0.5407 - lcm_precision_3k: 0.4343 - lcm_precision_5k: 0.3059 - lcm_recall_1k: 0.4393 - lcm_recall_2k: 0.6401 - lcm_recall_3k: 0.7432 - lcm_recall_5k: 0.8423 - lcm_f1_1k: 0.5374 - lcm_f1_2k: 0.5862 - lcm_f1_3k: 0.5481 - lcm_f1_5k: 0.4488 - lcm_accuracy_1k: 0.6921 - lcm_accuracy_2k: 0.8405 - lcm_accuracy_3k: 0.8984 - lcm_accuracy_5k: 0.9452 - lcm_hamming_loss_k: 0.0036
Epoch 00012: val_loss improved from 0.26989 to 0.26981, saving model to logs/cqtoew-labs-0604-121935/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2273 - lcm_precision_1k: 0.6921 - lcm_precision_2k: 0.5407 - lcm_precision_3k: 0.4343 - lcm_precision_5k: 0.3059 - lcm_recall_1k: 0.4393 - lcm_recall_2k: 0.6401 - lcm_recall_3k: 0.7432 - lcm_recall_5k: 0.8423 - lcm_f1_1k: 0.5374 - lcm_f1_2k: 0.5862 - lcm_f1_3k: 0.5481 - lcm_f1_5k: 0.4488 - lcm_accuracy_1k: 0.6921 - lcm_accuracy_2k: 0.8405 - lcm_accuracy_3k: 0.8984 - lcm_accuracy_5k: 0.9452 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2698 - val_lcm_precision_1k: 0.6227 - val_lcm_precision_2k: 0.4880 - val_lcm_precision_3k: 0.3950 - val_lcm_precision_5k: 0.2798 - val_lcm_recall_1k: 0.3866 - val_lcm_recall_2k: 0.5704 - val_lcm_recall_3k: 0.6704 - val_lcm_recall_5k: 0.7673 - val_lcm_f1_1k: 0.4768 - val_lcm_f1_2k: 0.5258 - val_lcm_f1_3k: 0.4969 - val_lcm_f1_5k: 0.4099 - val_lcm_accuracy_1k: 0.6227 - val_lcm_accuracy_2k: 0.7647 - val_lcm_accuracy_3k: 0.8318 - val_lcm_accuracy_5k: 0.8938 - val_lcm_hamming_loss_k: 0.0039
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.2213 - lcm_precision_1k: 0.7025 - lcm_precision_2k: 0.5494 - lcm_precision_3k: 0.4404 - lcm_precision_5k: 0.3103 - lcm_recall_1k: 0.4469 - lcm_recall_2k: 0.6509 - lcm_recall_3k: 0.7545 - lcm_recall_5k: 0.8550 - lcm_f1_1k: 0.5462 - lcm_f1_2k: 0.5958 - lcm_f1_3k: 0.5561 - lcm_f1_5k: 0.4553 - lcm_accuracy_1k: 0.7025 - lcm_accuracy_2k: 0.8493 - lcm_accuracy_3k: 0.9070 - lcm_accuracy_5k: 0.9528 - lcm_hamming_loss_k: 0.0035
Epoch 00013: val_loss did not improve from 0.26981
27/27 [==============================] - 11s 392ms/step - loss: 0.2213 - lcm_precision_1k: 0.7025 - lcm_precision_2k: 0.5494 - lcm_precision_3k: 0.4404 - lcm_precision_5k: 0.3103 - lcm_recall_1k: 0.4469 - lcm_recall_2k: 0.6509 - lcm_recall_3k: 0.7545 - lcm_recall_5k: 0.8550 - lcm_f1_1k: 0.5462 - lcm_f1_2k: 0.5958 - lcm_f1_3k: 0.5561 - lcm_f1_5k: 0.4553 - lcm_accuracy_1k: 0.7025 - lcm_accuracy_2k: 0.8493 - lcm_accuracy_3k: 0.9070 - lcm_accuracy_5k: 0.9528 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2704 - val_lcm_precision_1k: 0.6257 - val_lcm_precision_2k: 0.4912 - val_lcm_precision_3k: 0.3946 - val_lcm_precision_5k: 0.2776 - val_lcm_recall_1k: 0.3889 - val_lcm_recall_2k: 0.5772 - val_lcm_recall_3k: 0.6728 - val_lcm_recall_5k: 0.7667 - val_lcm_f1_1k: 0.4795 - val_lcm_f1_2k: 0.5306 - val_lcm_f1_3k: 0.4973 - val_lcm_f1_5k: 0.4075 - val_lcm_accuracy_1k: 0.6257 - val_lcm_accuracy_2k: 0.7770 - val_lcm_accuracy_3k: 0.8393 - val_lcm_accuracy_5k: 0.8980 - val_lcm_hamming_loss_k: 0.0039
Epoch 00013: early stopping
176/176 [==============================] - 8s 43ms/step - loss: 0.2376 - lcm_precision_1k: 0.6704 - lcm_precision_2k: 0.5246 - lcm_precision_3k: 0.4223 - lcm_precision_5k: 0.2969 - lcm_recall_1k: 0.4268 - lcm_recall_2k: 0.6237 - lcm_recall_3k: 0.7269 - lcm_recall_5k: 0.8222 - lcm_f1_1k: 0.5203 - lcm_f1_2k: 0.5687 - lcm_f1_3k: 0.5332 - lcm_f1_5k: 0.4355 - lcm_accuracy_1k: 0.6704 - lcm_accuracy_2k: 0.8222 - lcm_accuracy_3k: 0.8828 - lcm_accuracy_5k: 0.9312 - lcm_hamming_loss_k: 0.0036 3s - loss: 0.2381 - lcm_precision_1k: 0.6736 - lcm_precision_2k: 0.5243 - lcm_precision_3k: 0.4212 - lcm_precision_5k: 0.2954 - lcm_recall_1k: 0.4288 - lcm_recall_2k: 0.6244 - lcm_recall_3k: 0.7277 - lcm_recall_5k: 0.8231 - lcm_f1_1k: 0.5227 - lcm_f1_2k: 0.5688 - lcm_f1_3k: 0.5324 - lcm_f1_5k: 0.4340 - lcm_accuracy_1k: 0.6736 - lcm_accuracy_2k:
Best model result:  [0.237636998295784, 0.6704132556915283, 0.5246450304985046, 0.42229774594306946, 0.2968762218952179, 0.42680931091308594, 0.6237283945083618, 0.7268832325935364, 0.8221635222434998, 0.5202641487121582, 0.5687291622161865, 0.5331758856773376, 0.4355260133743286, 0.6704132556915283, 0.822195291519165, 0.8827915787696838, 0.9311913847923279, 0.003645578632131219]
13498
3375
5625
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_6 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_10 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_6[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_10[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_11 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_4 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_11[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_4[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_6 (Dense)                (None, 1024)         2098176     ['tf.concat_2[0][0]']            
                                                                                                  
 dense_7 (Dense)                (None, 15)           15375       ['dense_6[0][0]']                
                                                                                                  
 tf.nn.softmax_2 (TFOpLambda)   (None, 15)           0           ['dense_7[0][0]']                
                                                                                                  
 tf.expand_dims_4 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_2[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_4[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_7 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_12 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_7[0][0]']              
                                                                                                  
 dense_8 (Dense)                (None, 15, 150)      22650       ['lambda_12[0][0]']              
                                                                                                  
 tf.math.reduce_mean_5 (TFOpLam  (None, 150)         0           ['dense_8[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_5 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_5[0][0]']  
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_2 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_5[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 permute_8 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_2[0][0]']     
                                                                                                  
 lambda_13 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_8[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_13[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_14 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_2[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_14[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_5"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_6[0][0
                                                                 ]']                              
                                                                                                  
 permute_6 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_10 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_6[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_10[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_11 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_4 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_11[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_4[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_6 (Dense)                (None, 1024)         2098176     ['tf.concat_2[0][0]']            
                                                                                                  
 dense_7 (Dense)                (None, 15)           15375       ['dense_6[0][0]']                
                                                                                                  
 tf.nn.softmax_2 (TFOpLambda)   (None, 15)           0           ['dense_7[0][0]']                
                                                                                                  
 tf.expand_dims_4 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_2[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_4[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_7 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_12 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_7[0][0]']              
                                                                                                  
 dense_8 (Dense)                (None, 15, 150)      22650       ['lambda_12[0][0]']              
                                                                                                  
 tf.math.reduce_mean_5 (TFOpLam  (None, 150)         0           ['dense_8[0][0]']                
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_5 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_5[0][0]']  
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 tf.math.multiply_2 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_5[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_7[0][0
                                                                 ]']                              
                                                                                                  
 permute_8 (Permute)            (None, 1024, 150)    0           ['tf.math.multiply_2[0][0]']     
                                                                                                  
 lambda_13 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_8[0][0]']              
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_13[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_14 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_2[0][0]']     
                                                                                                  
 tf.__operators__.getitem_8 (Sl  (None, 427, 300)    0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_14[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_8[0][0
                                                                 ]']                              
                                                                                                  
 dot_2 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_2[0][0]']                  
                                                                                                  
 concatenate_2 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.4897 - lcm_precision_1k: 0.2183 - lcm_precision_2k: 0.1835 - lcm_precision_3k: 0.1605 - lcm_precision_5k: 0.1357 - lcm_recall_1k: 0.1244 - lcm_recall_2k: 0.2047 - lcm_recall_3k: 0.2643 - lcm_recall_5k: 0.3674 - lcm_f1_1k: 0.1584 - lcm_f1_2k: 0.1934 - lcm_f1_3k: 0.1996 - lcm_f1_5k: 0.1981 - lcm_accuracy_1k: 0.2183 - lcm_accuracy_2k: 0.3263 - lcm_accuracy_3k: 0.3977 - lcm_accuracy_5k: 0.5104 - lcm_hamming_loss_k: 0.0058 ETA: 5s - loss: 0.5453 - lcm_precision_1k: 0.1318 - lcm_precision_2k: 0.1019 - lcm_precision_3k: 0.0920 - lcm_precision_5k: 0.0854 - lcm_recall_1k: 0.0697 - lcm_recall_2k: 0.1064 - lcm_recall_3k: 0.1430 - lcm_recall_5k: 0.2240 - lcm_f1_1k: 0.0912 - lcm_f1_2k: 0.1040 - lcm_f1_3k: 0.1119 - lcm_f1_5k: 0.1236 - lcm_accuracy_1k: 0.1318 - lcm_accuracy_2k: 0.1898 - lcm_accuracy_3k: 0.2395 - lcm_accuracy_5k: 0
Epoch 00001: val_loss improved from inf to 0.41819, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 13s 423ms/step - loss: 0.4897 - lcm_precision_1k: 0.2183 - lcm_precision_2k: 0.1835 - lcm_precision_3k: 0.1605 - lcm_precision_5k: 0.1357 - lcm_recall_1k: 0.1244 - lcm_recall_2k: 0.2047 - lcm_recall_3k: 0.2643 - lcm_recall_5k: 0.3674 - lcm_f1_1k: 0.1584 - lcm_f1_2k: 0.1934 - lcm_f1_3k: 0.1996 - lcm_f1_5k: 0.1981 - lcm_accuracy_1k: 0.2183 - lcm_accuracy_2k: 0.3263 - lcm_accuracy_3k: 0.3977 - lcm_accuracy_5k: 0.5104 - lcm_hamming_loss_k: 0.0058 - val_loss: 0.4182 - val_lcm_precision_1k: 0.3546 - val_lcm_precision_2k: 0.2886 - val_lcm_precision_3k: 0.2472 - val_lcm_precision_5k: 0.1876 - val_lcm_recall_1k: 0.2060 - val_lcm_recall_2k: 0.3271 - val_lcm_recall_3k: 0.4106 - val_lcm_recall_5k: 0.5094 - val_lcm_f1_1k: 0.2604 - val_lcm_f1_2k: 0.3063 - val_lcm_f1_3k: 0.3084 - val_lcm_f1_5k: 0.2740 - val_lcm_accuracy_1k: 0.3546 - val_lcm_accuracy_2k: 0.5043 - val_lcm_accuracy_3k: 0.5918 - val_lcm_accuracy_5k: 0.6765 - val_lcm_hamming_loss_k: 0.0052
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.3606 - lcm_precision_1k: 0.4123 - lcm_precision_2k: 0.3388 - lcm_precision_3k: 0.2854 - lcm_precision_5k: 0.2176 - lcm_recall_1k: 0.2450 - lcm_recall_2k: 0.3883 - lcm_recall_3k: 0.4816 - lcm_recall_5k: 0.5952 - lcm_f1_1k: 0.3073 - lcm_f1_2k: 0.3617 - lcm_f1_3k: 0.3583 - lcm_f1_5k: 0.3186 - lcm_accuracy_1k: 0.4123 - lcm_accuracy_2k: 0.5699 - lcm_accuracy_3k: 0.6563 - lcm_accuracy_5k: 0.7460 - lcm_hamming_loss_k: 0.0049
Epoch 00002: val_loss improved from 0.41819 to 0.34493, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 439ms/step - loss: 0.3606 - lcm_precision_1k: 0.4123 - lcm_precision_2k: 0.3388 - lcm_precision_3k: 0.2854 - lcm_precision_5k: 0.2176 - lcm_recall_1k: 0.2450 - lcm_recall_2k: 0.3883 - lcm_recall_3k: 0.4816 - lcm_recall_5k: 0.5952 - lcm_f1_1k: 0.3073 - lcm_f1_2k: 0.3617 - lcm_f1_3k: 0.3583 - lcm_f1_5k: 0.3186 - lcm_accuracy_1k: 0.4123 - lcm_accuracy_2k: 0.5699 - lcm_accuracy_3k: 0.6563 - lcm_accuracy_5k: 0.7460 - lcm_hamming_loss_k: 0.0049 - val_loss: 0.3449 - val_lcm_precision_1k: 0.4670 - val_lcm_precision_2k: 0.3712 - val_lcm_precision_3k: 0.3082 - val_lcm_precision_5k: 0.2297 - val_lcm_recall_1k: 0.2795 - val_lcm_recall_2k: 0.4207 - val_lcm_recall_3k: 0.5140 - val_lcm_recall_5k: 0.6217 - val_lcm_f1_1k: 0.3494 - val_lcm_f1_2k: 0.3941 - val_lcm_f1_3k: 0.3851 - val_lcm_f1_5k: 0.3353 - val_lcm_accuracy_1k: 0.4670 - val_lcm_accuracy_2k: 0.6141 - val_lcm_accuracy_3k: 0.6948 - val_lcm_accuracy_5k: 0.7760 - val_lcm_hamming_loss_k: 0.0047
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.3165 - lcm_precision_1k: 0.4952 - lcm_precision_2k: 0.4027 - lcm_precision_3k: 0.3316 - lcm_precision_5k: 0.2447 - lcm_recall_1k: 0.3016 - lcm_recall_2k: 0.4664 - lcm_recall_3k: 0.5629 - lcm_recall_5k: 0.6734 - lcm_f1_1k: 0.3748 - lcm_f1_2k: 0.4321 - lcm_f1_3k: 0.4173 - lcm_f1_5k: 0.3589 - lcm_accuracy_1k: 0.4952 - lcm_accuracy_2k: 0.6572 - lcm_accuracy_3k: 0.7399 - lcm_accuracy_5k: 0.8190 - lcm_hamming_loss_k: 0.0045 ETA: 6s - loss: 0.3238 - lcm_precision_1k: 0.4778 - lcm_precision_2k: 0.3952 - lcm_precision_3k: 0.3261 - lcm_precision_5k: 0.2385 - lcm_recall_1k: 0.2912 - lcm_recall_2k: 0.4583 - lcm_recall_3k: 0.5554 - lcm_recall_5k: 0.6603 - lcm_f1_1k: 0.3618 - lcm_f1_2k: 0.4243 - lcm_f1_3k: 0.4109 - lcm_f1_5k: 0.3504 - lcm_accuracy_1k: 0.4778 - lcm_accuracy_2k: 0.6443 - lcm_accuracy_3k: 0.7302 - lcm_accuracy_5
Epoch 00003: val_loss improved from 0.34493 to 0.31924, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3165 - lcm_precision_1k: 0.4952 - lcm_precision_2k: 0.4027 - lcm_precision_3k: 0.3316 - lcm_precision_5k: 0.2447 - lcm_recall_1k: 0.3016 - lcm_recall_2k: 0.4664 - lcm_recall_3k: 0.5629 - lcm_recall_5k: 0.6734 - lcm_f1_1k: 0.3748 - lcm_f1_2k: 0.4321 - lcm_f1_3k: 0.4173 - lcm_f1_5k: 0.3589 - lcm_accuracy_1k: 0.4952 - lcm_accuracy_2k: 0.6572 - lcm_accuracy_3k: 0.7399 - lcm_accuracy_5k: 0.8190 - lcm_hamming_loss_k: 0.0045 - val_loss: 0.3192 - val_lcm_precision_1k: 0.5091 - val_lcm_precision_2k: 0.4001 - val_lcm_precision_3k: 0.3333 - val_lcm_precision_5k: 0.2455 - val_lcm_recall_1k: 0.3100 - val_lcm_recall_2k: 0.4627 - val_lcm_recall_3k: 0.5638 - val_lcm_recall_5k: 0.6709 - val_lcm_f1_1k: 0.3851 - val_lcm_f1_2k: 0.4289 - val_lcm_f1_3k: 0.4187 - val_lcm_f1_5k: 0.3593 - val_lcm_accuracy_1k: 0.5091 - val_lcm_accuracy_2k: 0.6541 - val_lcm_accuracy_3k: 0.7419 - val_lcm_accuracy_5k: 0.8165 - val_lcm_hamming_loss_k: 0.0045
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.2959 - lcm_precision_1k: 0.5415 - lcm_precision_2k: 0.4331 - lcm_precision_3k: 0.3547 - lcm_precision_5k: 0.2590 - lcm_recall_1k: 0.3330 - lcm_recall_2k: 0.5061 - lcm_recall_3k: 0.6046 - lcm_recall_5k: 0.7149 - lcm_f1_1k: 0.4124 - lcm_f1_2k: 0.4667 - lcm_f1_3k: 0.4471 - lcm_f1_5k: 0.3802 - lcm_accuracy_1k: 0.5415 - lcm_accuracy_2k: 0.7019 - lcm_accuracy_3k: 0.7773 - lcm_accuracy_5k: 0.8506 - lcm_hamming_loss_k: 0.0043
Epoch 00004: val_loss improved from 0.31924 to 0.30944, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2959 - lcm_precision_1k: 0.5415 - lcm_precision_2k: 0.4331 - lcm_precision_3k: 0.3547 - lcm_precision_5k: 0.2590 - lcm_recall_1k: 0.3330 - lcm_recall_2k: 0.5061 - lcm_recall_3k: 0.6046 - lcm_recall_5k: 0.7149 - lcm_f1_1k: 0.4124 - lcm_f1_2k: 0.4667 - lcm_f1_3k: 0.4471 - lcm_f1_5k: 0.3802 - lcm_accuracy_1k: 0.5415 - lcm_accuracy_2k: 0.7019 - lcm_accuracy_3k: 0.7773 - lcm_accuracy_5k: 0.8506 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3094 - val_lcm_precision_1k: 0.5372 - val_lcm_precision_2k: 0.4194 - val_lcm_precision_3k: 0.3486 - val_lcm_precision_5k: 0.2546 - val_lcm_recall_1k: 0.3293 - val_lcm_recall_2k: 0.4882 - val_lcm_recall_3k: 0.5909 - val_lcm_recall_5k: 0.6977 - val_lcm_f1_1k: 0.4081 - val_lcm_f1_2k: 0.4509 - val_lcm_f1_3k: 0.4383 - val_lcm_f1_5k: 0.3729 - val_lcm_accuracy_1k: 0.5372 - val_lcm_accuracy_2k: 0.6876 - val_lcm_accuracy_3k: 0.7668 - val_lcm_accuracy_5k: 0.8442 - val_lcm_hamming_loss_k: 0.0043
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.2816 - lcm_precision_1k: 0.5736 - lcm_precision_2k: 0.4583 - lcm_precision_3k: 0.3717 - lcm_precision_5k: 0.2694 - lcm_recall_1k: 0.3572 - lcm_recall_2k: 0.5385 - lcm_recall_3k: 0.6357 - lcm_recall_5k: 0.7439 - lcm_f1_1k: 0.4402 - lcm_f1_2k: 0.4951 - lcm_f1_3k: 0.4691 - lcm_f1_5k: 0.3955 - lcm_accuracy_1k: 0.5736 - lcm_accuracy_2k: 0.7369 - lcm_accuracy_3k: 0.8078 - lcm_accuracy_5k: 0.8757 - lcm_hamming_loss_k: 0.0041
Epoch 00005: val_loss improved from 0.30944 to 0.30085, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2816 - lcm_precision_1k: 0.5736 - lcm_precision_2k: 0.4583 - lcm_precision_3k: 0.3717 - lcm_precision_5k: 0.2694 - lcm_recall_1k: 0.3572 - lcm_recall_2k: 0.5385 - lcm_recall_3k: 0.6357 - lcm_recall_5k: 0.7439 - lcm_f1_1k: 0.4402 - lcm_f1_2k: 0.4951 - lcm_f1_3k: 0.4691 - lcm_f1_5k: 0.3955 - lcm_accuracy_1k: 0.5736 - lcm_accuracy_2k: 0.7369 - lcm_accuracy_3k: 0.8078 - lcm_accuracy_5k: 0.8757 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.3008 - val_lcm_precision_1k: 0.5634 - val_lcm_precision_2k: 0.4407 - val_lcm_precision_3k: 0.3584 - val_lcm_precision_5k: 0.2601 - val_lcm_recall_1k: 0.3459 - val_lcm_recall_2k: 0.5131 - val_lcm_recall_3k: 0.6084 - val_lcm_recall_5k: 0.7118 - val_lcm_f1_1k: 0.4285 - val_lcm_f1_2k: 0.4739 - val_lcm_f1_3k: 0.4508 - val_lcm_f1_5k: 0.3808 - val_lcm_accuracy_1k: 0.5634 - val_lcm_accuracy_2k: 0.7139 - val_lcm_accuracy_3k: 0.7822 - val_lcm_accuracy_5k: 0.8552 - val_lcm_hamming_loss_k: 0.0042
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.2716 - lcm_precision_1k: 0.5988 - lcm_precision_2k: 0.4738 - lcm_precision_3k: 0.3842 - lcm_precision_5k: 0.2763 - lcm_recall_1k: 0.3738 - lcm_recall_2k: 0.5573 - lcm_recall_3k: 0.6578 - lcm_recall_5k: 0.7630 - lcm_f1_1k: 0.4602 - lcm_f1_2k: 0.5121 - lcm_f1_3k: 0.4850 - lcm_f1_5k: 0.4057 - lcm_accuracy_1k: 0.5988 - lcm_accuracy_2k: 0.7559 - lcm_accuracy_3k: 0.8271 - lcm_accuracy_5k: 0.8894 - lcm_hamming_loss_k: 0.0040
Epoch 00006: val_loss improved from 0.30085 to 0.29392, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2716 - lcm_precision_1k: 0.5988 - lcm_precision_2k: 0.4738 - lcm_precision_3k: 0.3842 - lcm_precision_5k: 0.2763 - lcm_recall_1k: 0.3738 - lcm_recall_2k: 0.5573 - lcm_recall_3k: 0.6578 - lcm_recall_5k: 0.7630 - lcm_f1_1k: 0.4602 - lcm_f1_2k: 0.5121 - lcm_f1_3k: 0.4850 - lcm_f1_5k: 0.4057 - lcm_accuracy_1k: 0.5988 - lcm_accuracy_2k: 0.7559 - lcm_accuracy_3k: 0.8271 - lcm_accuracy_5k: 0.8894 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2939 - val_lcm_precision_1k: 0.5692 - val_lcm_precision_2k: 0.4503 - val_lcm_precision_3k: 0.3658 - val_lcm_precision_5k: 0.2650 - val_lcm_recall_1k: 0.3487 - val_lcm_recall_2k: 0.5214 - val_lcm_recall_3k: 0.6198 - val_lcm_recall_5k: 0.7270 - val_lcm_f1_1k: 0.4323 - val_lcm_f1_2k: 0.4830 - val_lcm_f1_3k: 0.4599 - val_lcm_f1_5k: 0.3882 - val_lcm_accuracy_1k: 0.5692 - val_lcm_accuracy_2k: 0.7198 - val_lcm_accuracy_3k: 0.7922 - val_lcm_accuracy_5k: 0.8625 - val_lcm_hamming_loss_k: 0.0042
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.2596 - lcm_precision_1k: 0.6242 - lcm_precision_2k: 0.4937 - lcm_precision_3k: 0.3975 - lcm_precision_5k: 0.2832 - lcm_recall_1k: 0.3916 - lcm_recall_2k: 0.5818 - lcm_recall_3k: 0.6806 - lcm_recall_5k: 0.7831 - lcm_f1_1k: 0.4811 - lcm_f1_2k: 0.5341 - lcm_f1_3k: 0.5018 - lcm_f1_5k: 0.4160 - lcm_accuracy_1k: 0.6242 - lcm_accuracy_2k: 0.7809 - lcm_accuracy_3k: 0.8460 - lcm_accuracy_5k: 0.9043 - lcm_hamming_loss_k: 0.0039
Epoch 00007: val_loss improved from 0.29392 to 0.28424, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2596 - lcm_precision_1k: 0.6242 - lcm_precision_2k: 0.4937 - lcm_precision_3k: 0.3975 - lcm_precision_5k: 0.2832 - lcm_recall_1k: 0.3916 - lcm_recall_2k: 0.5818 - lcm_recall_3k: 0.6806 - lcm_recall_5k: 0.7831 - lcm_f1_1k: 0.4811 - lcm_f1_2k: 0.5341 - lcm_f1_3k: 0.5018 - lcm_f1_5k: 0.4160 - lcm_accuracy_1k: 0.6242 - lcm_accuracy_2k: 0.7809 - lcm_accuracy_3k: 0.8460 - lcm_accuracy_5k: 0.9043 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2842 - val_lcm_precision_1k: 0.5833 - val_lcm_precision_2k: 0.4557 - val_lcm_precision_3k: 0.3727 - val_lcm_precision_5k: 0.2684 - val_lcm_recall_1k: 0.3610 - val_lcm_recall_2k: 0.5307 - val_lcm_recall_3k: 0.6310 - val_lcm_recall_5k: 0.7338 - val_lcm_f1_1k: 0.4457 - val_lcm_f1_2k: 0.4901 - val_lcm_f1_3k: 0.4685 - val_lcm_f1_5k: 0.3928 - val_lcm_accuracy_1k: 0.5833 - val_lcm_accuracy_2k: 0.7319 - val_lcm_accuracy_3k: 0.8022 - val_lcm_accuracy_5k: 0.8710 - val_lcm_hamming_loss_k: 0.0041
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.2528 - lcm_precision_1k: 0.6380 - lcm_precision_2k: 0.5041 - lcm_precision_3k: 0.4058 - lcm_precision_5k: 0.2888 - lcm_recall_1k: 0.4017 - lcm_recall_2k: 0.5948 - lcm_recall_3k: 0.6939 - lcm_recall_5k: 0.7962 - lcm_f1_1k: 0.4929 - lcm_f1_2k: 0.5457 - lcm_f1_3k: 0.5121 - lcm_f1_5k: 0.4238 - lcm_accuracy_1k: 0.6380 - lcm_accuracy_2k: 0.7933 - lcm_accuracy_3k: 0.8574 - lcm_accuracy_5k: 0.9132 - lcm_hamming_loss_k: 0.0038
Epoch 00008: val_loss improved from 0.28424 to 0.28174, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2528 - lcm_precision_1k: 0.6380 - lcm_precision_2k: 0.5041 - lcm_precision_3k: 0.4058 - lcm_precision_5k: 0.2888 - lcm_recall_1k: 0.4017 - lcm_recall_2k: 0.5948 - lcm_recall_3k: 0.6939 - lcm_recall_5k: 0.7962 - lcm_f1_1k: 0.4929 - lcm_f1_2k: 0.5457 - lcm_f1_3k: 0.5121 - lcm_f1_5k: 0.4238 - lcm_accuracy_1k: 0.6380 - lcm_accuracy_2k: 0.7933 - lcm_accuracy_3k: 0.8574 - lcm_accuracy_5k: 0.9132 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2817 - val_lcm_precision_1k: 0.5970 - val_lcm_precision_2k: 0.4630 - val_lcm_precision_3k: 0.3763 - val_lcm_precision_5k: 0.2712 - val_lcm_recall_1k: 0.3668 - val_lcm_recall_2k: 0.5404 - val_lcm_recall_3k: 0.6397 - val_lcm_recall_5k: 0.7452 - val_lcm_f1_1k: 0.4542 - val_lcm_f1_2k: 0.4985 - val_lcm_f1_3k: 0.4737 - val_lcm_f1_5k: 0.3975 - val_lcm_accuracy_1k: 0.5970 - val_lcm_accuracy_2k: 0.7395 - val_lcm_accuracy_3k: 0.8114 - val_lcm_accuracy_5k: 0.8794 - val_lcm_hamming_loss_k: 0.0040
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.2451 - lcm_precision_1k: 0.6553 - lcm_precision_2k: 0.5147 - lcm_precision_3k: 0.4142 - lcm_precision_5k: 0.2926 - lcm_recall_1k: 0.4134 - lcm_recall_2k: 0.6077 - lcm_recall_3k: 0.7076 - lcm_recall_5k: 0.8069 - lcm_f1_1k: 0.5069 - lcm_f1_2k: 0.5573 - lcm_f1_3k: 0.5225 - lcm_f1_5k: 0.4294 - lcm_accuracy_1k: 0.6553 - lcm_accuracy_2k: 0.8068 - lcm_accuracy_3k: 0.8693 - lcm_accuracy_5k: 0.9210 - lcm_hamming_loss_k: 0.0037
Epoch 00009: val_loss improved from 0.28174 to 0.27903, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2451 - lcm_precision_1k: 0.6553 - lcm_precision_2k: 0.5147 - lcm_precision_3k: 0.4142 - lcm_precision_5k: 0.2926 - lcm_recall_1k: 0.4134 - lcm_recall_2k: 0.6077 - lcm_recall_3k: 0.7076 - lcm_recall_5k: 0.8069 - lcm_f1_1k: 0.5069 - lcm_f1_2k: 0.5573 - lcm_f1_3k: 0.5225 - lcm_f1_5k: 0.4294 - lcm_accuracy_1k: 0.6553 - lcm_accuracy_2k: 0.8068 - lcm_accuracy_3k: 0.8693 - lcm_accuracy_5k: 0.9210 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2790 - val_lcm_precision_1k: 0.5968 - val_lcm_precision_2k: 0.4649 - val_lcm_precision_3k: 0.3799 - val_lcm_precision_5k: 0.2731 - val_lcm_recall_1k: 0.3699 - val_lcm_recall_2k: 0.5410 - val_lcm_recall_3k: 0.6454 - val_lcm_recall_5k: 0.7500 - val_lcm_f1_1k: 0.4565 - val_lcm_f1_2k: 0.4999 - val_lcm_f1_3k: 0.4780 - val_lcm_f1_5k: 0.4002 - val_lcm_accuracy_1k: 0.5968 - val_lcm_accuracy_2k: 0.7382 - val_lcm_accuracy_3k: 0.8152 - val_lcm_accuracy_5k: 0.8867 - val_lcm_hamming_loss_k: 0.0040
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.2372 - lcm_precision_1k: 0.6715 - lcm_precision_2k: 0.5249 - lcm_precision_3k: 0.4210 - lcm_precision_5k: 0.2976 - lcm_recall_1k: 0.4254 - lcm_recall_2k: 0.6197 - lcm_recall_3k: 0.7195 - lcm_recall_5k: 0.8208 - lcm_f1_1k: 0.5208 - lcm_f1_2k: 0.5683 - lcm_f1_3k: 0.5311 - lcm_f1_5k: 0.4368 - lcm_accuracy_1k: 0.6715 - lcm_accuracy_2k: 0.8201 - lcm_accuracy_3k: 0.8783 - lcm_accuracy_5k: 0.9319 - lcm_hamming_loss_k: 0.0037
Epoch 00010: val_loss improved from 0.27903 to 0.27580, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2372 - lcm_precision_1k: 0.6715 - lcm_precision_2k: 0.5249 - lcm_precision_3k: 0.4210 - lcm_precision_5k: 0.2976 - lcm_recall_1k: 0.4254 - lcm_recall_2k: 0.6197 - lcm_recall_3k: 0.7195 - lcm_recall_5k: 0.8208 - lcm_f1_1k: 0.5208 - lcm_f1_2k: 0.5683 - lcm_f1_3k: 0.5311 - lcm_f1_5k: 0.4368 - lcm_accuracy_1k: 0.6715 - lcm_accuracy_2k: 0.8201 - lcm_accuracy_3k: 0.8783 - lcm_accuracy_5k: 0.9319 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2758 - val_lcm_precision_1k: 0.6081 - val_lcm_precision_2k: 0.4668 - val_lcm_precision_3k: 0.3817 - val_lcm_precision_5k: 0.2734 - val_lcm_recall_1k: 0.3770 - val_lcm_recall_2k: 0.5446 - val_lcm_recall_3k: 0.6461 - val_lcm_recall_5k: 0.7491 - val_lcm_f1_1k: 0.4652 - val_lcm_f1_2k: 0.5025 - val_lcm_f1_3k: 0.4797 - val_lcm_f1_5k: 0.4004 - val_lcm_accuracy_1k: 0.6081 - val_lcm_accuracy_2k: 0.7429 - val_lcm_accuracy_3k: 0.8144 - val_lcm_accuracy_5k: 0.8814 - val_lcm_hamming_loss_k: 0.0040
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.2313 - lcm_precision_1k: 0.6844 - lcm_precision_2k: 0.5335 - lcm_precision_3k: 0.4282 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4342 - lcm_recall_2k: 0.6305 - lcm_recall_3k: 0.7327 - lcm_recall_5k: 0.8323 - lcm_f1_1k: 0.5312 - lcm_f1_2k: 0.5778 - lcm_f1_3k: 0.5405 - lcm_f1_5k: 0.4430 - lcm_accuracy_1k: 0.6844 - lcm_accuracy_2k: 0.8279 - lcm_accuracy_3k: 0.8899 - lcm_accuracy_5k: 0.9378 - lcm_hamming_loss_k: 0.0036
Epoch 00011: val_loss improved from 0.27580 to 0.27232, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2313 - lcm_precision_1k: 0.6844 - lcm_precision_2k: 0.5335 - lcm_precision_3k: 0.4282 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4342 - lcm_recall_2k: 0.6305 - lcm_recall_3k: 0.7327 - lcm_recall_5k: 0.8323 - lcm_f1_1k: 0.5312 - lcm_f1_2k: 0.5778 - lcm_f1_3k: 0.5405 - lcm_f1_5k: 0.4430 - lcm_accuracy_1k: 0.6844 - lcm_accuracy_2k: 0.8279 - lcm_accuracy_3k: 0.8899 - lcm_accuracy_5k: 0.9378 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2723 - val_lcm_precision_1k: 0.6116 - val_lcm_precision_2k: 0.4754 - val_lcm_precision_3k: 0.3842 - val_lcm_precision_5k: 0.2742 - val_lcm_recall_1k: 0.3785 - val_lcm_recall_2k: 0.5512 - val_lcm_recall_3k: 0.6465 - val_lcm_recall_5k: 0.7504 - val_lcm_f1_1k: 0.4674 - val_lcm_f1_2k: 0.5103 - val_lcm_f1_3k: 0.4818 - val_lcm_f1_5k: 0.4014 - val_lcm_accuracy_1k: 0.6116 - val_lcm_accuracy_2k: 0.7529 - val_lcm_accuracy_3k: 0.8180 - val_lcm_accuracy_5k: 0.8856 - val_lcm_hamming_loss_k: 0.0040
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.2257 - lcm_precision_1k: 0.6957 - lcm_precision_2k: 0.5423 - lcm_precision_3k: 0.4347 - lcm_precision_5k: 0.3064 - lcm_recall_1k: 0.4439 - lcm_recall_2k: 0.6404 - lcm_recall_3k: 0.7425 - lcm_recall_5k: 0.8437 - lcm_f1_1k: 0.5419 - lcm_f1_2k: 0.5872 - lcm_f1_3k: 0.5483 - lcm_f1_5k: 0.4495 - lcm_accuracy_1k: 0.6957 - lcm_accuracy_2k: 0.8376 - lcm_accuracy_3k: 0.8969 - lcm_accuracy_5k: 0.9454 - lcm_hamming_loss_k: 0.0035 ETA: 3s - loss: 0.2240 - lcm_precision_1k: 0.7011 - lcm_precision_2k: 0.5448 - lcm_precision_3k: 0.4353 - lcm_precision_5k: 0.3060 - lcm_recall_1k: 0.4481 - lcm_recall_2k: 0.6432 - lcm_recall_3k: 0.7440 - lcm_recall_5k: 0.8445 - lcm_f1_1k: 0.5467 - lcm_f1_2k: 0.5899 - lcm_f1_3k: 0.5492 - lcm_f1_5k: 0.4492 - lcm_accuracy_1k: 0.7011 - lcm_accuracy_2k: 0.8377 - lcm_accuracy_3k: 0.8973 - lcm_accuracy_5k: 0.9458 - lcm_ha
Epoch 00012: val_loss improved from 0.27232 to 0.27071, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2257 - lcm_precision_1k: 0.6957 - lcm_precision_2k: 0.5423 - lcm_precision_3k: 0.4347 - lcm_precision_5k: 0.3064 - lcm_recall_1k: 0.4439 - lcm_recall_2k: 0.6404 - lcm_recall_3k: 0.7425 - lcm_recall_5k: 0.8437 - lcm_f1_1k: 0.5419 - lcm_f1_2k: 0.5872 - lcm_f1_3k: 0.5483 - lcm_f1_5k: 0.4495 - lcm_accuracy_1k: 0.6957 - lcm_accuracy_2k: 0.8376 - lcm_accuracy_3k: 0.8969 - lcm_accuracy_5k: 0.9454 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2707 - val_lcm_precision_1k: 0.6195 - val_lcm_precision_2k: 0.4805 - val_lcm_precision_3k: 0.3865 - val_lcm_precision_5k: 0.2766 - val_lcm_recall_1k: 0.3846 - val_lcm_recall_2k: 0.5595 - val_lcm_recall_3k: 0.6544 - val_lcm_recall_5k: 0.7580 - val_lcm_f1_1k: 0.4743 - val_lcm_f1_2k: 0.5168 - val_lcm_f1_3k: 0.4857 - val_lcm_f1_5k: 0.4051 - val_lcm_accuracy_1k: 0.6195 - val_lcm_accuracy_2k: 0.7553 - val_lcm_accuracy_3k: 0.8237 - val_lcm_accuracy_5k: 0.8900 - val_lcm_hamming_loss_k: 0.0039
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.2179 - lcm_precision_1k: 0.7132 - lcm_precision_2k: 0.5564 - lcm_precision_3k: 0.4439 - lcm_precision_5k: 0.3108 - lcm_recall_1k: 0.4542 - lcm_recall_2k: 0.6571 - lcm_recall_3k: 0.7575 - lcm_recall_5k: 0.8543 - lcm_f1_1k: 0.5549 - lcm_f1_2k: 0.6025 - lcm_f1_3k: 0.5597 - lcm_f1_5k: 0.4557 - lcm_accuracy_1k: 0.7132 - lcm_accuracy_2k: 0.8531 - lcm_accuracy_3k: 0.9074 - lcm_accuracy_5k: 0.9513 - lcm_hamming_loss_k: 0.0035
Epoch 00013: val_loss improved from 0.27071 to 0.27031, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2179 - lcm_precision_1k: 0.7132 - lcm_precision_2k: 0.5564 - lcm_precision_3k: 0.4439 - lcm_precision_5k: 0.3108 - lcm_recall_1k: 0.4542 - lcm_recall_2k: 0.6571 - lcm_recall_3k: 0.7575 - lcm_recall_5k: 0.8543 - lcm_f1_1k: 0.5549 - lcm_f1_2k: 0.6025 - lcm_f1_3k: 0.5597 - lcm_f1_5k: 0.4557 - lcm_accuracy_1k: 0.7132 - lcm_accuracy_2k: 0.8531 - lcm_accuracy_3k: 0.9074 - lcm_accuracy_5k: 0.9513 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2703 - val_lcm_precision_1k: 0.6221 - val_lcm_precision_2k: 0.4798 - val_lcm_precision_3k: 0.3846 - val_lcm_precision_5k: 0.2748 - val_lcm_recall_1k: 0.3842 - val_lcm_recall_2k: 0.5588 - val_lcm_recall_3k: 0.6519 - val_lcm_recall_5k: 0.7534 - val_lcm_f1_1k: 0.4748 - val_lcm_f1_2k: 0.5160 - val_lcm_f1_3k: 0.4835 - val_lcm_f1_5k: 0.4025 - val_lcm_accuracy_1k: 0.6221 - val_lcm_accuracy_2k: 0.7537 - val_lcm_accuracy_3k: 0.8187 - val_lcm_accuracy_5k: 0.8873 - val_lcm_hamming_loss_k: 0.0039
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.2120 - lcm_precision_1k: 0.7268 - lcm_precision_2k: 0.5660 - lcm_precision_3k: 0.4511 - lcm_precision_5k: 0.3152 - lcm_recall_1k: 0.4635 - lcm_recall_2k: 0.6682 - lcm_recall_3k: 0.7685 - lcm_recall_5k: 0.8654 - lcm_f1_1k: 0.5659 - lcm_f1_2k: 0.6128 - lcm_f1_3k: 0.5684 - lcm_f1_5k: 0.4621 - lcm_accuracy_1k: 0.7268 - lcm_accuracy_2k: 0.8644 - lcm_accuracy_3k: 0.9146 - lcm_accuracy_5k: 0.9571 - lcm_hamming_loss_k: 0.0034
Epoch 00014: val_loss improved from 0.27031 to 0.26707, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.2120 - lcm_precision_1k: 0.7268 - lcm_precision_2k: 0.5660 - lcm_precision_3k: 0.4511 - lcm_precision_5k: 0.3152 - lcm_recall_1k: 0.4635 - lcm_recall_2k: 0.6682 - lcm_recall_3k: 0.7685 - lcm_recall_5k: 0.8654 - lcm_f1_1k: 0.5659 - lcm_f1_2k: 0.6128 - lcm_f1_3k: 0.5684 - lcm_f1_5k: 0.4621 - lcm_accuracy_1k: 0.7268 - lcm_accuracy_2k: 0.8644 - lcm_accuracy_3k: 0.9146 - lcm_accuracy_5k: 0.9571 - lcm_hamming_loss_k: 0.0034 - val_loss: 0.2671 - val_lcm_precision_1k: 0.6281 - val_lcm_precision_2k: 0.4866 - val_lcm_precision_3k: 0.3904 - val_lcm_precision_5k: 0.2775 - val_lcm_recall_1k: 0.3924 - val_lcm_recall_2k: 0.5669 - val_lcm_recall_3k: 0.6640 - val_lcm_recall_5k: 0.7613 - val_lcm_f1_1k: 0.4828 - val_lcm_f1_2k: 0.5235 - val_lcm_f1_3k: 0.4915 - val_lcm_f1_5k: 0.4066 - val_lcm_accuracy_1k: 0.6281 - val_lcm_accuracy_2k: 0.7631 - val_lcm_accuracy_3k: 0.8284 - val_lcm_accuracy_5k: 0.8906 - val_lcm_hamming_loss_k: 0.0039
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.2074 - lcm_precision_1k: 0.7363 - lcm_precision_2k: 0.5734 - lcm_precision_3k: 0.4553 - lcm_precision_5k: 0.3179 - lcm_recall_1k: 0.4712 - lcm_recall_2k: 0.6784 - lcm_recall_3k: 0.7776 - lcm_recall_5k: 0.8734 - lcm_f1_1k: 0.5745 - lcm_f1_2k: 0.6214 - lcm_f1_3k: 0.5742 - lcm_f1_5k: 0.4661 - lcm_accuracy_1k: 0.7363 - lcm_accuracy_2k: 0.8734 - lcm_accuracy_3k: 0.9218 - lcm_accuracy_5k: 0.9628 - lcm_hamming_loss_k: 0.0033 ETA: 7s - loss: 0.1986 - lcm_precision_1k: 0.7578 - lcm_precision_2k: 0.5843 - lcm_precision_3k: 0.4642 - lcm_precision_5k: 0.3187 - lcm_recall_1k: 0.4905 - lcm_recall_2k: 0.7008 - lcm_recall_3k: 0.7995 - lcm_recall_5k: 0.8840 - lcm_f1_1k: 0.5954 - lcm_f1_2k: 0.6373 - lcm_f1_3k: 0.5873 - lcm_f1_5k: 0.4685 - lcm_accuracy_1k: 0.7578 - lcm_accuracy_2k: 0.8952 - lcm_accuracy_3k: 0.9355 - lcm_
Epoch 00015: val_loss improved from 0.26707 to 0.26575, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2074 - lcm_precision_1k: 0.7363 - lcm_precision_2k: 0.5734 - lcm_precision_3k: 0.4553 - lcm_precision_5k: 0.3179 - lcm_recall_1k: 0.4712 - lcm_recall_2k: 0.6784 - lcm_recall_3k: 0.7776 - lcm_recall_5k: 0.8734 - lcm_f1_1k: 0.5745 - lcm_f1_2k: 0.6214 - lcm_f1_3k: 0.5742 - lcm_f1_5k: 0.4661 - lcm_accuracy_1k: 0.7363 - lcm_accuracy_2k: 0.8734 - lcm_accuracy_3k: 0.9218 - lcm_accuracy_5k: 0.9628 - lcm_hamming_loss_k: 0.0033 - val_loss: 0.2658 - val_lcm_precision_1k: 0.6292 - val_lcm_precision_2k: 0.4849 - val_lcm_precision_3k: 0.3924 - val_lcm_precision_5k: 0.2788 - val_lcm_recall_1k: 0.3920 - val_lcm_recall_2k: 0.5666 - val_lcm_recall_3k: 0.6679 - val_lcm_recall_5k: 0.7641 - val_lcm_f1_1k: 0.4828 - val_lcm_f1_2k: 0.5224 - val_lcm_f1_3k: 0.4941 - val_lcm_f1_5k: 0.4083 - val_lcm_accuracy_1k: 0.6292 - val_lcm_accuracy_2k: 0.7573 - val_lcm_accuracy_3k: 0.8359 - val_lcm_accuracy_5k: 0.8949 - val_lcm_hamming_loss_k: 0.0039
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.2006 - lcm_precision_1k: 0.7493 - lcm_precision_2k: 0.5833 - lcm_precision_3k: 0.4630 - lcm_precision_5k: 0.3222 - lcm_recall_1k: 0.4807 - lcm_recall_2k: 0.6905 - lcm_recall_3k: 0.7901 - lcm_recall_5k: 0.8841 - lcm_f1_1k: 0.5855 - lcm_f1_2k: 0.6322 - lcm_f1_3k: 0.5837 - lcm_f1_5k: 0.4722 - lcm_accuracy_1k: 0.7493 - lcm_accuracy_2k: 0.8824 - lcm_accuracy_3k: 0.9300 - lcm_accuracy_5k: 0.9665 - lcm_hamming_loss_k: 0.0033
Epoch 00016: val_loss improved from 0.26575 to 0.26511, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 434ms/step - loss: 0.2006 - lcm_precision_1k: 0.7493 - lcm_precision_2k: 0.5833 - lcm_precision_3k: 0.4630 - lcm_precision_5k: 0.3222 - lcm_recall_1k: 0.4807 - lcm_recall_2k: 0.6905 - lcm_recall_3k: 0.7901 - lcm_recall_5k: 0.8841 - lcm_f1_1k: 0.5855 - lcm_f1_2k: 0.6322 - lcm_f1_3k: 0.5837 - lcm_f1_5k: 0.4722 - lcm_accuracy_1k: 0.7493 - lcm_accuracy_2k: 0.8824 - lcm_accuracy_3k: 0.9300 - lcm_accuracy_5k: 0.9665 - lcm_hamming_loss_k: 0.0033 - val_loss: 0.2651 - val_lcm_precision_1k: 0.6261 - val_lcm_precision_2k: 0.4895 - val_lcm_precision_3k: 0.3956 - val_lcm_precision_5k: 0.2793 - val_lcm_recall_1k: 0.3895 - val_lcm_recall_2k: 0.5686 - val_lcm_recall_3k: 0.6696 - val_lcm_recall_5k: 0.7626 - val_lcm_f1_1k: 0.4800 - val_lcm_f1_2k: 0.5259 - val_lcm_f1_3k: 0.4971 - val_lcm_f1_5k: 0.4087 - val_lcm_accuracy_1k: 0.6261 - val_lcm_accuracy_2k: 0.7666 - val_lcm_accuracy_3k: 0.8370 - val_lcm_accuracy_5k: 0.8903 - val_lcm_hamming_loss_k: 0.0039
Epoch 17/150
27/27 [==============================] - ETA: 0s - loss: 0.1940 - lcm_precision_1k: 0.7611 - lcm_precision_2k: 0.5945 - lcm_precision_3k: 0.4736 - lcm_precision_5k: 0.3275 - lcm_recall_1k: 0.4868 - lcm_recall_2k: 0.7011 - lcm_recall_3k: 0.8034 - lcm_recall_5k: 0.8941 - lcm_f1_1k: 0.5937 - lcm_f1_2k: 0.6433 - lcm_f1_3k: 0.5959 - lcm_f1_5k: 0.4794 - lcm_accuracy_1k: 0.7611 - lcm_accuracy_2k: 0.8936 - lcm_accuracy_3k: 0.9387 - lcm_accuracy_5k: 0.9720 - lcm_hamming_loss_k: 0.0032
Epoch 00017: val_loss improved from 0.26511 to 0.26420, saving model to logs/tbpcqi-labs-0604-122216/model/checkpoint_labs.h5
27/27 [==============================] - 12s 433ms/step - loss: 0.1940 - lcm_precision_1k: 0.7611 - lcm_precision_2k: 0.5945 - lcm_precision_3k: 0.4736 - lcm_precision_5k: 0.3275 - lcm_recall_1k: 0.4868 - lcm_recall_2k: 0.7011 - lcm_recall_3k: 0.8034 - lcm_recall_5k: 0.8941 - lcm_f1_1k: 0.5937 - lcm_f1_2k: 0.6433 - lcm_f1_3k: 0.5959 - lcm_f1_5k: 0.4794 - lcm_accuracy_1k: 0.7611 - lcm_accuracy_2k: 0.8936 - lcm_accuracy_3k: 0.9387 - lcm_accuracy_5k: 0.9720 - lcm_hamming_loss_k: 0.0032 - val_loss: 0.2642 - val_lcm_precision_1k: 0.6251 - val_lcm_precision_2k: 0.4869 - val_lcm_precision_3k: 0.3915 - val_lcm_precision_5k: 0.2785 - val_lcm_recall_1k: 0.3893 - val_lcm_recall_2k: 0.5688 - val_lcm_recall_3k: 0.6669 - val_lcm_recall_5k: 0.7640 - val_lcm_f1_1k: 0.4795 - val_lcm_f1_2k: 0.5245 - val_lcm_f1_3k: 0.4931 - val_lcm_f1_5k: 0.4080 - val_lcm_accuracy_1k: 0.6251 - val_lcm_accuracy_2k: 0.7645 - val_lcm_accuracy_3k: 0.8335 - val_lcm_accuracy_5k: 0.8938 - val_lcm_hamming_loss_k: 0.0039
Epoch 18/150
27/27 [==============================] - ETA: 0s - loss: 0.1894 - lcm_precision_1k: 0.7750 - lcm_precision_2k: 0.6053 - lcm_precision_3k: 0.4783 - lcm_precision_5k: 0.3307 - lcm_recall_1k: 0.4988 - lcm_recall_2k: 0.7136 - lcm_recall_3k: 0.8120 - lcm_recall_5k: 0.9019 - lcm_f1_1k: 0.6069 - lcm_f1_2k: 0.6549 - lcm_f1_3k: 0.6019 - lcm_f1_5k: 0.4839 - lcm_accuracy_1k: 0.7750 - lcm_accuracy_2k: 0.9025 - lcm_accuracy_3k: 0.9449 - lcm_accuracy_5k: 0.9742 - lcm_hamming_loss_k: 0.0032
Epoch 00018: val_loss did not improve from 0.26420
27/27 [==============================] - 10s 391ms/step - loss: 0.1894 - lcm_precision_1k: 0.7750 - lcm_precision_2k: 0.6053 - lcm_precision_3k: 0.4783 - lcm_precision_5k: 0.3307 - lcm_recall_1k: 0.4988 - lcm_recall_2k: 0.7136 - lcm_recall_3k: 0.8120 - lcm_recall_5k: 0.9019 - lcm_f1_1k: 0.6069 - lcm_f1_2k: 0.6549 - lcm_f1_3k: 0.6019 - lcm_f1_5k: 0.4839 - lcm_accuracy_1k: 0.7750 - lcm_accuracy_2k: 0.9025 - lcm_accuracy_3k: 0.9449 - lcm_accuracy_5k: 0.9742 - lcm_hamming_loss_k: 0.0032 - val_loss: 0.2695 - val_lcm_precision_1k: 0.6265 - val_lcm_precision_2k: 0.4888 - val_lcm_precision_3k: 0.3937 - val_lcm_precision_5k: 0.2794 - val_lcm_recall_1k: 0.3897 - val_lcm_recall_2k: 0.5697 - val_lcm_recall_3k: 0.6688 - val_lcm_recall_5k: 0.7648 - val_lcm_f1_1k: 0.4803 - val_lcm_f1_2k: 0.5259 - val_lcm_f1_3k: 0.4954 - val_lcm_f1_5k: 0.4091 - val_lcm_accuracy_1k: 0.6265 - val_lcm_accuracy_2k: 0.7675 - val_lcm_accuracy_3k: 0.8372 - val_lcm_accuracy_5k: 0.8957 - val_lcm_hamming_loss_k: 0.0039
Epoch 19/150
27/27 [==============================] - ETA: 0s - loss: 0.1833 - lcm_precision_1k: 0.7848 - lcm_precision_2k: 0.6127 - lcm_precision_3k: 0.4870 - lcm_precision_5k: 0.3348 - lcm_recall_1k: 0.5057 - lcm_recall_2k: 0.7224 - lcm_recall_3k: 0.8253 - lcm_recall_5k: 0.9109 - lcm_f1_1k: 0.6150 - lcm_f1_2k: 0.6630 - lcm_f1_3k: 0.6125 - lcm_f1_5k: 0.4896 - lcm_accuracy_1k: 0.7848 - lcm_accuracy_2k: 0.9092 - lcm_accuracy_3k: 0.9521 - lcm_accuracy_5k: 0.9786 - lcm_hamming_loss_k: 0.0031
Epoch 00019: val_loss did not improve from 0.26420
27/27 [==============================] - 11s 392ms/step - loss: 0.1833 - lcm_precision_1k: 0.7848 - lcm_precision_2k: 0.6127 - lcm_precision_3k: 0.4870 - lcm_precision_5k: 0.3348 - lcm_recall_1k: 0.5057 - lcm_recall_2k: 0.7224 - lcm_recall_3k: 0.8253 - lcm_recall_5k: 0.9109 - lcm_f1_1k: 0.6150 - lcm_f1_2k: 0.6630 - lcm_f1_3k: 0.6125 - lcm_f1_5k: 0.4896 - lcm_accuracy_1k: 0.7848 - lcm_accuracy_2k: 0.9092 - lcm_accuracy_3k: 0.9521 - lcm_accuracy_5k: 0.9786 - lcm_hamming_loss_k: 0.0031 - val_loss: 0.2683 - val_lcm_precision_1k: 0.6216 - val_lcm_precision_2k: 0.4896 - val_lcm_precision_3k: 0.3947 - val_lcm_precision_5k: 0.2808 - val_lcm_recall_1k: 0.3877 - val_lcm_recall_2k: 0.5705 - val_lcm_recall_3k: 0.6701 - val_lcm_recall_5k: 0.7689 - val_lcm_f1_1k: 0.4773 - val_lcm_f1_2k: 0.5267 - val_lcm_f1_3k: 0.4966 - val_lcm_f1_5k: 0.4112 - val_lcm_accuracy_1k: 0.6216 - val_lcm_accuracy_2k: 0.7688 - val_lcm_accuracy_3k: 0.8380 - val_lcm_accuracy_5k: 0.8990 - val_lcm_hamming_loss_k: 0.0039
Epoch 00019: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.2112 - lcm_precision_1k: 0.7295 - lcm_precision_2k: 0.5633 - lcm_precision_3k: 0.4492 - lcm_precision_5k: 0.3116 - lcm_recall_1k: 0.4713 - lcm_recall_2k: 0.6684 - lcm_recall_3k: 0.7691 - lcm_recall_5k: 0.8578 - lcm_f1_1k: 0.5714 - lcm_f1_2k: 0.6103 - lcm_f1_3k: 0.5661 - lcm_f1_5k: 0.4564 - lcm_accuracy_1k: 0.7295 - lcm_accuracy_2k: 0.8574 - lcm_accuracy_3k: 0.9098 - lcm_accuracy_5k: 0.9471 - lcm_hamming_loss_k: 0.0034
Best model result:  [0.21121945977210999, 0.7295375466346741, 0.5633057951927185, 0.4492250680923462, 0.31158632040023804, 0.47133633494377136, 0.6684495210647583, 0.7691040635108948, 0.8578312397003174, 0.5714375972747803, 0.6102519035339355, 0.5661414265632629, 0.4564063251018524, 0.7295375466346741, 0.8574018478393555, 0.9097753763198853, 0.9471006393432617, 0.003368639387190342]
13499
3374
5625
Model: "model_6"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 permute_9 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_15 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_9[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_15[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_16 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_6 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_16[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_3 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_6[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_9 (Dense)                (None, 1024)         2098176     ['tf.concat_3[0][0]']            
                                                                                                  
 dense_10 (Dense)               (None, 15)           15375       ['dense_9[0][0]']                
                                                                                                  
 tf.nn.softmax_3 (TFOpLambda)   (None, 15)           0           ['dense_10[0][0]']               
                                                                                                  
 tf.expand_dims_6 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_3[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_6[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_10 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_17 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_10[0][0]']             
                                                                                                  
 dense_11 (Dense)               (None, 15, 150)      22650       ['lambda_17[0][0]']              
                                                                                                  
 tf.math.reduce_mean_7 (TFOpLam  (None, 150)         0           ['dense_11[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_7 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_7[0][0]']  
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_3 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_7[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_10[0][
                                                                 0]']                             
                                                                                                  
 permute_11 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_3[0][0]']     
                                                                                                  
 lambda_18 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_11[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_18[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_19 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_3[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_19[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_7"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_9 (Sl  (None, 15, 300)     0           ['label_emb[0][0]']              
 icingOpLambda)                                                                                   
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_9[0][0
                                                                 ]']                              
                                                                                                  
 permute_9 (Permute)            (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_15 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_9[0][0]']              
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_15[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_16 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_6 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_16[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_3 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_6[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_9 (Dense)                (None, 1024)         2098176     ['tf.concat_3[0][0]']            
                                                                                                  
 dense_10 (Dense)               (None, 15)           15375       ['dense_9[0][0]']                
                                                                                                  
 tf.nn.softmax_3 (TFOpLambda)   (None, 15)           0           ['dense_10[0][0]']               
                                                                                                  
 tf.expand_dims_6 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_3[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_6[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_10 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_17 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_10[0][0]']             
                                                                                                  
 dense_11 (Dense)               (None, 15, 150)      22650       ['lambda_17[0][0]']              
                                                                                                  
 tf.math.reduce_mean_7 (TFOpLam  (None, 150)         0           ['dense_11[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_7 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_7[0][0]']  
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_3 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_7[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_10[0][
                                                                 0]']                             
                                                                                                  
 permute_11 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_3[0][0]']     
                                                                                                  
 lambda_18 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_11[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_18[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_19 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_3[0][0]']     
                                                                                                  
 tf.__operators__.getitem_11 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_19[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_11[0][
                                                                 0]']                             
                                                                                                  
 dot_3 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_3[0][0]']                  
                                                                                                  
 concatenate_3 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.4919 - lcm_precision_1k: 0.2337 - lcm_precision_2k: 0.2008 - lcm_precision_3k: 0.1729 - lcm_precision_5k: 0.1385 - lcm_recall_1k: 0.1263 - lcm_recall_2k: 0.2208 - lcm_recall_3k: 0.2823 - lcm_recall_5k: 0.3712 - lcm_f1_1k: 0.1638 - lcm_f1_2k: 0.2103 - lcm_f1_3k: 0.2144 - lcm_f1_5k: 0.2017 - lcm_accuracy_1k: 0.2337 - lcm_accuracy_2k: 0.3539 - lcm_accuracy_3k: 0.4267 - lcm_accuracy_5k: 0.5145 - lcm_hamming_loss_k: 0.0057 ETA: 3s - loss: 0.5242 - lcm_precision_1k: 0.1870 - lcm_precision_2k: 0.1621 - lcm_precision_3k: 0.1407 - lcm_precision_5k: 0.1139 - lcm_recall_1k: 0.0963 - lcm_recall_2k: 0.1733 - lcm_recall_3k: 0.2253 - lcm_recall_5k: 0.3006 - lcm_f1_1k: 0.1270 - lcm_f1_2k: 0.1674 - lcm_f1_3k: 0.1732 - lcm_f1_5k: 0.1652 - lcm_accuracy_1k: 0.1870 - lcm_accuracy_2k: 0.2899 - lcm_accuracy_3k: 0.3560 - lcm_accuracy_5k: 0.4355 - lcm_ha
Epoch 00001: val_loss improved from inf to 0.40850, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 13s 415ms/step - loss: 0.4919 - lcm_precision_1k: 0.2337 - lcm_precision_2k: 0.2008 - lcm_precision_3k: 0.1729 - lcm_precision_5k: 0.1385 - lcm_recall_1k: 0.1263 - lcm_recall_2k: 0.2208 - lcm_recall_3k: 0.2823 - lcm_recall_5k: 0.3712 - lcm_f1_1k: 0.1638 - lcm_f1_2k: 0.2103 - lcm_f1_3k: 0.2144 - lcm_f1_5k: 0.2017 - lcm_accuracy_1k: 0.2337 - lcm_accuracy_2k: 0.3539 - lcm_accuracy_3k: 0.4267 - lcm_accuracy_5k: 0.5145 - lcm_hamming_loss_k: 0.0057 - val_loss: 0.4085 - val_lcm_precision_1k: 0.3411 - val_lcm_precision_2k: 0.2702 - val_lcm_precision_3k: 0.2352 - val_lcm_precision_5k: 0.1897 - val_lcm_recall_1k: 0.1995 - val_lcm_recall_2k: 0.3041 - val_lcm_recall_3k: 0.3896 - val_lcm_recall_5k: 0.5113 - val_lcm_f1_1k: 0.2516 - val_lcm_f1_2k: 0.2859 - val_lcm_f1_3k: 0.2931 - val_lcm_f1_5k: 0.2766 - val_lcm_accuracy_1k: 0.3411 - val_lcm_accuracy_2k: 0.4729 - val_lcm_accuracy_3k: 0.5570 - val_lcm_accuracy_5k: 0.6682 - val_lcm_hamming_loss_k: 0.0052
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.3604 - lcm_precision_1k: 0.4178 - lcm_precision_2k: 0.3364 - lcm_precision_3k: 0.2828 - lcm_precision_5k: 0.2174 - lcm_recall_1k: 0.2494 - lcm_recall_2k: 0.3861 - lcm_recall_3k: 0.4772 - lcm_recall_5k: 0.5939 - lcm_f1_1k: 0.3123 - lcm_f1_2k: 0.3595 - lcm_f1_3k: 0.3550 - lcm_f1_5k: 0.3183 - lcm_accuracy_1k: 0.4178 - lcm_accuracy_2k: 0.5697 - lcm_accuracy_3k: 0.6544 - lcm_accuracy_5k: 0.7442 - lcm_hamming_loss_k: 0.0049
Epoch 00002: val_loss improved from 0.40850 to 0.34229, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 434ms/step - loss: 0.3604 - lcm_precision_1k: 0.4178 - lcm_precision_2k: 0.3364 - lcm_precision_3k: 0.2828 - lcm_precision_5k: 0.2174 - lcm_recall_1k: 0.2494 - lcm_recall_2k: 0.3861 - lcm_recall_3k: 0.4772 - lcm_recall_5k: 0.5939 - lcm_f1_1k: 0.3123 - lcm_f1_2k: 0.3595 - lcm_f1_3k: 0.3550 - lcm_f1_5k: 0.3183 - lcm_accuracy_1k: 0.4178 - lcm_accuracy_2k: 0.5697 - lcm_accuracy_3k: 0.6544 - lcm_accuracy_5k: 0.7442 - lcm_hamming_loss_k: 0.0049 - val_loss: 0.3423 - val_lcm_precision_1k: 0.4499 - val_lcm_precision_2k: 0.3686 - val_lcm_precision_3k: 0.3078 - val_lcm_precision_5k: 0.2277 - val_lcm_recall_1k: 0.2760 - val_lcm_recall_2k: 0.4287 - val_lcm_recall_3k: 0.5243 - val_lcm_recall_5k: 0.6229 - val_lcm_f1_1k: 0.3419 - val_lcm_f1_2k: 0.3962 - val_lcm_f1_3k: 0.3877 - val_lcm_f1_5k: 0.3334 - val_lcm_accuracy_1k: 0.4499 - val_lcm_accuracy_2k: 0.6123 - val_lcm_accuracy_3k: 0.6992 - val_lcm_accuracy_5k: 0.7774 - val_lcm_hamming_loss_k: 0.0047
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.3191 - lcm_precision_1k: 0.4937 - lcm_precision_2k: 0.3979 - lcm_precision_3k: 0.3286 - lcm_precision_5k: 0.2429 - lcm_recall_1k: 0.3009 - lcm_recall_2k: 0.4617 - lcm_recall_3k: 0.5582 - lcm_recall_5k: 0.6694 - lcm_f1_1k: 0.3739 - lcm_f1_2k: 0.4273 - lcm_f1_3k: 0.4137 - lcm_f1_5k: 0.3564 - lcm_accuracy_1k: 0.4937 - lcm_accuracy_2k: 0.6523 - lcm_accuracy_3k: 0.7321 - lcm_accuracy_5k: 0.8131 - lcm_hamming_loss_k: 0.0045
Epoch 00003: val_loss improved from 0.34229 to 0.32034, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3191 - lcm_precision_1k: 0.4937 - lcm_precision_2k: 0.3979 - lcm_precision_3k: 0.3286 - lcm_precision_5k: 0.2429 - lcm_recall_1k: 0.3009 - lcm_recall_2k: 0.4617 - lcm_recall_3k: 0.5582 - lcm_recall_5k: 0.6694 - lcm_f1_1k: 0.3739 - lcm_f1_2k: 0.4273 - lcm_f1_3k: 0.4137 - lcm_f1_5k: 0.3564 - lcm_accuracy_1k: 0.4937 - lcm_accuracy_2k: 0.6523 - lcm_accuracy_3k: 0.7321 - lcm_accuracy_5k: 0.8131 - lcm_hamming_loss_k: 0.0045 - val_loss: 0.3203 - val_lcm_precision_1k: 0.5048 - val_lcm_precision_2k: 0.4013 - val_lcm_precision_3k: 0.3289 - val_lcm_precision_5k: 0.2418 - val_lcm_recall_1k: 0.3083 - val_lcm_recall_2k: 0.4647 - val_lcm_recall_3k: 0.5564 - val_lcm_recall_5k: 0.6609 - val_lcm_f1_1k: 0.3826 - val_lcm_f1_2k: 0.4304 - val_lcm_f1_3k: 0.4132 - val_lcm_f1_5k: 0.3539 - val_lcm_accuracy_1k: 0.5048 - val_lcm_accuracy_2k: 0.6598 - val_lcm_accuracy_3k: 0.7393 - val_lcm_accuracy_5k: 0.8100 - val_lcm_hamming_loss_k: 0.0044
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.2977 - lcm_precision_1k: 0.5396 - lcm_precision_2k: 0.4320 - lcm_precision_3k: 0.3556 - lcm_precision_5k: 0.2581 - lcm_recall_1k: 0.3322 - lcm_recall_2k: 0.5033 - lcm_recall_3k: 0.6068 - lcm_recall_5k: 0.7127 - lcm_f1_1k: 0.4112 - lcm_f1_2k: 0.4648 - lcm_f1_3k: 0.4484 - lcm_f1_5k: 0.3790 - lcm_accuracy_1k: 0.5396 - lcm_accuracy_2k: 0.6974 - lcm_accuracy_3k: 0.7790 - lcm_accuracy_5k: 0.8520 - lcm_hamming_loss_k: 0.0043
Epoch 00004: val_loss improved from 0.32034 to 0.31079, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2977 - lcm_precision_1k: 0.5396 - lcm_precision_2k: 0.4320 - lcm_precision_3k: 0.3556 - lcm_precision_5k: 0.2581 - lcm_recall_1k: 0.3322 - lcm_recall_2k: 0.5033 - lcm_recall_3k: 0.6068 - lcm_recall_5k: 0.7127 - lcm_f1_1k: 0.4112 - lcm_f1_2k: 0.4648 - lcm_f1_3k: 0.4484 - lcm_f1_5k: 0.3790 - lcm_accuracy_1k: 0.5396 - lcm_accuracy_2k: 0.6974 - lcm_accuracy_3k: 0.7790 - lcm_accuracy_5k: 0.8520 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3108 - val_lcm_precision_1k: 0.5101 - val_lcm_precision_2k: 0.4136 - val_lcm_precision_3k: 0.3396 - val_lcm_precision_5k: 0.2485 - val_lcm_recall_1k: 0.3131 - val_lcm_recall_2k: 0.4810 - val_lcm_recall_3k: 0.5760 - val_lcm_recall_5k: 0.6809 - val_lcm_f1_1k: 0.3878 - val_lcm_f1_2k: 0.4445 - val_lcm_f1_3k: 0.4270 - val_lcm_f1_5k: 0.3639 - val_lcm_accuracy_1k: 0.5101 - val_lcm_accuracy_2k: 0.6685 - val_lcm_accuracy_3k: 0.7525 - val_lcm_accuracy_5k: 0.8258 - val_lcm_hamming_loss_k: 0.0044
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.2845 - lcm_precision_1k: 0.5721 - lcm_precision_2k: 0.4546 - lcm_precision_3k: 0.3703 - lcm_precision_5k: 0.2683 - lcm_recall_1k: 0.3543 - lcm_recall_2k: 0.5319 - lcm_recall_3k: 0.6330 - lcm_recall_5k: 0.7417 - lcm_f1_1k: 0.4375 - lcm_f1_2k: 0.4901 - lcm_f1_3k: 0.4672 - lcm_f1_5k: 0.3940 - lcm_accuracy_1k: 0.5721 - lcm_accuracy_2k: 0.7269 - lcm_accuracy_3k: 0.8044 - lcm_accuracy_5k: 0.8738 - lcm_hamming_loss_k: 0.0041
Epoch 00005: val_loss improved from 0.31079 to 0.29930, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 433ms/step - loss: 0.2845 - lcm_precision_1k: 0.5721 - lcm_precision_2k: 0.4546 - lcm_precision_3k: 0.3703 - lcm_precision_5k: 0.2683 - lcm_recall_1k: 0.3543 - lcm_recall_2k: 0.5319 - lcm_recall_3k: 0.6330 - lcm_recall_5k: 0.7417 - lcm_f1_1k: 0.4375 - lcm_f1_2k: 0.4901 - lcm_f1_3k: 0.4672 - lcm_f1_5k: 0.3940 - lcm_accuracy_1k: 0.5721 - lcm_accuracy_2k: 0.7269 - lcm_accuracy_3k: 0.8044 - lcm_accuracy_5k: 0.8738 - lcm_hamming_loss_k: 0.0041 - val_loss: 0.2993 - val_lcm_precision_1k: 0.5339 - val_lcm_precision_2k: 0.4258 - val_lcm_precision_3k: 0.3496 - val_lcm_precision_5k: 0.2542 - val_lcm_recall_1k: 0.3302 - val_lcm_recall_2k: 0.4980 - val_lcm_recall_3k: 0.5948 - val_lcm_recall_5k: 0.6958 - val_lcm_f1_1k: 0.4078 - val_lcm_f1_2k: 0.4589 - val_lcm_f1_3k: 0.4402 - val_lcm_f1_5k: 0.3723 - val_lcm_accuracy_1k: 0.5339 - val_lcm_accuracy_2k: 0.6928 - val_lcm_accuracy_3k: 0.7767 - val_lcm_accuracy_5k: 0.8382 - val_lcm_hamming_loss_k: 0.0043
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.2723 - lcm_precision_1k: 0.5919 - lcm_precision_2k: 0.4721 - lcm_precision_3k: 0.3836 - lcm_precision_5k: 0.2762 - lcm_recall_1k: 0.3676 - lcm_recall_2k: 0.5544 - lcm_recall_3k: 0.6559 - lcm_recall_5k: 0.7621 - lcm_f1_1k: 0.4534 - lcm_f1_2k: 0.5099 - lcm_f1_3k: 0.4840 - lcm_f1_5k: 0.4054 - lcm_accuracy_1k: 0.5919 - lcm_accuracy_2k: 0.7525 - lcm_accuracy_3k: 0.8245 - lcm_accuracy_5k: 0.8900 - lcm_hamming_loss_k: 0.0040
Epoch 00006: val_loss improved from 0.29930 to 0.29351, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2723 - lcm_precision_1k: 0.5919 - lcm_precision_2k: 0.4721 - lcm_precision_3k: 0.3836 - lcm_precision_5k: 0.2762 - lcm_recall_1k: 0.3676 - lcm_recall_2k: 0.5544 - lcm_recall_3k: 0.6559 - lcm_recall_5k: 0.7621 - lcm_f1_1k: 0.4534 - lcm_f1_2k: 0.5099 - lcm_f1_3k: 0.4840 - lcm_f1_5k: 0.4054 - lcm_accuracy_1k: 0.5919 - lcm_accuracy_2k: 0.7525 - lcm_accuracy_3k: 0.8245 - lcm_accuracy_5k: 0.8900 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2935 - val_lcm_precision_1k: 0.5600 - val_lcm_precision_2k: 0.4389 - val_lcm_precision_3k: 0.3566 - val_lcm_precision_5k: 0.2567 - val_lcm_recall_1k: 0.3481 - val_lcm_recall_2k: 0.5111 - val_lcm_recall_3k: 0.6065 - val_lcm_recall_5k: 0.7047 - val_lcm_f1_1k: 0.4291 - val_lcm_f1_2k: 0.4720 - val_lcm_f1_3k: 0.4489 - val_lcm_f1_5k: 0.3762 - val_lcm_accuracy_1k: 0.5600 - val_lcm_accuracy_2k: 0.7116 - val_lcm_accuracy_3k: 0.7830 - val_lcm_accuracy_5k: 0.8465 - val_lcm_hamming_loss_k: 0.0042
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.2632 - lcm_precision_1k: 0.6169 - lcm_precision_2k: 0.4861 - lcm_precision_3k: 0.3946 - lcm_precision_5k: 0.2825 - lcm_recall_1k: 0.3869 - lcm_recall_2k: 0.5740 - lcm_recall_3k: 0.6744 - lcm_recall_5k: 0.7797 - lcm_f1_1k: 0.4755 - lcm_f1_2k: 0.5263 - lcm_f1_3k: 0.4978 - lcm_f1_5k: 0.4147 - lcm_accuracy_1k: 0.6169 - lcm_accuracy_2k: 0.7735 - lcm_accuracy_3k: 0.8424 - lcm_accuracy_5k: 0.9042 - lcm_hamming_loss_k: 0.0039
Epoch 00007: val_loss improved from 0.29351 to 0.28747, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 439ms/step - loss: 0.2632 - lcm_precision_1k: 0.6169 - lcm_precision_2k: 0.4861 - lcm_precision_3k: 0.3946 - lcm_precision_5k: 0.2825 - lcm_recall_1k: 0.3869 - lcm_recall_2k: 0.5740 - lcm_recall_3k: 0.6744 - lcm_recall_5k: 0.7797 - lcm_f1_1k: 0.4755 - lcm_f1_2k: 0.5263 - lcm_f1_3k: 0.4978 - lcm_f1_5k: 0.4147 - lcm_accuracy_1k: 0.6169 - lcm_accuracy_2k: 0.7735 - lcm_accuracy_3k: 0.8424 - lcm_accuracy_5k: 0.9042 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2875 - val_lcm_precision_1k: 0.5642 - val_lcm_precision_2k: 0.4431 - val_lcm_precision_3k: 0.3620 - val_lcm_precision_5k: 0.2616 - val_lcm_recall_1k: 0.3517 - val_lcm_recall_2k: 0.5213 - val_lcm_recall_3k: 0.6190 - val_lcm_recall_5k: 0.7205 - val_lcm_f1_1k: 0.4331 - val_lcm_f1_2k: 0.4788 - val_lcm_f1_3k: 0.4566 - val_lcm_f1_5k: 0.3837 - val_lcm_accuracy_1k: 0.5642 - val_lcm_accuracy_2k: 0.7192 - val_lcm_accuracy_3k: 0.7950 - val_lcm_accuracy_5k: 0.8585 - val_lcm_hamming_loss_k: 0.0042
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.2553 - lcm_precision_1k: 0.6363 - lcm_precision_2k: 0.5022 - lcm_precision_3k: 0.4046 - lcm_precision_5k: 0.2878 - lcm_recall_1k: 0.4003 - lcm_recall_2k: 0.5913 - lcm_recall_3k: 0.6922 - lcm_recall_5k: 0.7938 - lcm_f1_1k: 0.4914 - lcm_f1_2k: 0.5430 - lcm_f1_3k: 0.5106 - lcm_f1_5k: 0.4225 - lcm_accuracy_1k: 0.6363 - lcm_accuracy_2k: 0.7908 - lcm_accuracy_3k: 0.8570 - lcm_accuracy_5k: 0.9145 - lcm_hamming_loss_k: 0.0038
Epoch 00008: val_loss did not improve from 0.28747
27/27 [==============================] - 10s 390ms/step - loss: 0.2553 - lcm_precision_1k: 0.6363 - lcm_precision_2k: 0.5022 - lcm_precision_3k: 0.4046 - lcm_precision_5k: 0.2878 - lcm_recall_1k: 0.4003 - lcm_recall_2k: 0.5913 - lcm_recall_3k: 0.6922 - lcm_recall_5k: 0.7938 - lcm_f1_1k: 0.4914 - lcm_f1_2k: 0.5430 - lcm_f1_3k: 0.5106 - lcm_f1_5k: 0.4225 - lcm_accuracy_1k: 0.6363 - lcm_accuracy_2k: 0.7908 - lcm_accuracy_3k: 0.8570 - lcm_accuracy_5k: 0.9145 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2881 - val_lcm_precision_1k: 0.5637 - val_lcm_precision_2k: 0.4414 - val_lcm_precision_3k: 0.3639 - val_lcm_precision_5k: 0.2638 - val_lcm_recall_1k: 0.3506 - val_lcm_recall_2k: 0.5155 - val_lcm_recall_3k: 0.6181 - val_lcm_recall_5k: 0.7245 - val_lcm_f1_1k: 0.4321 - val_lcm_f1_2k: 0.4753 - val_lcm_f1_3k: 0.4579 - val_lcm_f1_5k: 0.3866 - val_lcm_accuracy_1k: 0.5637 - val_lcm_accuracy_2k: 0.7094 - val_lcm_accuracy_3k: 0.7941 - val_lcm_accuracy_5k: 0.8624 - val_lcm_hamming_loss_k: 0.0042
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.2487 - lcm_precision_1k: 0.6444 - lcm_precision_2k: 0.5082 - lcm_precision_3k: 0.4113 - lcm_precision_5k: 0.2924 - lcm_recall_1k: 0.4054 - lcm_recall_2k: 0.5995 - lcm_recall_3k: 0.7026 - lcm_recall_5k: 0.8045 - lcm_f1_1k: 0.4976 - lcm_f1_2k: 0.5500 - lcm_f1_3k: 0.5188 - lcm_f1_5k: 0.4289 - lcm_accuracy_1k: 0.6444 - lcm_accuracy_2k: 0.8007 - lcm_accuracy_3k: 0.8682 - lcm_accuracy_5k: 0.9212 - lcm_hamming_loss_k: 0.0038
Epoch 00009: val_loss improved from 0.28747 to 0.28088, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2487 - lcm_precision_1k: 0.6444 - lcm_precision_2k: 0.5082 - lcm_precision_3k: 0.4113 - lcm_precision_5k: 0.2924 - lcm_recall_1k: 0.4054 - lcm_recall_2k: 0.5995 - lcm_recall_3k: 0.7026 - lcm_recall_5k: 0.8045 - lcm_f1_1k: 0.4976 - lcm_f1_2k: 0.5500 - lcm_f1_3k: 0.5188 - lcm_f1_5k: 0.4289 - lcm_accuracy_1k: 0.6444 - lcm_accuracy_2k: 0.8007 - lcm_accuracy_3k: 0.8682 - lcm_accuracy_5k: 0.9212 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2809 - val_lcm_precision_1k: 0.5901 - val_lcm_precision_2k: 0.4621 - val_lcm_precision_3k: 0.3699 - val_lcm_precision_5k: 0.2663 - val_lcm_recall_1k: 0.3680 - val_lcm_recall_2k: 0.5399 - val_lcm_recall_3k: 0.6287 - val_lcm_recall_5k: 0.7324 - val_lcm_f1_1k: 0.4530 - val_lcm_f1_2k: 0.4977 - val_lcm_f1_3k: 0.4655 - val_lcm_f1_5k: 0.3905 - val_lcm_accuracy_1k: 0.5901 - val_lcm_accuracy_2k: 0.7399 - val_lcm_accuracy_3k: 0.8054 - val_lcm_accuracy_5k: 0.8691 - val_lcm_hamming_loss_k: 0.0040
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.2415 - lcm_precision_1k: 0.6661 - lcm_precision_2k: 0.5222 - lcm_precision_3k: 0.4204 - lcm_precision_5k: 0.2978 - lcm_recall_1k: 0.4200 - lcm_recall_2k: 0.6158 - lcm_recall_3k: 0.7173 - lcm_recall_5k: 0.8203 - lcm_f1_1k: 0.5150 - lcm_f1_2k: 0.5650 - lcm_f1_3k: 0.5300 - lcm_f1_5k: 0.4369 - lcm_accuracy_1k: 0.6661 - lcm_accuracy_2k: 0.8159 - lcm_accuracy_3k: 0.8782 - lcm_accuracy_5k: 0.9322 - lcm_hamming_loss_k: 0.0037
Epoch 00010: val_loss improved from 0.28088 to 0.27847, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2415 - lcm_precision_1k: 0.6661 - lcm_precision_2k: 0.5222 - lcm_precision_3k: 0.4204 - lcm_precision_5k: 0.2978 - lcm_recall_1k: 0.4200 - lcm_recall_2k: 0.6158 - lcm_recall_3k: 0.7173 - lcm_recall_5k: 0.8203 - lcm_f1_1k: 0.5150 - lcm_f1_2k: 0.5650 - lcm_f1_3k: 0.5300 - lcm_f1_5k: 0.4369 - lcm_accuracy_1k: 0.6661 - lcm_accuracy_2k: 0.8159 - lcm_accuracy_3k: 0.8782 - lcm_accuracy_5k: 0.9322 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2785 - val_lcm_precision_1k: 0.5881 - val_lcm_precision_2k: 0.4579 - val_lcm_precision_3k: 0.3707 - val_lcm_precision_5k: 0.2657 - val_lcm_recall_1k: 0.3663 - val_lcm_recall_2k: 0.5357 - val_lcm_recall_3k: 0.6319 - val_lcm_recall_5k: 0.7305 - val_lcm_f1_1k: 0.4512 - val_lcm_f1_2k: 0.4936 - val_lcm_f1_3k: 0.4670 - val_lcm_f1_5k: 0.3895 - val_lcm_accuracy_1k: 0.5881 - val_lcm_accuracy_2k: 0.7335 - val_lcm_accuracy_3k: 0.8073 - val_lcm_accuracy_5k: 0.8651 - val_lcm_hamming_loss_k: 0.0041
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.2348 - lcm_precision_1k: 0.6782 - lcm_precision_2k: 0.5321 - lcm_precision_3k: 0.4260 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4286 - lcm_recall_2k: 0.6277 - lcm_recall_3k: 0.7277 - lcm_recall_5k: 0.8310 - lcm_f1_1k: 0.5252 - lcm_f1_2k: 0.5758 - lcm_f1_3k: 0.5374 - lcm_f1_5k: 0.4428 - lcm_accuracy_1k: 0.6782 - lcm_accuracy_2k: 0.8278 - lcm_accuracy_3k: 0.8865 - lcm_accuracy_5k: 0.9375 - lcm_hamming_loss_k: 0.0036
Epoch 00011: val_loss improved from 0.27847 to 0.27316, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2348 - lcm_precision_1k: 0.6782 - lcm_precision_2k: 0.5321 - lcm_precision_3k: 0.4260 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4286 - lcm_recall_2k: 0.6277 - lcm_recall_3k: 0.7277 - lcm_recall_5k: 0.8310 - lcm_f1_1k: 0.5252 - lcm_f1_2k: 0.5758 - lcm_f1_3k: 0.5374 - lcm_f1_5k: 0.4428 - lcm_accuracy_1k: 0.6782 - lcm_accuracy_2k: 0.8278 - lcm_accuracy_3k: 0.8865 - lcm_accuracy_5k: 0.9375 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2732 - val_lcm_precision_1k: 0.6001 - val_lcm_precision_2k: 0.4663 - val_lcm_precision_3k: 0.3786 - val_lcm_precision_5k: 0.2713 - val_lcm_recall_1k: 0.3760 - val_lcm_recall_2k: 0.5440 - val_lcm_recall_3k: 0.6433 - val_lcm_recall_5k: 0.7443 - val_lcm_f1_1k: 0.4621 - val_lcm_f1_2k: 0.5020 - val_lcm_f1_3k: 0.4765 - val_lcm_f1_5k: 0.3975 - val_lcm_accuracy_1k: 0.6001 - val_lcm_accuracy_2k: 0.7410 - val_lcm_accuracy_3k: 0.8156 - val_lcm_accuracy_5k: 0.8754 - val_lcm_hamming_loss_k: 0.0040
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.2279 - lcm_precision_1k: 0.6908 - lcm_precision_2k: 0.5390 - lcm_precision_3k: 0.4340 - lcm_precision_5k: 0.3064 - lcm_recall_1k: 0.4376 - lcm_recall_2k: 0.6361 - lcm_recall_3k: 0.7402 - lcm_recall_5k: 0.8413 - lcm_f1_1k: 0.5357 - lcm_f1_2k: 0.5834 - lcm_f1_3k: 0.5472 - lcm_f1_5k: 0.4492 - lcm_accuracy_1k: 0.6908 - lcm_accuracy_2k: 0.8356 - lcm_accuracy_3k: 0.8962 - lcm_accuracy_5k: 0.9447 - lcm_hamming_loss_k: 0.0036 ETA: 4s - loss: 0.2263 - lcm_precision_1k: 0.6908 - lcm_precision_2k: 0.5412 - lcm_precision_3k: 0.4368 - lcm_precision_5k: 0.3074 - lcm_recall_1k: 0.4375 - lcm_recall_2k: 0.6408 - lcm_recall_3k: 0.7469 - lcm_recall_5k: 0.8474 - lcm_f1_1k: 0.5357 - lcm_f1_2k: 0.5868 - lcm_f1_3k: 0.5512 - lcm_f1_5k: 0.4511 - lcm_accuracy_1k: 0.6908 - lcm_accuracy_2k: 0.8370 - lcm_accuracy_3k: 0.8998 - lcm_accuracy_5k: 0.9494 
Epoch 00012: val_loss improved from 0.27316 to 0.26960, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 438ms/step - loss: 0.2279 - lcm_precision_1k: 0.6908 - lcm_precision_2k: 0.5390 - lcm_precision_3k: 0.4340 - lcm_precision_5k: 0.3064 - lcm_recall_1k: 0.4376 - lcm_recall_2k: 0.6361 - lcm_recall_3k: 0.7402 - lcm_recall_5k: 0.8413 - lcm_f1_1k: 0.5357 - lcm_f1_2k: 0.5834 - lcm_f1_3k: 0.5472 - lcm_f1_5k: 0.4492 - lcm_accuracy_1k: 0.6908 - lcm_accuracy_2k: 0.8356 - lcm_accuracy_3k: 0.8962 - lcm_accuracy_5k: 0.9447 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2696 - val_lcm_precision_1k: 0.5997 - val_lcm_precision_2k: 0.4697 - val_lcm_precision_3k: 0.3807 - val_lcm_precision_5k: 0.2726 - val_lcm_recall_1k: 0.3748 - val_lcm_recall_2k: 0.5489 - val_lcm_recall_3k: 0.6461 - val_lcm_recall_5k: 0.7475 - val_lcm_f1_1k: 0.4611 - val_lcm_f1_2k: 0.5060 - val_lcm_f1_3k: 0.4788 - val_lcm_f1_5k: 0.3994 - val_lcm_accuracy_1k: 0.5997 - val_lcm_accuracy_2k: 0.7481 - val_lcm_accuracy_3k: 0.8183 - val_lcm_accuracy_5k: 0.8786 - val_lcm_hamming_loss_k: 0.0040
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.2215 - lcm_precision_1k: 0.7053 - lcm_precision_2k: 0.5518 - lcm_precision_3k: 0.4403 - lcm_precision_5k: 0.3098 - lcm_recall_1k: 0.4488 - lcm_recall_2k: 0.6518 - lcm_recall_3k: 0.7517 - lcm_recall_5k: 0.8506 - lcm_f1_1k: 0.5485 - lcm_f1_2k: 0.5976 - lcm_f1_3k: 0.5553 - lcm_f1_5k: 0.4541 - lcm_accuracy_1k: 0.7053 - lcm_accuracy_2k: 0.8510 - lcm_accuracy_3k: 0.9050 - lcm_accuracy_5k: 0.9506 - lcm_hamming_loss_k: 0.0035
Epoch 00013: val_loss improved from 0.26960 to 0.26656, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 437ms/step - loss: 0.2215 - lcm_precision_1k: 0.7053 - lcm_precision_2k: 0.5518 - lcm_precision_3k: 0.4403 - lcm_precision_5k: 0.3098 - lcm_recall_1k: 0.4488 - lcm_recall_2k: 0.6518 - lcm_recall_3k: 0.7517 - lcm_recall_5k: 0.8506 - lcm_f1_1k: 0.5485 - lcm_f1_2k: 0.5976 - lcm_f1_3k: 0.5553 - lcm_f1_5k: 0.4541 - lcm_accuracy_1k: 0.7053 - lcm_accuracy_2k: 0.8510 - lcm_accuracy_3k: 0.9050 - lcm_accuracy_5k: 0.9506 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2666 - val_lcm_precision_1k: 0.6091 - val_lcm_precision_2k: 0.4784 - val_lcm_precision_3k: 0.3848 - val_lcm_precision_5k: 0.2751 - val_lcm_recall_1k: 0.3817 - val_lcm_recall_2k: 0.5616 - val_lcm_recall_3k: 0.6569 - val_lcm_recall_5k: 0.7561 - val_lcm_f1_1k: 0.4691 - val_lcm_f1_2k: 0.5164 - val_lcm_f1_3k: 0.4851 - val_lcm_f1_5k: 0.4032 - val_lcm_accuracy_1k: 0.6091 - val_lcm_accuracy_2k: 0.7594 - val_lcm_accuracy_3k: 0.8295 - val_lcm_accuracy_5k: 0.8872 - val_lcm_hamming_loss_k: 0.0040
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.2152 - lcm_precision_1k: 0.7130 - lcm_precision_2k: 0.5592 - lcm_precision_3k: 0.4481 - lcm_precision_5k: 0.3149 - lcm_recall_1k: 0.4522 - lcm_recall_2k: 0.6600 - lcm_recall_3k: 0.7629 - lcm_recall_5k: 0.8623 - lcm_f1_1k: 0.5534 - lcm_f1_2k: 0.6054 - lcm_f1_3k: 0.5646 - lcm_f1_5k: 0.4613 - lcm_accuracy_1k: 0.7130 - lcm_accuracy_2k: 0.8583 - lcm_accuracy_3k: 0.9125 - lcm_accuracy_5k: 0.9564 - lcm_hamming_loss_k: 0.0035 ETA: 4s - loss: 0.2142 - lcm_precision_1k: 0.7180 - lcm_precision_2k: 0.5588 - lcm_precision_3k: 0.4487 - lcm_precision_5k: 0.3156 - lcm_recall_1k: 0.4575 - lcm_recall_2k: 0.6613 - lcm_recall_3k: 0.7642 - lcm_recall_5k: 0.8635 - lcm_f1_1k: 0.5589 - lcm_f1_2k: 0.6057 - lcm_f1_3k: 0.5654 - lcm_f1_5k: 0.4622 - lcm_accuracy_1k: 0.7180 - lcm_accuracy_2k: 0.8613 - lcm_accuracy_3k: 0.9144 - lcm_accuracy_5k: 0.9569 
Epoch 00014: val_loss did not improve from 0.26656
27/27 [==============================] - 10s 386ms/step - loss: 0.2152 - lcm_precision_1k: 0.7130 - lcm_precision_2k: 0.5592 - lcm_precision_3k: 0.4481 - lcm_precision_5k: 0.3149 - lcm_recall_1k: 0.4522 - lcm_recall_2k: 0.6600 - lcm_recall_3k: 0.7629 - lcm_recall_5k: 0.8623 - lcm_f1_1k: 0.5534 - lcm_f1_2k: 0.6054 - lcm_f1_3k: 0.5646 - lcm_f1_5k: 0.4613 - lcm_accuracy_1k: 0.7130 - lcm_accuracy_2k: 0.8583 - lcm_accuracy_3k: 0.9125 - lcm_accuracy_5k: 0.9564 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2666 - val_lcm_precision_1k: 0.6127 - val_lcm_precision_2k: 0.4803 - val_lcm_precision_3k: 0.3879 - val_lcm_precision_5k: 0.2751 - val_lcm_recall_1k: 0.3839 - val_lcm_recall_2k: 0.5634 - val_lcm_recall_3k: 0.6608 - val_lcm_recall_5k: 0.7555 - val_lcm_f1_1k: 0.4719 - val_lcm_f1_2k: 0.5183 - val_lcm_f1_3k: 0.4886 - val_lcm_f1_5k: 0.4032 - val_lcm_accuracy_1k: 0.6127 - val_lcm_accuracy_2k: 0.7653 - val_lcm_accuracy_3k: 0.8320 - val_lcm_accuracy_5k: 0.8834 - val_lcm_hamming_loss_k: 0.0039
Epoch 15/150
27/27 [==============================] - ETA: 0s - loss: 0.2105 - lcm_precision_1k: 0.7235 - lcm_precision_2k: 0.5689 - lcm_precision_3k: 0.4536 - lcm_precision_5k: 0.3168 - lcm_recall_1k: 0.4609 - lcm_recall_2k: 0.6724 - lcm_recall_3k: 0.7734 - lcm_recall_5k: 0.8684 - lcm_f1_1k: 0.5630 - lcm_f1_2k: 0.6163 - lcm_f1_3k: 0.5718 - lcm_f1_5k: 0.4642 - lcm_accuracy_1k: 0.7235 - lcm_accuracy_2k: 0.8696 - lcm_accuracy_3k: 0.9209 - lcm_accuracy_5k: 0.9591 - lcm_hamming_loss_k: 0.0034
Epoch 00015: val_loss improved from 0.26656 to 0.26638, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2105 - lcm_precision_1k: 0.7235 - lcm_precision_2k: 0.5689 - lcm_precision_3k: 0.4536 - lcm_precision_5k: 0.3168 - lcm_recall_1k: 0.4609 - lcm_recall_2k: 0.6724 - lcm_recall_3k: 0.7734 - lcm_recall_5k: 0.8684 - lcm_f1_1k: 0.5630 - lcm_f1_2k: 0.6163 - lcm_f1_3k: 0.5718 - lcm_f1_5k: 0.4642 - lcm_accuracy_1k: 0.7235 - lcm_accuracy_2k: 0.8696 - lcm_accuracy_3k: 0.9209 - lcm_accuracy_5k: 0.9591 - lcm_hamming_loss_k: 0.0034 - val_loss: 0.2664 - val_lcm_precision_1k: 0.6209 - val_lcm_precision_2k: 0.4804 - val_lcm_precision_3k: 0.3870 - val_lcm_precision_5k: 0.2760 - val_lcm_recall_1k: 0.3930 - val_lcm_recall_2k: 0.5659 - val_lcm_recall_3k: 0.6617 - val_lcm_recall_5k: 0.7598 - val_lcm_f1_1k: 0.4812 - val_lcm_f1_2k: 0.5194 - val_lcm_f1_3k: 0.4881 - val_lcm_f1_5k: 0.4048 - val_lcm_accuracy_1k: 0.6209 - val_lcm_accuracy_2k: 0.7646 - val_lcm_accuracy_3k: 0.8310 - val_lcm_accuracy_5k: 0.8873 - val_lcm_hamming_loss_k: 0.0039
Epoch 16/150
27/27 [==============================] - ETA: 0s - loss: 0.2053 - lcm_precision_1k: 0.7405 - lcm_precision_2k: 0.5745 - lcm_precision_3k: 0.4595 - lcm_precision_5k: 0.3215 - lcm_recall_1k: 0.4737 - lcm_recall_2k: 0.6782 - lcm_recall_3k: 0.7811 - lcm_recall_5k: 0.8772 - lcm_f1_1k: 0.5777 - lcm_f1_2k: 0.6220 - lcm_f1_3k: 0.5786 - lcm_f1_5k: 0.4705 - lcm_accuracy_1k: 0.7405 - lcm_accuracy_2k: 0.8745 - lcm_accuracy_3k: 0.9252 - lcm_accuracy_5k: 0.9624 - lcm_hamming_loss_k: 0.0034
Epoch 00016: val_loss did not improve from 0.26638
27/27 [==============================] - 11s 391ms/step - loss: 0.2053 - lcm_precision_1k: 0.7405 - lcm_precision_2k: 0.5745 - lcm_precision_3k: 0.4595 - lcm_precision_5k: 0.3215 - lcm_recall_1k: 0.4737 - lcm_recall_2k: 0.6782 - lcm_recall_3k: 0.7811 - lcm_recall_5k: 0.8772 - lcm_f1_1k: 0.5777 - lcm_f1_2k: 0.6220 - lcm_f1_3k: 0.5786 - lcm_f1_5k: 0.4705 - lcm_accuracy_1k: 0.7405 - lcm_accuracy_2k: 0.8745 - lcm_accuracy_3k: 0.9252 - lcm_accuracy_5k: 0.9624 - lcm_hamming_loss_k: 0.0034 - val_loss: 0.2701 - val_lcm_precision_1k: 0.6120 - val_lcm_precision_2k: 0.4770 - val_lcm_precision_3k: 0.3877 - val_lcm_precision_5k: 0.2753 - val_lcm_recall_1k: 0.3843 - val_lcm_recall_2k: 0.5614 - val_lcm_recall_3k: 0.6623 - val_lcm_recall_5k: 0.7580 - val_lcm_f1_1k: 0.4719 - val_lcm_f1_2k: 0.5155 - val_lcm_f1_3k: 0.4889 - val_lcm_f1_5k: 0.4037 - val_lcm_accuracy_1k: 0.6120 - val_lcm_accuracy_2k: 0.7604 - val_lcm_accuracy_3k: 0.8306 - val_lcm_accuracy_5k: 0.8862 - val_lcm_hamming_loss_k: 0.0039
Epoch 17/150
27/27 [==============================] - ETA: 0s - loss: 0.1989 - lcm_precision_1k: 0.7526 - lcm_precision_2k: 0.5907 - lcm_precision_3k: 0.4700 - lcm_precision_5k: 0.3263 - lcm_recall_1k: 0.4798 - lcm_recall_2k: 0.6956 - lcm_recall_3k: 0.7971 - lcm_recall_5k: 0.8886 - lcm_f1_1k: 0.5859 - lcm_f1_2k: 0.6387 - lcm_f1_3k: 0.5912 - lcm_f1_5k: 0.4773 - lcm_accuracy_1k: 0.7526 - lcm_accuracy_2k: 0.8872 - lcm_accuracy_3k: 0.9356 - lcm_accuracy_5k: 0.9694 - lcm_hamming_loss_k: 0.0033
Epoch 00017: val_loss improved from 0.26638 to 0.26612, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 434ms/step - loss: 0.1989 - lcm_precision_1k: 0.7526 - lcm_precision_2k: 0.5907 - lcm_precision_3k: 0.4700 - lcm_precision_5k: 0.3263 - lcm_recall_1k: 0.4798 - lcm_recall_2k: 0.6956 - lcm_recall_3k: 0.7971 - lcm_recall_5k: 0.8886 - lcm_f1_1k: 0.5859 - lcm_f1_2k: 0.6387 - lcm_f1_3k: 0.5912 - lcm_f1_5k: 0.4773 - lcm_accuracy_1k: 0.7526 - lcm_accuracy_2k: 0.8872 - lcm_accuracy_3k: 0.9356 - lcm_accuracy_5k: 0.9694 - lcm_hamming_loss_k: 0.0033 - val_loss: 0.2661 - val_lcm_precision_1k: 0.6150 - val_lcm_precision_2k: 0.4815 - val_lcm_precision_3k: 0.3890 - val_lcm_precision_5k: 0.2767 - val_lcm_recall_1k: 0.3881 - val_lcm_recall_2k: 0.5720 - val_lcm_recall_3k: 0.6679 - val_lcm_recall_5k: 0.7640 - val_lcm_f1_1k: 0.4757 - val_lcm_f1_2k: 0.5226 - val_lcm_f1_3k: 0.4914 - val_lcm_f1_5k: 0.4061 - val_lcm_accuracy_1k: 0.6150 - val_lcm_accuracy_2k: 0.7740 - val_lcm_accuracy_3k: 0.8389 - val_lcm_accuracy_5k: 0.8917 - val_lcm_hamming_loss_k: 0.0039
Epoch 18/150
27/27 [==============================] - ETA: 0s - loss: 0.1934 - lcm_precision_1k: 0.7622 - lcm_precision_2k: 0.5966 - lcm_precision_3k: 0.4745 - lcm_precision_5k: 0.3296 - lcm_recall_1k: 0.4891 - lcm_recall_2k: 0.7051 - lcm_recall_3k: 0.8066 - lcm_recall_5k: 0.8987 - lcm_f1_1k: 0.5958 - lcm_f1_2k: 0.6463 - lcm_f1_3k: 0.5974 - lcm_f1_5k: 0.4822 - lcm_accuracy_1k: 0.7622 - lcm_accuracy_2k: 0.8961 - lcm_accuracy_3k: 0.9420 - lcm_accuracy_5k: 0.9739 - lcm_hamming_loss_k: 0.0032
Epoch 00018: val_loss improved from 0.26612 to 0.26612, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.1934 - lcm_precision_1k: 0.7622 - lcm_precision_2k: 0.5966 - lcm_precision_3k: 0.4745 - lcm_precision_5k: 0.3296 - lcm_recall_1k: 0.4891 - lcm_recall_2k: 0.7051 - lcm_recall_3k: 0.8066 - lcm_recall_5k: 0.8987 - lcm_f1_1k: 0.5958 - lcm_f1_2k: 0.6463 - lcm_f1_3k: 0.5974 - lcm_f1_5k: 0.4822 - lcm_accuracy_1k: 0.7622 - lcm_accuracy_2k: 0.8961 - lcm_accuracy_3k: 0.9420 - lcm_accuracy_5k: 0.9739 - lcm_hamming_loss_k: 0.0032 - val_loss: 0.2661 - val_lcm_precision_1k: 0.6227 - val_lcm_precision_2k: 0.4847 - val_lcm_precision_3k: 0.3892 - val_lcm_precision_5k: 0.2760 - val_lcm_recall_1k: 0.3940 - val_lcm_recall_2k: 0.5726 - val_lcm_recall_3k: 0.6660 - val_lcm_recall_5k: 0.7603 - val_lcm_f1_1k: 0.4824 - val_lcm_f1_2k: 0.5248 - val_lcm_f1_3k: 0.4911 - val_lcm_f1_5k: 0.4048 - val_lcm_accuracy_1k: 0.6227 - val_lcm_accuracy_2k: 0.7664 - val_lcm_accuracy_3k: 0.8316 - val_lcm_accuracy_5k: 0.8903 - val_lcm_hamming_loss_k: 0.0039
Epoch 19/150
27/27 [==============================] - ETA: 0s - loss: 0.1866 - lcm_precision_1k: 0.7799 - lcm_precision_2k: 0.6074 - lcm_precision_3k: 0.4833 - lcm_precision_5k: 0.3339 - lcm_recall_1k: 0.5007 - lcm_recall_2k: 0.7160 - lcm_recall_3k: 0.8185 - lcm_recall_5k: 0.9062 - lcm_f1_1k: 0.6098 - lcm_f1_2k: 0.6571 - lcm_f1_3k: 0.6077 - lcm_f1_5k: 0.4879 - lcm_accuracy_1k: 0.7799 - lcm_accuracy_2k: 0.9041 - lcm_accuracy_3k: 0.9487 - lcm_accuracy_5k: 0.9761 - lcm_hamming_loss_k: 0.0032
Epoch 00019: val_loss improved from 0.26612 to 0.26431, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.1866 - lcm_precision_1k: 0.7799 - lcm_precision_2k: 0.6074 - lcm_precision_3k: 0.4833 - lcm_precision_5k: 0.3339 - lcm_recall_1k: 0.5007 - lcm_recall_2k: 0.7160 - lcm_recall_3k: 0.8185 - lcm_recall_5k: 0.9062 - lcm_f1_1k: 0.6098 - lcm_f1_2k: 0.6571 - lcm_f1_3k: 0.6077 - lcm_f1_5k: 0.4879 - lcm_accuracy_1k: 0.7799 - lcm_accuracy_2k: 0.9041 - lcm_accuracy_3k: 0.9487 - lcm_accuracy_5k: 0.9761 - lcm_hamming_loss_k: 0.0032 - val_loss: 0.2643 - val_lcm_precision_1k: 0.6211 - val_lcm_precision_2k: 0.4838 - val_lcm_precision_3k: 0.3894 - val_lcm_precision_5k: 0.2756 - val_lcm_recall_1k: 0.3930 - val_lcm_recall_2k: 0.5712 - val_lcm_recall_3k: 0.6668 - val_lcm_recall_5k: 0.7588 - val_lcm_f1_1k: 0.4812 - val_lcm_f1_2k: 0.5237 - val_lcm_f1_3k: 0.4914 - val_lcm_f1_5k: 0.4042 - val_lcm_accuracy_1k: 0.6211 - val_lcm_accuracy_2k: 0.7721 - val_lcm_accuracy_3k: 0.8380 - val_lcm_accuracy_5k: 0.8884 - val_lcm_hamming_loss_k: 0.0039
Epoch 20/150
27/27 [==============================] - ETA: 0s - loss: 0.1827 - lcm_precision_1k: 0.7873 - lcm_precision_2k: 0.6170 - lcm_precision_3k: 0.4882 - lcm_precision_5k: 0.3372 - lcm_recall_1k: 0.5061 - lcm_recall_2k: 0.7249 - lcm_recall_3k: 0.8245 - lcm_recall_5k: 0.9138 - lcm_f1_1k: 0.6161 - lcm_f1_2k: 0.6665 - lcm_f1_3k: 0.6132 - lcm_f1_5k: 0.4925 - lcm_accuracy_1k: 0.7873 - lcm_accuracy_2k: 0.9117 - lcm_accuracy_3k: 0.9511 - lcm_accuracy_5k: 0.9788 - lcm_hamming_loss_k: 0.0031
Epoch 00020: val_loss improved from 0.26431 to 0.26422, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 441ms/step - loss: 0.1827 - lcm_precision_1k: 0.7873 - lcm_precision_2k: 0.6170 - lcm_precision_3k: 0.4882 - lcm_precision_5k: 0.3372 - lcm_recall_1k: 0.5061 - lcm_recall_2k: 0.7249 - lcm_recall_3k: 0.8245 - lcm_recall_5k: 0.9138 - lcm_f1_1k: 0.6161 - lcm_f1_2k: 0.6665 - lcm_f1_3k: 0.6132 - lcm_f1_5k: 0.4925 - lcm_accuracy_1k: 0.7873 - lcm_accuracy_2k: 0.9117 - lcm_accuracy_3k: 0.9511 - lcm_accuracy_5k: 0.9788 - lcm_hamming_loss_k: 0.0031 - val_loss: 0.2642 - val_lcm_precision_1k: 0.6199 - val_lcm_precision_2k: 0.4878 - val_lcm_precision_3k: 0.3903 - val_lcm_precision_5k: 0.2770 - val_lcm_recall_1k: 0.3906 - val_lcm_recall_2k: 0.5762 - val_lcm_recall_3k: 0.6700 - val_lcm_recall_5k: 0.7632 - val_lcm_f1_1k: 0.4790 - val_lcm_f1_2k: 0.5281 - val_lcm_f1_3k: 0.4930 - val_lcm_f1_5k: 0.4063 - val_lcm_accuracy_1k: 0.6199 - val_lcm_accuracy_2k: 0.7727 - val_lcm_accuracy_3k: 0.8421 - val_lcm_accuracy_5k: 0.8912 - val_lcm_hamming_loss_k: 0.0039
Epoch 21/150
27/27 [==============================] - ETA: 0s - loss: 0.1749 - lcm_precision_1k: 0.8054 - lcm_precision_2k: 0.6270 - lcm_precision_3k: 0.4978 - lcm_precision_5k: 0.3410 - lcm_recall_1k: 0.5190 - lcm_recall_2k: 0.7362 - lcm_recall_3k: 0.8391 - lcm_recall_5k: 0.9226 - lcm_f1_1k: 0.6311 - lcm_f1_2k: 0.6771 - lcm_f1_3k: 0.6248 - lcm_f1_5k: 0.4979 - lcm_accuracy_1k: 0.8054 - lcm_accuracy_2k: 0.9198 - lcm_accuracy_3k: 0.9594 - lcm_accuracy_5k: 0.9825 - lcm_hamming_loss_k: 0.0030
Epoch 00021: val_loss improved from 0.26422 to 0.26420, saving model to logs/voyiyu-labs-0604-122607/model/checkpoint_labs.h5
27/27 [==============================] - 12s 436ms/step - loss: 0.1749 - lcm_precision_1k: 0.8054 - lcm_precision_2k: 0.6270 - lcm_precision_3k: 0.4978 - lcm_precision_5k: 0.3410 - lcm_recall_1k: 0.5190 - lcm_recall_2k: 0.7362 - lcm_recall_3k: 0.8391 - lcm_recall_5k: 0.9226 - lcm_f1_1k: 0.6311 - lcm_f1_2k: 0.6771 - lcm_f1_3k: 0.6248 - lcm_f1_5k: 0.4979 - lcm_accuracy_1k: 0.8054 - lcm_accuracy_2k: 0.9198 - lcm_accuracy_3k: 0.9594 - lcm_accuracy_5k: 0.9825 - lcm_hamming_loss_k: 0.0030 - val_loss: 0.2642 - val_lcm_precision_1k: 0.6322 - val_lcm_precision_2k: 0.4874 - val_lcm_precision_3k: 0.3901 - val_lcm_precision_5k: 0.2763 - val_lcm_recall_1k: 0.3990 - val_lcm_recall_2k: 0.5762 - val_lcm_recall_3k: 0.6688 - val_lcm_recall_5k: 0.7647 - val_lcm_f1_1k: 0.4890 - val_lcm_f1_2k: 0.5278 - val_lcm_f1_3k: 0.4925 - val_lcm_f1_5k: 0.4058 - val_lcm_accuracy_1k: 0.6322 - val_lcm_accuracy_2k: 0.7725 - val_lcm_accuracy_3k: 0.8337 - val_lcm_accuracy_5k: 0.8936 - val_lcm_hamming_loss_k: 0.0038
Epoch 22/150
27/27 [==============================] - ETA: 0s - loss: 0.1689 - lcm_precision_1k: 0.8146 - lcm_precision_2k: 0.6391 - lcm_precision_3k: 0.5067 - lcm_precision_5k: 0.3456 - lcm_recall_1k: 0.5247 - lcm_recall_2k: 0.7477 - lcm_recall_3k: 0.8494 - lcm_recall_5k: 0.9298 - lcm_f1_1k: 0.6381 - lcm_f1_2k: 0.6890 - lcm_f1_3k: 0.6346 - lcm_f1_5k: 0.5038 - lcm_accuracy_1k: 0.8146 - lcm_accuracy_2k: 0.9280 - lcm_accuracy_3k: 0.9625 - lcm_accuracy_5k: 0.9846 - lcm_hamming_loss_k: 0.0030
Epoch 00022: val_loss did not improve from 0.26420
27/27 [==============================] - 10s 388ms/step - loss: 0.1689 - lcm_precision_1k: 0.8146 - lcm_precision_2k: 0.6391 - lcm_precision_3k: 0.5067 - lcm_precision_5k: 0.3456 - lcm_recall_1k: 0.5247 - lcm_recall_2k: 0.7477 - lcm_recall_3k: 0.8494 - lcm_recall_5k: 0.9298 - lcm_f1_1k: 0.6381 - lcm_f1_2k: 0.6890 - lcm_f1_3k: 0.6346 - lcm_f1_5k: 0.5038 - lcm_accuracy_1k: 0.8146 - lcm_accuracy_2k: 0.9280 - lcm_accuracy_3k: 0.9625 - lcm_accuracy_5k: 0.9846 - lcm_hamming_loss_k: 0.0030 - val_loss: 0.2745 - val_lcm_precision_1k: 0.6195 - val_lcm_precision_2k: 0.4854 - val_lcm_precision_3k: 0.3885 - val_lcm_precision_5k: 0.2736 - val_lcm_recall_1k: 0.3929 - val_lcm_recall_2k: 0.5747 - val_lcm_recall_3k: 0.6683 - val_lcm_recall_5k: 0.7580 - val_lcm_f1_1k: 0.4806 - val_lcm_f1_2k: 0.5261 - val_lcm_f1_3k: 0.4911 - val_lcm_f1_5k: 0.4020 - val_lcm_accuracy_1k: 0.6195 - val_lcm_accuracy_2k: 0.7738 - val_lcm_accuracy_3k: 0.8368 - val_lcm_accuracy_5k: 0.8890 - val_lcm_hamming_loss_k: 0.0039
Epoch 23/150
27/27 [==============================] - ETA: 0s - loss: 0.1649 - lcm_precision_1k: 0.8267 - lcm_precision_2k: 0.6475 - lcm_precision_3k: 0.5134 - lcm_precision_5k: 0.3491 - lcm_recall_1k: 0.5341 - lcm_recall_2k: 0.7579 - lcm_recall_3k: 0.8589 - lcm_recall_5k: 0.9369 - lcm_f1_1k: 0.6488 - lcm_f1_2k: 0.6983 - lcm_f1_3k: 0.6426 - lcm_f1_5k: 0.5086 - lcm_accuracy_1k: 0.8267 - lcm_accuracy_2k: 0.9345 - lcm_accuracy_3k: 0.9674 - lcm_accuracy_5k: 0.9861 - lcm_hamming_loss_k: 0.0029
Epoch 00023: val_loss did not improve from 0.26420
27/27 [==============================] - 10s 388ms/step - loss: 0.1649 - lcm_precision_1k: 0.8267 - lcm_precision_2k: 0.6475 - lcm_precision_3k: 0.5134 - lcm_precision_5k: 0.3491 - lcm_recall_1k: 0.5341 - lcm_recall_2k: 0.7579 - lcm_recall_3k: 0.8589 - lcm_recall_5k: 0.9369 - lcm_f1_1k: 0.6488 - lcm_f1_2k: 0.6983 - lcm_f1_3k: 0.6426 - lcm_f1_5k: 0.5086 - lcm_accuracy_1k: 0.8267 - lcm_accuracy_2k: 0.9345 - lcm_accuracy_3k: 0.9674 - lcm_accuracy_5k: 0.9861 - lcm_hamming_loss_k: 0.0029 - val_loss: 0.2661 - val_lcm_precision_1k: 0.6284 - val_lcm_precision_2k: 0.4855 - val_lcm_precision_3k: 0.3903 - val_lcm_precision_5k: 0.2755 - val_lcm_recall_1k: 0.3970 - val_lcm_recall_2k: 0.5721 - val_lcm_recall_3k: 0.6695 - val_lcm_recall_5k: 0.7620 - val_lcm_f1_1k: 0.4864 - val_lcm_f1_2k: 0.5251 - val_lcm_f1_3k: 0.4929 - val_lcm_f1_5k: 0.4045 - val_lcm_accuracy_1k: 0.6284 - val_lcm_accuracy_2k: 0.7687 - val_lcm_accuracy_3k: 0.8375 - val_lcm_accuracy_5k: 0.8930 - val_lcm_hamming_loss_k: 0.0039
Epoch 00023: early stopping
176/176 [==============================] - 8s 42ms/step - loss: 0.1964 - lcm_precision_1k: 0.7538 - lcm_precision_2k: 0.5920 - lcm_precision_3k: 0.4694 - lcm_precision_5k: 0.3225 - lcm_recall_1k: 0.4866 - lcm_recall_2k: 0.6996 - lcm_recall_3k: 0.7985 - lcm_recall_5k: 0.8799 - lcm_f1_1k: 0.5902 - lcm_f1_2k: 0.6401 - lcm_f1_3k: 0.5901 - lcm_f1_5k: 0.4712 - lcm_accuracy_1k: 0.7538 - lcm_accuracy_2k: 0.8814 - lcm_accuracy_3k: 0.9274 - lcm_accuracy_5k: 0.9555 - lcm_hamming_loss_k: 0.0033A: 6s - loss: 0.2097 - lcm_precision_1k: 0.7563 - lcm_precision_2k: 0.5969 - lcm_precision_3k: 0.4667 - lcm_precision_5k: 0.3088 - lcm_recall_1k: 0.4877 - lcm_recall_2k: 0.6910 - lcm_recall_3k: 0.7864 - lcm_recall_5k: 0.8485 - lcm_f1_1k: 0.5916 - lcm_f1_2k: 0.6393 - lcm_f1_3k: 0.5846 - lcm_f1_5k: 0.4520 - lcm_accuracy_1k: 0.7563 - lcm_accuracy_2k: 0.8875 - lcm_accuracy_3k: 0 - ETA: 4s - loss: 0.1976 - lcm_precision_1k: 0.7476 - lcm_precision_2k: 0.5942 - lcm_precision_3k: 0.4702 - lcm_precision_5k: 0.3216 - lcm_recall_1k: 0.4838 - lcm_recall_2k: 0.7030 - lcm_recall_3k: 0.8034 - lcm_recall_5k: 0.8809 - lcm_f1_1k: 0.5861 - lcm_f1_2k: 0.6428 - lcm_f1_3k: 0.5922 - lcm_f1_5k: 0.4704 - lcm_accuracy_1k: 0.7476 - lcm_accuracy_2k: 0.8851 - lcm_accuracy_3k: 0.9313 - lcm_ - ETA: 2s - loss: 0.1959 - lcm_precision_1k: 0.7539 - lcm_precision_2k: 0.5932 - lcm_precision_3k: 0.4692 - lcm_precision_5k: 0.3217 - lcm_recall_1k: 0.4869 - lcm_recall_2k: 0.7019 - lcm_recall_3k: 0.8010 - lcm_recall_5k: 0.8811 - lcm_f1_1k: 0.5905 - lcm_f1_2k: 0.6418 - lcm_f1_3k: 0.5907 - lcm_f1_5k: 0.4706 - lcm_accuracy_1k: 0.7539 - lcm_accuracy_2k: 0.8816 - lcm_accuracy_3k: 0.9300 - lcm_accuracy_5k: 0.9566 - lcm_ - ETA: 1s - loss: 0.1957 - lcm_precision_1k: 0.7539 - lcm_precision_2k: 0.5926 - lcm_precision_3k: 0.4691 - lcm_precision_5k: 0.3216 - lcm_recall_1k: 0.4888 - lcm_recall_2k: 0.7022 - lcm_recall_3k: 0.8016 - lcm_recall_5k: 0.8819 - lcm_f1_1k: 0.5919 - lcm_f1_2k: 0.6415 - lcm_f1_3k: 0.5908 - lcm_f1_5k: 0.4706 - lcm_accuracy_1k: 0.7539 - lcm_accuracy_2k: 0.8810 - lcm_accuracy_3k: 0.9296 - lcm_accuracy_5k: 0.9572 - lcm_ - ETA: 0s - loss: 0.1963 - lcm_precision_1k: 0.7556 - lcm_precision_2k: 0.5950 - lcm_precision_3k: 0.4697 - lcm_precision_5k: 0.3223 - lcm_recall_1k: 0.4877 - lcm_recall_2k: 0.7021 - lcm_recall_3k: 0.7993 - lcm_recall_5k: 0.8802 - lcm_f1_1k: 0.5915 - lcm_f1_2k: 0.6429 - lcm_f1_3k: 0.5907 - lcm_f1_5k: 0.4712 - lcm_accuracy_1k: 0.7556 - lcm_accuracy_2k: 0.8822 - lcm_accuracy_3k: 0.9283 - lcm_accuracy_5k: 0.9568 - lcm_hamm - ETA: 0s - loss: 0.1966 - lcm_precision_1k: 0.7538 - lcm_precision_2k: 0.5923 - lcm_precision_3k: 0.4698 - lcm_precision_5k: 0.3228 - lcm_recall_1k: 0.4863 - lcm_recall_2k: 0.6995 - lcm_recall_3k: 0.7988 - lcm_recall_5k: 0.8801 - lcm_f1_1k: 0.5900 - lcm_f1_2k: 0.6403 - lcm_f1_3k: 0.5906 - lcm_f1_5k: 0.4716 - lcm_accuracy_1k: 0.7538 - lcm_accuracy_2k: 0.8816 - lcm_accuracy_3k: 0.9279 - lcm_accuracy_5k: 0.9559 - lcm_hamming_loss_k: 0.00
Best model result:  [0.19639618694782257, 0.7537881731987, 0.5920263528823853, 0.46935299038887024, 0.32246720790863037, 0.4865584671497345, 0.6996245980262756, 0.7984914183616638, 0.8798694014549255, 0.5901800394058228, 0.6401146650314331, 0.5901197791099548, 0.4712243378162384, 0.7537881731987, 0.8813679218292236, 0.9273601770401001, 0.9554733633995056, 0.0032550685573369265]
13499
3374
5625
Model: "model_8"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_12 (S  (None, 15, 300)     0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_12[0][
                                                                 0]']                             
                                                                                                  
 permute_12 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_20 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_12[0][0]']             
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_20[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_21 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_8 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_21[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_4 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_8[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_12 (Dense)               (None, 1024)         2098176     ['tf.concat_4[0][0]']            
                                                                                                  
 dense_13 (Dense)               (None, 15)           15375       ['dense_12[0][0]']               
                                                                                                  
 tf.nn.softmax_4 (TFOpLambda)   (None, 15)           0           ['dense_13[0][0]']               
                                                                                                  
 tf.expand_dims_8 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_4[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_8[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_13 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_22 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_13[0][0]']             
                                                                                                  
 dense_14 (Dense)               (None, 15, 150)      22650       ['lambda_22[0][0]']              
                                                                                                  
 tf.math.reduce_mean_9 (TFOpLam  (None, 150)         0           ['dense_14[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_9 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_9[0][0]']  
                                                                                                  
 tf.__operators__.getitem_13 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_4 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_9[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_13[0][
                                                                 0]']                             
                                                                                                  
 permute_14 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_4[0][0]']     
                                                                                                  
 lambda_23 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_14[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_23[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_24 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_4[0][0]']     
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_24[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
==================================================================================================
Total params: 31,474,320
Trainable params: 6,695,820
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
2 patience
Model: "model_9"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 label_input (InputLayer)       [(None, 442)]        0           []                               
                                                                                                  
 text_input (InputLayer)        [(None, 150)]        0           []                               
                                                                                                  
 label_emb (Embedding)          (None, 442, 300)     128100      ['label_input[0][0]']            
                                                                                                  
 text_emb (Embedding)           (None, 150, 300)     24778500    ['text_input[0][0]']             
                                                                                                  
 BiLSTM (Bidirectional)         (None, 150, 1024)    3330048     ['text_emb[0][0]']               
                                                                                                  
 tf.__operators__.getitem_12 (S  (None, 15, 300)     0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 0_level_label_emb (Dense)      (None, 15, 1024)     308224      ['tf.__operators__.getitem_12[0][
                                                                 0]']                             
                                                                                                  
 permute_12 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_20 (Lambda)             (None, 15, 150)      0           ['0_level_label_emb[0][0]',      
                                                                  'permute_12[0][0]']             
                                                                                                  
 0_attention_layer_att_weight (  (None, 15, 150)     22650       ['lambda_20[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_21 (Lambda)             (None, 15, 1024)     0           ['0_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'BiLSTM[0][0]']                 
                                                                                                  
 tf.math.reduce_mean_8 (TFOpLam  (None, 1024)        0           ['BiLSTM[0][0]']                 
 bda)                                                                                             
                                                                                                  
 0_attention_layer_att_context   (None, 1024)        0           ['lambda_21[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 tf.concat_4 (TFOpLambda)       (None, 2048)         0           ['tf.math.reduce_mean_8[0][0]',  
                                                                  '0_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 dense_12 (Dense)               (None, 1024)         2098176     ['tf.concat_4[0][0]']            
                                                                                                  
 dense_13 (Dense)               (None, 15)           15375       ['dense_12[0][0]']               
                                                                                                  
 tf.nn.softmax_4 (TFOpLambda)   (None, 15)           0           ['dense_13[0][0]']               
                                                                                                  
 tf.expand_dims_8 (TFOpLambda)  (None, 15, 1)        0           ['tf.nn.softmax_4[0][0]']        
                                                                                                  
 0_local_layer_predict_att_emb   (None, 15, 1024)    2048        ['tf.expand_dims_8[0][0]']       
 (Dense)                                                                                          
                                                                                                  
 permute_13 (Permute)           (None, 1024, 150)    0           ['BiLSTM[0][0]']                 
                                                                                                  
 lambda_22 (Lambda)             (None, 15, 150)      0           ['0_local_layer_predict_att_emb[0
                                                                 ][0]',                           
                                                                  'permute_13[0][0]']             
                                                                                                  
 dense_14 (Dense)               (None, 15, 150)      22650       ['lambda_22[0][0]']              
                                                                                                  
 tf.math.reduce_mean_9 (TFOpLam  (None, 150)         0           ['dense_14[0][0]']               
 bda)                                                                                             
                                                                                                  
 tf.expand_dims_9 (TFOpLambda)  (None, 150, 1)       0           ['tf.math.reduce_mean_9[0][0]']  
                                                                                                  
 tf.__operators__.getitem_13 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 tf.math.multiply_4 (TFOpLambda  (None, 150, 1024)   0           ['BiLSTM[0][0]',                 
 )                                                                'tf.expand_dims_9[0][0]']       
                                                                                                  
 1_level_label_emb (Dense)      (None, 427, 1024)    308224      ['tf.__operators__.getitem_13[0][
                                                                 0]']                             
                                                                                                  
 permute_14 (Permute)           (None, 1024, 150)    0           ['tf.math.multiply_4[0][0]']     
                                                                                                  
 lambda_23 (Lambda)             (None, 427, 150)     0           ['1_level_label_emb[0][0]',      
                                                                  'permute_14[0][0]']             
                                                                                                  
 1_attention_layer_att_weight (  (None, 427, 150)    22650       ['lambda_23[0][0]']              
 Dense)                                                                                           
                                                                                                  
 lambda_24 (Lambda)             (None, 427, 1024)    0           ['1_attention_layer_att_weight[0]
                                                                 [0]',                            
                                                                  'tf.math.multiply_4[0][0]']     
                                                                                                  
 tf.__operators__.getitem_14 (S  (None, 427, 300)    0           ['label_emb[0][0]']              
 licingOpLambda)                                                                                  
                                                                                                  
 1_attention_layer_att_context   (None, 1024)        0           ['lambda_24[0][0]']              
 (Lambda)                                                                                         
                                                                                                  
 label_lcm_emb (Dense)          (None, 427, 1024)    308224      ['tf.__operators__.getitem_14[0][
                                                                 0]']                             
                                                                                                  
 dot_4 (Dot)                    (None, 427)          0           ['label_lcm_emb[0][0]',          
                                                                  '1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 pred_probs (Dense)             (None, 427)          437675      ['1_attention_layer_att_context[0
                                                                 ][0]']                           
                                                                                                  
 label_sim_dict (Dense)         (None, 427)          182756      ['dot_4[0][0]']                  
                                                                                                  
 concatenate_4 (Concatenate)    (None, 854)          0           ['pred_probs[0][0]',             
                                                                  'label_sim_dict[0][0]']         
                                                                                                  
==================================================================================================
Total params: 31,965,300
Trainable params: 7,186,800
Non-trainable params: 24,778,500
__________________________________________________________________________________________________
None
Epoch 1/150
27/27 [==============================] - ETA: 0s - loss: 0.4905 - lcm_precision_1k: 0.2629 - lcm_precision_2k: 0.2156 - lcm_precision_3k: 0.1834 - lcm_precision_5k: 0.1458 - lcm_recall_1k: 0.1521 - lcm_recall_2k: 0.2413 - lcm_recall_3k: 0.3012 - lcm_recall_5k: 0.3915 - lcm_f1_1k: 0.1925 - lcm_f1_2k: 0.2276 - lcm_f1_3k: 0.2278 - lcm_f1_5k: 0.2124 - lcm_accuracy_1k: 0.2629 - lcm_accuracy_2k: 0.3800 - lcm_accuracy_3k: 0.4491 - lcm_accuracy_5k: 0.5392 - lcm_hamming_loss_k: 0.0056
Epoch 00001: val_loss improved from inf to 0.40219, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 13s 427ms/step - loss: 0.4905 - lcm_precision_1k: 0.2629 - lcm_precision_2k: 0.2156 - lcm_precision_3k: 0.1834 - lcm_precision_5k: 0.1458 - lcm_recall_1k: 0.1521 - lcm_recall_2k: 0.2413 - lcm_recall_3k: 0.3012 - lcm_recall_5k: 0.3915 - lcm_f1_1k: 0.1925 - lcm_f1_2k: 0.2276 - lcm_f1_3k: 0.2278 - lcm_f1_5k: 0.2124 - lcm_accuracy_1k: 0.2629 - lcm_accuracy_2k: 0.3800 - lcm_accuracy_3k: 0.4491 - lcm_accuracy_5k: 0.5392 - lcm_hamming_loss_k: 0.0056 - val_loss: 0.4022 - val_lcm_precision_1k: 0.3633 - val_lcm_precision_2k: 0.3041 - val_lcm_precision_3k: 0.2560 - val_lcm_precision_5k: 0.1945 - val_lcm_recall_1k: 0.2129 - val_lcm_recall_2k: 0.3472 - val_lcm_recall_3k: 0.4310 - val_lcm_recall_5k: 0.5352 - val_lcm_f1_1k: 0.2684 - val_lcm_f1_2k: 0.3240 - val_lcm_f1_3k: 0.3210 - val_lcm_f1_5k: 0.2851 - val_lcm_accuracy_1k: 0.3633 - val_lcm_accuracy_2k: 0.5150 - val_lcm_accuracy_3k: 0.6013 - val_lcm_accuracy_5k: 0.6888 - val_lcm_hamming_loss_k: 0.0051
Epoch 2/150
27/27 [==============================] - ETA: 0s - loss: 0.3611 - lcm_precision_1k: 0.4162 - lcm_precision_2k: 0.3404 - lcm_precision_3k: 0.2845 - lcm_precision_5k: 0.2175 - lcm_recall_1k: 0.2474 - lcm_recall_2k: 0.3892 - lcm_recall_3k: 0.4782 - lcm_recall_5k: 0.5911 - lcm_f1_1k: 0.3102 - lcm_f1_2k: 0.3630 - lcm_f1_3k: 0.3566 - lcm_f1_5k: 0.3179 - lcm_accuracy_1k: 0.4162 - lcm_accuracy_2k: 0.5706 - lcm_accuracy_3k: 0.6546 - lcm_accuracy_5k: 0.7467 - lcm_hamming_loss_k: 0.0049
Epoch 00002: val_loss improved from 0.40219 to 0.34122, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 12s 433ms/step - loss: 0.3611 - lcm_precision_1k: 0.4162 - lcm_precision_2k: 0.3404 - lcm_precision_3k: 0.2845 - lcm_precision_5k: 0.2175 - lcm_recall_1k: 0.2474 - lcm_recall_2k: 0.3892 - lcm_recall_3k: 0.4782 - lcm_recall_5k: 0.5911 - lcm_f1_1k: 0.3102 - lcm_f1_2k: 0.3630 - lcm_f1_3k: 0.3566 - lcm_f1_5k: 0.3179 - lcm_accuracy_1k: 0.4162 - lcm_accuracy_2k: 0.5706 - lcm_accuracy_3k: 0.6546 - lcm_accuracy_5k: 0.7467 - lcm_hamming_loss_k: 0.0049 - val_loss: 0.3412 - val_lcm_precision_1k: 0.4553 - val_lcm_precision_2k: 0.3656 - val_lcm_precision_3k: 0.2952 - val_lcm_precision_5k: 0.2219 - val_lcm_recall_1k: 0.2803 - val_lcm_recall_2k: 0.4233 - val_lcm_recall_3k: 0.5055 - val_lcm_recall_5k: 0.6156 - val_lcm_f1_1k: 0.3468 - val_lcm_f1_2k: 0.3921 - val_lcm_f1_3k: 0.3725 - val_lcm_f1_5k: 0.3260 - val_lcm_accuracy_1k: 0.4553 - val_lcm_accuracy_2k: 0.6007 - val_lcm_accuracy_3k: 0.6789 - val_lcm_accuracy_5k: 0.7603 - val_lcm_hamming_loss_k: 0.0046
Epoch 3/150
27/27 [==============================] - ETA: 0s - loss: 0.3193 - lcm_precision_1k: 0.4953 - lcm_precision_2k: 0.3993 - lcm_precision_3k: 0.3308 - lcm_precision_5k: 0.2452 - lcm_recall_1k: 0.3002 - lcm_recall_2k: 0.4613 - lcm_recall_3k: 0.5600 - lcm_recall_5k: 0.6709 - lcm_f1_1k: 0.3737 - lcm_f1_2k: 0.4280 - lcm_f1_3k: 0.4158 - lcm_f1_5k: 0.3591 - lcm_accuracy_1k: 0.4953 - lcm_accuracy_2k: 0.6560 - lcm_accuracy_3k: 0.7376 - lcm_accuracy_5k: 0.8161 - lcm_hamming_loss_k: 0.0045
Epoch 00003: val_loss improved from 0.34122 to 0.31695, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.3193 - lcm_precision_1k: 0.4953 - lcm_precision_2k: 0.3993 - lcm_precision_3k: 0.3308 - lcm_precision_5k: 0.2452 - lcm_recall_1k: 0.3002 - lcm_recall_2k: 0.4613 - lcm_recall_3k: 0.5600 - lcm_recall_5k: 0.6709 - lcm_f1_1k: 0.3737 - lcm_f1_2k: 0.4280 - lcm_f1_3k: 0.4158 - lcm_f1_5k: 0.3591 - lcm_accuracy_1k: 0.4953 - lcm_accuracy_2k: 0.6560 - lcm_accuracy_3k: 0.7376 - lcm_accuracy_5k: 0.8161 - lcm_hamming_loss_k: 0.0045 - val_loss: 0.3169 - val_lcm_precision_1k: 0.5027 - val_lcm_precision_2k: 0.4043 - val_lcm_precision_3k: 0.3312 - val_lcm_precision_5k: 0.2400 - val_lcm_recall_1k: 0.3095 - val_lcm_recall_2k: 0.4751 - val_lcm_recall_3k: 0.5702 - val_lcm_recall_5k: 0.6708 - val_lcm_f1_1k: 0.3830 - val_lcm_f1_2k: 0.4366 - val_lcm_f1_3k: 0.4188 - val_lcm_f1_5k: 0.3533 - val_lcm_accuracy_1k: 0.5027 - val_lcm_accuracy_2k: 0.6629 - val_lcm_accuracy_3k: 0.7396 - val_lcm_accuracy_5k: 0.8125 - val_lcm_hamming_loss_k: 0.0044
Epoch 4/150
27/27 [==============================] - ETA: 0s - loss: 0.2985 - lcm_precision_1k: 0.5369 - lcm_precision_2k: 0.4300 - lcm_precision_3k: 0.3533 - lcm_precision_5k: 0.2587 - lcm_recall_1k: 0.3293 - lcm_recall_2k: 0.4993 - lcm_recall_3k: 0.5988 - lcm_recall_5k: 0.7111 - lcm_f1_1k: 0.4082 - lcm_f1_2k: 0.4620 - lcm_f1_3k: 0.4443 - lcm_f1_5k: 0.3794 - lcm_accuracy_1k: 0.5369 - lcm_accuracy_2k: 0.6957 - lcm_accuracy_3k: 0.7744 - lcm_accuracy_5k: 0.8506 - lcm_hamming_loss_k: 0.0043 ETA: 0s - loss: 0.2990 - lcm_precision_1k: 0.5371 - lcm_precision_2k: 0.4283 - lcm_precision_3k: 0.3528 - lcm_precision_5k: 0.2582 - lcm_recall_1k: 0.3291 - lcm_recall_2k: 0.4962 - lcm_recall_3k: 0.5972 - lcm_recall_5k: 0.7092 - lcm_f1_1k: 0.4081 - lcm_f1_2k: 0.4597 - lcm_f1_3k: 0.4435 - lcm_f1_5k: 0.3785 - lcm_accuracy_1k: 0.5371 - lcm_accuracy_2k: 0.6927 - lcm_accuracy_3k: 0.7721 - lcm_accuracy_5k: 0.8487 - lcm_hamming_loss_k: 
Epoch 00004: val_loss improved from 0.31695 to 0.30127, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 12s 439ms/step - loss: 0.2985 - lcm_precision_1k: 0.5369 - lcm_precision_2k: 0.4300 - lcm_precision_3k: 0.3533 - lcm_precision_5k: 0.2587 - lcm_recall_1k: 0.3293 - lcm_recall_2k: 0.4993 - lcm_recall_3k: 0.5988 - lcm_recall_5k: 0.7111 - lcm_f1_1k: 0.4082 - lcm_f1_2k: 0.4620 - lcm_f1_3k: 0.4443 - lcm_f1_5k: 0.3794 - lcm_accuracy_1k: 0.5369 - lcm_accuracy_2k: 0.6957 - lcm_accuracy_3k: 0.7744 - lcm_accuracy_5k: 0.8506 - lcm_hamming_loss_k: 0.0043 - val_loss: 0.3013 - val_lcm_precision_1k: 0.5362 - val_lcm_precision_2k: 0.4222 - val_lcm_precision_3k: 0.3463 - val_lcm_precision_5k: 0.2519 - val_lcm_recall_1k: 0.3305 - val_lcm_recall_2k: 0.4984 - val_lcm_recall_3k: 0.6009 - val_lcm_recall_5k: 0.7034 - val_lcm_f1_1k: 0.4089 - val_lcm_f1_2k: 0.4569 - val_lcm_f1_3k: 0.4392 - val_lcm_f1_5k: 0.3708 - val_lcm_accuracy_1k: 0.5362 - val_lcm_accuracy_2k: 0.6882 - val_lcm_accuracy_3k: 0.7677 - val_lcm_accuracy_5k: 0.8348 - val_lcm_hamming_loss_k: 0.0042
Epoch 5/150
27/27 [==============================] - ETA: 0s - loss: 0.2837 - lcm_precision_1k: 0.5691 - lcm_precision_2k: 0.4536 - lcm_precision_3k: 0.3705 - lcm_precision_5k: 0.2692 - lcm_recall_1k: 0.3505 - lcm_recall_2k: 0.5303 - lcm_recall_3k: 0.6315 - lcm_recall_5k: 0.7418 - lcm_f1_1k: 0.4338 - lcm_f1_2k: 0.4889 - lcm_f1_3k: 0.4670 - lcm_f1_5k: 0.3950 - lcm_accuracy_1k: 0.5691 - lcm_accuracy_2k: 0.7299 - lcm_accuracy_3k: 0.8067 - lcm_accuracy_5k: 0.8766 - lcm_hamming_loss_k: 0.0042
Epoch 00005: val_loss improved from 0.30127 to 0.29460, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 12s 435ms/step - loss: 0.2837 - lcm_precision_1k: 0.5691 - lcm_precision_2k: 0.4536 - lcm_precision_3k: 0.3705 - lcm_precision_5k: 0.2692 - lcm_recall_1k: 0.3505 - lcm_recall_2k: 0.5303 - lcm_recall_3k: 0.6315 - lcm_recall_5k: 0.7418 - lcm_f1_1k: 0.4338 - lcm_f1_2k: 0.4889 - lcm_f1_3k: 0.4670 - lcm_f1_5k: 0.3950 - lcm_accuracy_1k: 0.5691 - lcm_accuracy_2k: 0.7299 - lcm_accuracy_3k: 0.8067 - lcm_accuracy_5k: 0.8766 - lcm_hamming_loss_k: 0.0042 - val_loss: 0.2946 - val_lcm_precision_1k: 0.5530 - val_lcm_precision_2k: 0.4365 - val_lcm_precision_3k: 0.3549 - val_lcm_precision_5k: 0.2568 - val_lcm_recall_1k: 0.3447 - val_lcm_recall_2k: 0.5167 - val_lcm_recall_3k: 0.6128 - val_lcm_recall_5k: 0.7178 - val_lcm_f1_1k: 0.4246 - val_lcm_f1_2k: 0.4730 - val_lcm_f1_3k: 0.4492 - val_lcm_f1_5k: 0.3781 - val_lcm_accuracy_1k: 0.5530 - val_lcm_accuracy_2k: 0.7079 - val_lcm_accuracy_3k: 0.7810 - val_lcm_accuracy_5k: 0.8509 - val_lcm_hamming_loss_k: 0.0042
Epoch 6/150
27/27 [==============================] - ETA: 0s - loss: 0.2727 - lcm_precision_1k: 0.5954 - lcm_precision_2k: 0.4739 - lcm_precision_3k: 0.3840 - lcm_precision_5k: 0.2767 - lcm_recall_1k: 0.3692 - lcm_recall_2k: 0.5549 - lcm_recall_3k: 0.6529 - lcm_recall_5k: 0.7603 - lcm_f1_1k: 0.4557 - lcm_f1_2k: 0.5112 - lcm_f1_3k: 0.4835 - lcm_f1_5k: 0.4057 - lcm_accuracy_1k: 0.5954 - lcm_accuracy_2k: 0.7568 - lcm_accuracy_3k: 0.8243 - lcm_accuracy_5k: 0.8909 - lcm_hamming_loss_k: 0.0040
Epoch 00006: val_loss improved from 0.29460 to 0.28928, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 12s 434ms/step - loss: 0.2727 - lcm_precision_1k: 0.5954 - lcm_precision_2k: 0.4739 - lcm_precision_3k: 0.3840 - lcm_precision_5k: 0.2767 - lcm_recall_1k: 0.3692 - lcm_recall_2k: 0.5549 - lcm_recall_3k: 0.6529 - lcm_recall_5k: 0.7603 - lcm_f1_1k: 0.4557 - lcm_f1_2k: 0.5112 - lcm_f1_3k: 0.4835 - lcm_f1_5k: 0.4057 - lcm_accuracy_1k: 0.5954 - lcm_accuracy_2k: 0.7568 - lcm_accuracy_3k: 0.8243 - lcm_accuracy_5k: 0.8909 - lcm_hamming_loss_k: 0.0040 - val_loss: 0.2893 - val_lcm_precision_1k: 0.5685 - val_lcm_precision_2k: 0.4399 - val_lcm_precision_3k: 0.3630 - val_lcm_precision_5k: 0.2589 - val_lcm_recall_1k: 0.3563 - val_lcm_recall_2k: 0.5193 - val_lcm_recall_3k: 0.6263 - val_lcm_recall_5k: 0.7244 - val_lcm_f1_1k: 0.4379 - val_lcm_f1_2k: 0.4761 - val_lcm_f1_3k: 0.4594 - val_lcm_f1_5k: 0.3813 - val_lcm_accuracy_1k: 0.5685 - val_lcm_accuracy_2k: 0.7118 - val_lcm_accuracy_3k: 0.7925 - val_lcm_accuracy_5k: 0.8557 - val_lcm_hamming_loss_k: 0.0041
Epoch 7/150
27/27 [==============================] - ETA: 0s - loss: 0.2652 - lcm_precision_1k: 0.6191 - lcm_precision_2k: 0.4859 - lcm_precision_3k: 0.3930 - lcm_precision_5k: 0.2814 - lcm_recall_1k: 0.3873 - lcm_recall_2k: 0.5704 - lcm_recall_3k: 0.6713 - lcm_recall_5k: 0.7748 - lcm_f1_1k: 0.4764 - lcm_f1_2k: 0.5247 - lcm_f1_3k: 0.4957 - lcm_f1_5k: 0.4129 - lcm_accuracy_1k: 0.6191 - lcm_accuracy_2k: 0.7739 - lcm_accuracy_3k: 0.8415 - lcm_accuracy_5k: 0.9017 - lcm_hamming_loss_k: 0.0039
Epoch 00007: val_loss improved from 0.28928 to 0.28101, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2652 - lcm_precision_1k: 0.6191 - lcm_precision_2k: 0.4859 - lcm_precision_3k: 0.3930 - lcm_precision_5k: 0.2814 - lcm_recall_1k: 0.3873 - lcm_recall_2k: 0.5704 - lcm_recall_3k: 0.6713 - lcm_recall_5k: 0.7748 - lcm_f1_1k: 0.4764 - lcm_f1_2k: 0.5247 - lcm_f1_3k: 0.4957 - lcm_f1_5k: 0.4129 - lcm_accuracy_1k: 0.6191 - lcm_accuracy_2k: 0.7739 - lcm_accuracy_3k: 0.8415 - lcm_accuracy_5k: 0.9017 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2810 - val_lcm_precision_1k: 0.5751 - val_lcm_precision_2k: 0.4541 - val_lcm_precision_3k: 0.3688 - val_lcm_precision_5k: 0.2642 - val_lcm_recall_1k: 0.3575 - val_lcm_recall_2k: 0.5362 - val_lcm_recall_3k: 0.6359 - val_lcm_recall_5k: 0.7359 - val_lcm_f1_1k: 0.4408 - val_lcm_f1_2k: 0.4915 - val_lcm_f1_3k: 0.4666 - val_lcm_f1_5k: 0.3886 - val_lcm_accuracy_1k: 0.5751 - val_lcm_accuracy_2k: 0.7269 - val_lcm_accuracy_3k: 0.8008 - val_lcm_accuracy_5k: 0.8596 - val_lcm_hamming_loss_k: 0.0041
Epoch 8/150
27/27 [==============================] - ETA: 0s - loss: 0.2562 - lcm_precision_1k: 0.6352 - lcm_precision_2k: 0.5012 - lcm_precision_3k: 0.4031 - lcm_precision_5k: 0.2878 - lcm_recall_1k: 0.3978 - lcm_recall_2k: 0.5881 - lcm_recall_3k: 0.6864 - lcm_recall_5k: 0.7906 - lcm_f1_1k: 0.4891 - lcm_f1_2k: 0.5411 - lcm_f1_3k: 0.5078 - lcm_f1_5k: 0.4220 - lcm_accuracy_1k: 0.6352 - lcm_accuracy_2k: 0.7902 - lcm_accuracy_3k: 0.8553 - lcm_accuracy_5k: 0.9129 - lcm_hamming_loss_k: 0.0039
Epoch 00008: val_loss improved from 0.28101 to 0.27766, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 429ms/step - loss: 0.2562 - lcm_precision_1k: 0.6352 - lcm_precision_2k: 0.5012 - lcm_precision_3k: 0.4031 - lcm_precision_5k: 0.2878 - lcm_recall_1k: 0.3978 - lcm_recall_2k: 0.5881 - lcm_recall_3k: 0.6864 - lcm_recall_5k: 0.7906 - lcm_f1_1k: 0.4891 - lcm_f1_2k: 0.5411 - lcm_f1_3k: 0.5078 - lcm_f1_5k: 0.4220 - lcm_accuracy_1k: 0.6352 - lcm_accuracy_2k: 0.7902 - lcm_accuracy_3k: 0.8553 - lcm_accuracy_5k: 0.9129 - lcm_hamming_loss_k: 0.0039 - val_loss: 0.2777 - val_lcm_precision_1k: 0.5894 - val_lcm_precision_2k: 0.4597 - val_lcm_precision_3k: 0.3738 - val_lcm_precision_5k: 0.2672 - val_lcm_recall_1k: 0.3701 - val_lcm_recall_2k: 0.5467 - val_lcm_recall_3k: 0.6468 - val_lcm_recall_5k: 0.7436 - val_lcm_f1_1k: 0.4545 - val_lcm_f1_2k: 0.4992 - val_lcm_f1_3k: 0.4736 - val_lcm_f1_5k: 0.3930 - val_lcm_accuracy_1k: 0.5894 - val_lcm_accuracy_2k: 0.7408 - val_lcm_accuracy_3k: 0.8130 - val_lcm_accuracy_5k: 0.8674 - val_lcm_hamming_loss_k: 0.0040
Epoch 9/150
27/27 [==============================] - ETA: 0s - loss: 0.2470 - lcm_precision_1k: 0.6523 - lcm_precision_2k: 0.5140 - lcm_precision_3k: 0.4133 - lcm_precision_5k: 0.2938 - lcm_recall_1k: 0.4091 - lcm_recall_2k: 0.6041 - lcm_recall_3k: 0.7052 - lcm_recall_5k: 0.8065 - lcm_f1_1k: 0.5028 - lcm_f1_2k: 0.5553 - lcm_f1_3k: 0.5211 - lcm_f1_5k: 0.4306 - lcm_accuracy_1k: 0.6523 - lcm_accuracy_2k: 0.8050 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9243 - lcm_hamming_loss_k: 0.0038
Epoch 00009: val_loss improved from 0.27766 to 0.27423, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 426ms/step - loss: 0.2470 - lcm_precision_1k: 0.6523 - lcm_precision_2k: 0.5140 - lcm_precision_3k: 0.4133 - lcm_precision_5k: 0.2938 - lcm_recall_1k: 0.4091 - lcm_recall_2k: 0.6041 - lcm_recall_3k: 0.7052 - lcm_recall_5k: 0.8065 - lcm_f1_1k: 0.5028 - lcm_f1_2k: 0.5553 - lcm_f1_3k: 0.5211 - lcm_f1_5k: 0.4306 - lcm_accuracy_1k: 0.6523 - lcm_accuracy_2k: 0.8050 - lcm_accuracy_3k: 0.8707 - lcm_accuracy_5k: 0.9243 - lcm_hamming_loss_k: 0.0038 - val_loss: 0.2742 - val_lcm_precision_1k: 0.5963 - val_lcm_precision_2k: 0.4712 - val_lcm_precision_3k: 0.3790 - val_lcm_precision_5k: 0.2686 - val_lcm_recall_1k: 0.3737 - val_lcm_recall_2k: 0.5600 - val_lcm_recall_3k: 0.6562 - val_lcm_recall_5k: 0.7512 - val_lcm_f1_1k: 0.4593 - val_lcm_f1_2k: 0.5115 - val_lcm_f1_3k: 0.4802 - val_lcm_f1_5k: 0.3956 - val_lcm_accuracy_1k: 0.5963 - val_lcm_accuracy_2k: 0.7503 - val_lcm_accuracy_3k: 0.8210 - val_lcm_accuracy_5k: 0.8777 - val_lcm_hamming_loss_k: 0.0040
Epoch 10/150
27/27 [==============================] - ETA: 0s - loss: 0.2403 - lcm_precision_1k: 0.6638 - lcm_precision_2k: 0.5242 - lcm_precision_3k: 0.4198 - lcm_precision_5k: 0.2977 - lcm_recall_1k: 0.4175 - lcm_recall_2k: 0.6178 - lcm_recall_3k: 0.7164 - lcm_recall_5k: 0.8175 - lcm_f1_1k: 0.5125 - lcm_f1_2k: 0.5671 - lcm_f1_3k: 0.5293 - lcm_f1_5k: 0.4364 - lcm_accuracy_1k: 0.6638 - lcm_accuracy_2k: 0.8193 - lcm_accuracy_3k: 0.8791 - lcm_accuracy_5k: 0.9308 - lcm_hamming_loss_k: 0.0037
Epoch 00010: val_loss improved from 0.27423 to 0.26979, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 428ms/step - loss: 0.2403 - lcm_precision_1k: 0.6638 - lcm_precision_2k: 0.5242 - lcm_precision_3k: 0.4198 - lcm_precision_5k: 0.2977 - lcm_recall_1k: 0.4175 - lcm_recall_2k: 0.6178 - lcm_recall_3k: 0.7164 - lcm_recall_5k: 0.8175 - lcm_f1_1k: 0.5125 - lcm_f1_2k: 0.5671 - lcm_f1_3k: 0.5293 - lcm_f1_5k: 0.4364 - lcm_accuracy_1k: 0.6638 - lcm_accuracy_2k: 0.8193 - lcm_accuracy_3k: 0.8791 - lcm_accuracy_5k: 0.9308 - lcm_hamming_loss_k: 0.0037 - val_loss: 0.2698 - val_lcm_precision_1k: 0.6133 - val_lcm_precision_2k: 0.4772 - val_lcm_precision_3k: 0.3827 - val_lcm_precision_5k: 0.2721 - val_lcm_recall_1k: 0.3869 - val_lcm_recall_2k: 0.5664 - val_lcm_recall_3k: 0.6609 - val_lcm_recall_5k: 0.7583 - val_lcm_f1_1k: 0.4744 - val_lcm_f1_2k: 0.5178 - val_lcm_f1_3k: 0.4846 - val_lcm_f1_5k: 0.4003 - val_lcm_accuracy_1k: 0.6133 - val_lcm_accuracy_2k: 0.7605 - val_lcm_accuracy_3k: 0.8237 - val_lcm_accuracy_5k: 0.8787 - val_lcm_hamming_loss_k: 0.0039
Epoch 11/150
27/27 [==============================] - ETA: 0s - loss: 0.2345 - lcm_precision_1k: 0.6827 - lcm_precision_2k: 0.5335 - lcm_precision_3k: 0.4279 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4312 - lcm_recall_2k: 0.6285 - lcm_recall_3k: 0.7290 - lcm_recall_5k: 0.8283 - lcm_f1_1k: 0.5285 - lcm_f1_2k: 0.5771 - lcm_f1_3k: 0.5393 - lcm_f1_5k: 0.4425 - lcm_accuracy_1k: 0.6827 - lcm_accuracy_2k: 0.8307 - lcm_accuracy_3k: 0.8889 - lcm_accuracy_5k: 0.9369 - lcm_hamming_loss_k: 0.0036
Epoch 00011: val_loss improved from 0.26979 to 0.26838, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2345 - lcm_precision_1k: 0.6827 - lcm_precision_2k: 0.5335 - lcm_precision_3k: 0.4279 - lcm_precision_5k: 0.3019 - lcm_recall_1k: 0.4312 - lcm_recall_2k: 0.6285 - lcm_recall_3k: 0.7290 - lcm_recall_5k: 0.8283 - lcm_f1_1k: 0.5285 - lcm_f1_2k: 0.5771 - lcm_f1_3k: 0.5393 - lcm_f1_5k: 0.4425 - lcm_accuracy_1k: 0.6827 - lcm_accuracy_2k: 0.8307 - lcm_accuracy_3k: 0.8889 - lcm_accuracy_5k: 0.9369 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2684 - val_lcm_precision_1k: 0.6076 - val_lcm_precision_2k: 0.4813 - val_lcm_precision_3k: 0.3826 - val_lcm_precision_5k: 0.2717 - val_lcm_recall_1k: 0.3820 - val_lcm_recall_2k: 0.5709 - val_lcm_recall_3k: 0.6614 - val_lcm_recall_5k: 0.7602 - val_lcm_f1_1k: 0.4690 - val_lcm_f1_2k: 0.5221 - val_lcm_f1_3k: 0.4846 - val_lcm_f1_5k: 0.4002 - val_lcm_accuracy_1k: 0.6076 - val_lcm_accuracy_2k: 0.7637 - val_lcm_accuracy_3k: 0.8216 - val_lcm_accuracy_5k: 0.8834 - val_lcm_hamming_loss_k: 0.0039
Epoch 12/150
27/27 [==============================] - ETA: 0s - loss: 0.2273 - lcm_precision_1k: 0.6945 - lcm_precision_2k: 0.5421 - lcm_precision_3k: 0.4333 - lcm_precision_5k: 0.3060 - lcm_recall_1k: 0.4412 - lcm_recall_2k: 0.6403 - lcm_recall_3k: 0.7396 - lcm_recall_5k: 0.8399 - lcm_f1_1k: 0.5395 - lcm_f1_2k: 0.5870 - lcm_f1_3k: 0.5464 - lcm_f1_5k: 0.4485 - lcm_accuracy_1k: 0.6945 - lcm_accuracy_2k: 0.8401 - lcm_accuracy_3k: 0.8973 - lcm_accuracy_5k: 0.9450 - lcm_hamming_loss_k: 0.0036
Epoch 00012: val_loss improved from 0.26838 to 0.26349, saving model to logs/jxduin-labs-0604-123040/model/checkpoint_labs.h5
27/27 [==============================] - 11s 427ms/step - loss: 0.2273 - lcm_precision_1k: 0.6945 - lcm_precision_2k: 0.5421 - lcm_precision_3k: 0.4333 - lcm_precision_5k: 0.3060 - lcm_recall_1k: 0.4412 - lcm_recall_2k: 0.6403 - lcm_recall_3k: 0.7396 - lcm_recall_5k: 0.8399 - lcm_f1_1k: 0.5395 - lcm_f1_2k: 0.5870 - lcm_f1_3k: 0.5464 - lcm_f1_5k: 0.4485 - lcm_accuracy_1k: 0.6945 - lcm_accuracy_2k: 0.8401 - lcm_accuracy_3k: 0.8973 - lcm_accuracy_5k: 0.9450 - lcm_hamming_loss_k: 0.0036 - val_loss: 0.2635 - val_lcm_precision_1k: 0.6104 - val_lcm_precision_2k: 0.4841 - val_lcm_precision_3k: 0.3874 - val_lcm_precision_5k: 0.2740 - val_lcm_recall_1k: 0.3843 - val_lcm_recall_2k: 0.5755 - val_lcm_recall_3k: 0.6696 - val_lcm_recall_5k: 0.7656 - val_lcm_f1_1k: 0.4715 - val_lcm_f1_2k: 0.5257 - val_lcm_f1_3k: 0.4906 - val_lcm_f1_5k: 0.4034 - val_lcm_accuracy_1k: 0.6104 - val_lcm_accuracy_2k: 0.7672 - val_lcm_accuracy_3k: 0.8306 - val_lcm_accuracy_5k: 0.8865 - val_lcm_hamming_loss_k: 0.0039
Epoch 13/150
27/27 [==============================] - ETA: 0s - loss: 0.2216 - lcm_precision_1k: 0.7042 - lcm_precision_2k: 0.5524 - lcm_precision_3k: 0.4415 - lcm_precision_5k: 0.3109 - lcm_recall_1k: 0.4453 - lcm_recall_2k: 0.6503 - lcm_recall_3k: 0.7504 - lcm_recall_5k: 0.8508 - lcm_f1_1k: 0.5456 - lcm_f1_2k: 0.5973 - lcm_f1_3k: 0.5559 - lcm_f1_5k: 0.4553 - lcm_accuracy_1k: 0.7042 - lcm_accuracy_2k: 0.8488 - lcm_accuracy_3k: 0.9029 - lcm_accuracy_5k: 0.9511 - lcm_hamming_loss_k: 0.0035
Epoch 00013: val_loss did not improve from 0.26349
27/27 [==============================] - 10s 390ms/step - loss: 0.2216 - lcm_precision_1k: 0.7042 - lcm_precision_2k: 0.5524 - lcm_precision_3k: 0.4415 - lcm_precision_5k: 0.3109 - lcm_recall_1k: 0.4453 - lcm_recall_2k: 0.6503 - lcm_recall_3k: 0.7504 - lcm_recall_5k: 0.8508 - lcm_f1_1k: 0.5456 - lcm_f1_2k: 0.5973 - lcm_f1_3k: 0.5559 - lcm_f1_5k: 0.4553 - lcm_accuracy_1k: 0.7042 - lcm_accuracy_2k: 0.8488 - lcm_accuracy_3k: 0.9029 - lcm_accuracy_5k: 0.9511 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2652 - val_lcm_precision_1k: 0.6144 - val_lcm_precision_2k: 0.4835 - val_lcm_precision_3k: 0.3885 - val_lcm_precision_5k: 0.2746 - val_lcm_recall_1k: 0.3868 - val_lcm_recall_2k: 0.5767 - val_lcm_recall_3k: 0.6725 - val_lcm_recall_5k: 0.7664 - val_lcm_f1_1k: 0.4746 - val_lcm_f1_2k: 0.5258 - val_lcm_f1_3k: 0.4923 - val_lcm_f1_5k: 0.4042 - val_lcm_accuracy_1k: 0.6144 - val_lcm_accuracy_2k: 0.7718 - val_lcm_accuracy_3k: 0.8337 - val_lcm_accuracy_5k: 0.8861 - val_lcm_hamming_loss_k: 0.0039
Epoch 14/150
27/27 [==============================] - ETA: 0s - loss: 0.2165 - lcm_precision_1k: 0.7153 - lcm_precision_2k: 0.5602 - lcm_precision_3k: 0.4474 - lcm_precision_5k: 0.3144 - lcm_recall_1k: 0.4555 - lcm_recall_2k: 0.6608 - lcm_recall_3k: 0.7618 - lcm_recall_5k: 0.8604 - lcm_f1_1k: 0.5565 - lcm_f1_2k: 0.6062 - lcm_f1_3k: 0.5636 - lcm_f1_5k: 0.4604 - lcm_accuracy_1k: 0.7153 - lcm_accuracy_2k: 0.8587 - lcm_accuracy_3k: 0.9131 - lcm_accuracy_5k: 0.9558 - lcm_hamming_loss_k: 0.0035 ETA: 3s - loss: 0.2153 - lcm_precision_1k: 0.7221 - lcm_precision_2k: 0.5662 - lcm_precision_3k: 0.4503 - lcm_precision_5k: 0.3154 - lcm_recall_1k: 0.4585 - lcm_recall_2k: 0.6668 - lcm_recall_3k: 0.7662 - lcm_recall_5k: 0.8627 - lcm_f1_1k: 0.5608 - lcm_f1_2k: 0.6123 - lcm_f1_3k: 0.5672 - lcm_f1_5k: 0.4619 - lcm_accuracy_1k: 0.7221 - lcm_accuracy_2k: 0.8639 - lcm_accuracy_3k: 0.9170 - lcm_accuracy_5k: 0.9573 - lcm_ha
Epoch 00014: val_loss did not improve from 0.26349
27/27 [==============================] - 10s 390ms/step - loss: 0.2165 - lcm_precision_1k: 0.7153 - lcm_precision_2k: 0.5602 - lcm_precision_3k: 0.4474 - lcm_precision_5k: 0.3144 - lcm_recall_1k: 0.4555 - lcm_recall_2k: 0.6608 - lcm_recall_3k: 0.7618 - lcm_recall_5k: 0.8604 - lcm_f1_1k: 0.5565 - lcm_f1_2k: 0.6062 - lcm_f1_3k: 0.5636 - lcm_f1_5k: 0.4604 - lcm_accuracy_1k: 0.7153 - lcm_accuracy_2k: 0.8587 - lcm_accuracy_3k: 0.9131 - lcm_accuracy_5k: 0.9558 - lcm_hamming_loss_k: 0.0035 - val_loss: 0.2668 - val_lcm_precision_1k: 0.6181 - val_lcm_precision_2k: 0.4863 - val_lcm_precision_3k: 0.3905 - val_lcm_precision_5k: 0.2765 - val_lcm_recall_1k: 0.3893 - val_lcm_recall_2k: 0.5776 - val_lcm_recall_3k: 0.6730 - val_lcm_recall_5k: 0.7712 - val_lcm_f1_1k: 0.4776 - val_lcm_f1_2k: 0.5279 - val_lcm_f1_3k: 0.4941 - val_lcm_f1_5k: 0.4070 - val_lcm_accuracy_1k: 0.6181 - val_lcm_accuracy_2k: 0.7724 - val_lcm_accuracy_3k: 0.8319 - val_lcm_accuracy_5k: 0.8887 - val_lcm_hamming_loss_k: 0.0039
Epoch 00014: early stopping
176/176 [==============================] - 8s 41ms/step - loss: 0.2301 - lcm_precision_1k: 0.6871 - lcm_precision_2k: 0.5368 - lcm_precision_3k: 0.4303 - lcm_precision_5k: 0.3020 - lcm_recall_1k: 0.4383 - lcm_recall_2k: 0.6380 - lcm_recall_3k: 0.7402 - lcm_recall_5k: 0.8365 - lcm_f1_1k: 0.5339 - lcm_f1_2k: 0.5818 - lcm_f1_3k: 0.5431 - lcm_f1_5k: 0.4430 - lcm_accuracy_1k: 0.6871 - lcm_accuracy_2k: 0.8329 - lcm_accuracy_3k: 0.8929 - lcm_accuracy_5k: 0.9394 - lcm_hamming_loss_k: 0.0036 4s - loss: 0.2315 - lcm_precision_1k: 0.6811 - lcm_precision_2k: 0.5303 - lcm_precision_3k: 0.4280 - lcm_precision_5k: 0.3018 - lcm_recall_1k: 0.4306 - lcm_recall_2k: 0.6291 - lcm_recall_3k: 0.7376 - lcm_recall_5k: 0.8376 - lcm_f1_1k: 0.5262 - lcm_f1_2k: 0.5741 - lcm_f1_3k: 0.5405 - lcm_f1_5k: 0.4430 - lcm_accuracy_1k: 0.6811 - lcm_accuracy_2k: 0.8249 - lcm_accuracy_3k: 0.8902  - ETA: 2s - loss: 0.2297 - lcm_precision_1k: 0.6919 - lcm_precision_2k: 0.5360 - lcm_precision_3k: 0.4301 - lcm_precision_5k: 0.3006 - lcm_recall_1k: 0.4412 - lcm_recall_2k: 0.6374 - lcm_recall_3k: 0.7422 - lcm_recall_5k: 0.8362 - lcm_f1_1k: 0.5375 - lcm_f1_2k: 0.5810 - lcm_f1_3k: 0.5435 - lcm_f1_5k: 0.4415 - lcm_accuracy_1k: 0.6919 - lcm_accuracy_2k: 0.8311 - lcm_accuracy_3k: 0.892
Best model result:  [0.23009653389453888, 0.6870517134666443, 0.5367794632911682, 0.430259108543396, 0.3020080626010895, 0.4382738173007965, 0.6379643678665161, 0.7402006387710571, 0.8364652395248413, 0.5338870286941528, 0.5818224549293518, 0.5431176424026489, 0.4430427551269531, 0.6870517134666443, 0.8328540325164795, 0.8929104208946228, 0.9394088387489319, 0.0035676362458616495]
fold_result:  [[0.24239058792591095, 0.6643727421760559, 0.5163941979408264, 0.41083985567092896, 0.29219552874565125, 0.42024943232536316, 0.6129316687583923, 0.7045593857765198, 0.808739423751831, 0.5136027932167053, 0.5594011545181274, 0.5180511474609375, 0.4285776913166046, 0.6643727421760559, 0.8112583756446838, 0.8658292293548584, 0.9199478030204773, 0.003673854283988476], [0.237636998295784, 0.6704132556915283, 0.5246450304985046, 0.42229774594306946, 0.2968762218952179, 0.42680931091308594, 0.6237283945083618, 0.7268832325935364, 0.8221635222434998, 0.5202641487121582, 0.5687291622161865, 0.5331758856773376, 0.4355260133743286, 0.6704132556915283, 0.822195291519165, 0.8827915787696838, 0.9311913847923279, 0.003645578632131219], [0.21121945977210999, 0.7295375466346741, 0.5633057951927185, 0.4492250680923462, 0.31158632040023804, 0.47133633494377136, 0.6684495210647583, 0.7691040635108948, 0.8578312397003174, 0.5714375972747803, 0.6102519035339355, 0.5661414265632629, 0.4564063251018524, 0.7295375466346741, 0.8574018478393555, 0.9097753763198853, 0.9471006393432617, 0.003368639387190342], [0.19639618694782257, 0.7537881731987, 0.5920263528823853, 0.46935299038887024, 0.32246720790863037, 0.4865584671497345, 0.6996245980262756, 0.7984914183616638, 0.8798694014549255, 0.5901800394058228, 0.6401146650314331, 0.5901197791099548, 0.4712243378162384, 0.7537881731987, 0.8813679218292236, 0.9273601770401001, 0.9554733633995056, 0.0032550685573369265], [0.23009653389453888, 0.6870517134666443, 0.5367794632911682, 0.430259108543396, 0.3020080626010895, 0.4382738173007965, 0.6379643678665161, 0.7402006387710571, 0.8364652395248413, 0.5338870286941528, 0.5818224549293518, 0.5431176424026489, 0.4430427551269531, 0.6870517134666443, 0.8328540325164795, 0.8929104208946228, 0.9394088387489319, 0.0035676362458616495]]
average_result:  [0.22354795336723327, 0.7010326862335206, 0.5466301679611206, 0.4363949537277222, 0.3050266683101654, 0.4486454725265503, 0.6485397100448609, 0.7478477478027343, 0.841013765335083, 0.5458743214607239, 0.5920638680458069, 0.5501211762428284, 0.44695542454719545, 0.7010326862335206, 0.8410154938697815, 0.8957333564758301, 0.9386244058609009, 0.0035021554213017225]
2024-06-04 12:33:30,530 : INFO : =======End=======
