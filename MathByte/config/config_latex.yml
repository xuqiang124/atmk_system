# this config is for dataset DA-20K, Formula-T(formulas are cut as normal text)
cache_file_h5py: "../file_data/a32/math_data_latex.h5"
cache_file_pickle: "../file_data/a32/vocab_label.pkl"
embeddings: "../file_data/a32/embeddings.pkl"
maxlen: 150 # 句子最大长度
emb_size: 300
epochs: 150
batch_size: 512 # 批处理尺寸, 感觉原则上越大越好,尤其是样本不均衡的时候, batch_size设置影响比较大
alpha: 4 # new model 的 loss 中的 alpha
hidden_size: 512 # lstm
num_classes_list: [15,427]
l_patience: 2  # patience for early stopping
b_patience: 10 # patience for basic model with a bigger patience